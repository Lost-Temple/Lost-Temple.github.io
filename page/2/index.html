<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.blockchainof.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="自留地">
<meta property="og:url" content="http://www.blockchainof.com/page/2/index.html">
<meta property="og:site_name" content="自留地">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="暂留白">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://www.blockchainof.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>自留地</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">自留地</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-主页">

    <a href="/" rel="section"><i class="fas fa-home fa-fw"></i>主页</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fas fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fas fa-folder-open fa-fw"></i>分类</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2023/12/06/Storm%E3%80%81Spark%E4%B8%8EFlink%E5%AF%B9%E6%AF%94/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/06/Storm%E3%80%81Spark%E4%B8%8EFlink%E5%AF%B9%E6%AF%94/" class="post-title-link" itemprop="url">Storm、Spark与Flink对比</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-06 16:06:42" itemprop="dateCreated datePublished" datetime="2023-12-06T16:06:42+08:00">2023-12-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-12-08 11:43:04" itemprop="dateModified" datetime="2023-12-08T11:43:04+08:00">2023-12-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/" itemprop="url" rel="index"><span itemprop="name">Flink</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>引言</h1>
<p>Apache Flink(以下简称flink) 是一个旨在提供‘一站式’ 的分布式开源数据处理框架。和Spark的目标一样，都希望提供一个统一功能的计算平台给用户。虽然目标非常类似，但是flink在实现上和spark存在着很大的区别，flink是一个面向流的处理框架，输入在flink中是无界的，流数据是flink中的头等公民。这方面flink和storm有几分相似。那么有spark和storm这样成熟的计算框架存在，为什么flink还能占有一席之地呢？今天我们就从流处理的角度将flink和这两个框架进行一些分析和比较。</p>
<h1>名词解析</h1>
<table>
<thead>
<tr>
<th>名词</th>
<th>说明</th>
<th>举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>无边界数据</td>
<td>无边界数据通常指的是无限持续生成的数据流，没有明确定义的终点。这样的数据流可能是实时生成的事件、传感器数据、日志记录等，它们源源不断地产生，没有固定的结束点。</td>
<td>实时传感器数据，如温度传感器、网络日志，用户点击事件流等。这些数据是持续不断生成的，流式处理系统需要实时处理并适应新的数据产生。</td>
</tr>
<tr>
<td>有边界数据</td>
<td>有边界数据是有明确定义的开始和结束点的数据集。这样的数据集在某一时刻是完整的，不再发生变化。传统的批处理任务就是对有边界数据进行处理的典型例子。</td>
<td>关系数据库中的表、存储在文件中的数据集等。这些数据集在某一时刻是完整的，可以通过一次性加载到内存中进行批处理分析。</td>
</tr>
<tr>
<td>Exactly-Once 语义</td>
<td>精确一次（Exactly-Once）处理语义，确保在出现故障或恢复时不会丢失或重复处理事件。这是保证数据处理的一致性的重要特性。</td>
<td></td>
</tr>
<tr>
<td>有状态计算</td>
<td>有状态的计算指能够处理具有记忆和上下文关联的计算任务。这对于处理事件序列、实时聚合和复杂的模式匹配非常有用。</td>
<td></td>
</tr>
<tr>
<td>Sink</td>
<td>Sink是Flink中一种用于将数据从流处理应用程序发送到外部系统的组件，用于定义数据的最终目的地</td>
<td>File Sink（文件输出）; Kafka Sink; Elasticsearch Sink; JDBC Sink; Custom Sink;</td>
</tr>
</tbody>
</table>
<h1>流框架基于的实现方式</h1>
<p>本文涉及的流框架基于的实现方式分为两大类。</p>
<ul>
<li>
<p>第一类是Native Streaming，这类引擎中所有的data在到来的时候就会被立即处理，一条接着一条（HINT： 狭隘的来说是一条接着一条，但流引擎有时会为提高性能缓存一小部分data然后一次性处理），其中的代表就是storm和flink。</p>
</li>
<li>
<p>第二类则是基于Micro-batch，数据流被切分为一个一个小的批次， 然后再逐个被引擎处理。这些batch一般是以时间为单位进行切分，单位一般是‘秒‘，其中的典型代表则是spark了，不论是老的spark DStream还是2.0以后推出的spark structured streaming都是这样的处理机制；另外一个基于Micro-batch实现的就是storm trident，它是对storm的更高层的抽象，因为以batch为单位，所以storm trident的一些处理变得简单且高效。</p>
</li>
</ul>
<p><img src="/2023/12/06/Storm%E3%80%81Spark%E4%B8%8EFlink%E5%AF%B9%E6%AF%94/Snipaste_2023-12-06_17-54-25.png" alt="分类"></p>
<h1>流框架比较的关键指标</h1>
<p>从流处理的角度将flink与spark和storm这两个框架进行比较，会主要关注以下几点，后续的对比也主要基于这几点展开：</p>
<ul>
<li>
<p>功能性（Functionality）- 是否能很好解决流处理功能上的痛点 , 比如event time和out of order data。</p>
</li>
<li>
<p>容错性（Fault Tolerance） - 在failure之后能否恢复到故障之前的状态，并输出一致的结果；此外容错的代价也是越低越好，因为其直接影响性能。</p>
</li>
<li>
<p>吞吐量(throughputs)&amp; 延时(latency) - 性能相关的指标，高吞吐和低延迟某种意义上是不可兼得的，但好的流引擎应能兼顾高吞吐&amp;低延时。</p>
</li>
</ul>
<h1>功能性</h1>
<h2 id="各类时间的定义">各类时间的定义</h2>
<p><a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.2/dev/event_time.html">flink文档中关于各种时间的定义</a></p>
<ul>
<li>
<p>Event time: 指数据或事件真正发生时间，比如用户点击网页时产生一条点击事件的数据，点击时间就是这条数据固有的Event time。理解为每个单独事件在其产生设备上发生的时间。</p>
</li>
<li>
<p>Processing time: 指计算框架处理这条数据的时间</p>
</li>
<li>
<p>Ingestion Time: 摄入时间是事件进入 Flink 的时间。</p>
<p><img src="/2023/12/06/Storm%E3%80%81Spark%E4%B8%8EFlink%E5%AF%B9%E6%AF%94/times_clocks.svg" alt="时钟"></p>
<p>spark DStream和storm 1.0以前版本往往都折中地使用processing time来近似地实现event time相关的业务。显然，使用processing time模拟event time必然会产生一些误差， 特别是在产生数据堆积的时候，误差则更明显，甚至导致计算结果不可用。</p>
<p>在使用event time时，自然而然需要解决由网络延迟等因素导致的迟到或者乱序数据的问题。为了解决这个问题， spark、storm及flink都参考<a target="_blank" rel="noopener" href="https://www.oreilly.com/radar/the-world-beyond-batch-streaming-102/">streaming 102</a>引入了watermark和lateness的概念。</p>
</li>
<li>
<p>watermark: 是引擎处理事件的时间进度，代表一种状态，一般随着数据中的event time的增长而增长。比如 watermark(t)代表整个流的event time处理进度已经到达t， 时间是有序的，那么streaming不应该会再收到timestamp t’ &lt; t的数据，而只会接受到timestamp t’ &gt;= t的数据。 如果收到一条timestamp t’ &lt; t的数据， 那么就说明这条数据是迟到的。</p>
</li>
<li>
<p>lateness: 表示可以容忍迟到的程度，在lateness可容忍范围内的数据还会参与计算，超过的会被丢弃。</p>
</li>
</ul>
<h2 id="窗口操作">窗口操作</h2>
<h3 id="spark-structured-streaming-和flink对event-time处理机制的比较">spark structured streaming 和flink对event time处理机制的比较</h3>
<ul>
<li>
<p>Flink</p>
<p>首先，我们结合图来看flink， 时间轴从左往右增大。当watermark WM处于时 间窗口区间内时，即WM ∈ [start, end] , event time落在窗口范围内的任何乱序数据都会被接受；随着WM的增长并超过了窗口的结束时间，但还未超过可容忍的lateness时间范围，即WM ∈ (window_end,window_end+ lateness]， 这时乱序数据仍然可以被接受； 只有当WM超过 window_end+lateness, 即WM ∈ (window_end+ lateness, ∞)， 迟到的数据将会被丢弃。</p>
<p><img src="/2023/12/06/Storm%E3%80%81Spark%E4%B8%8EFlink%E5%AF%B9%E6%AF%94/handle-late-records.jpg" alt="handle-late-records"></p>
<p>fiink中watermark的计算也比较灵活，可以选择build-in的（如最大时间戳），也可以通过继承接口自定义实现。此外，用户可以选择周期性更新或者事件触发更新watermark。</p>
</li>
<li>
<p>Spark</p>
<p>首先,spark中watermark是通过上一个batch最大的timestamp再减去lateness得到的，即watermark = Max(last batch timestamps) - lateness。当数据的event time大于watermark时，数据会被接受，否则不论这条数据属于哪个窗口都会被丢弃。细节请参考<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#window-operations-on-event-time">Window Operations on Event Time</a></p>
</li>
</ul>
<p>下面来比较一下两者实现细节上的不同：</p>
<ul>
<li>
<p>lateness定义: 在spark中，迟到被定义为data的event time和watermark的比较结果，当data的event time &lt; watermark时，data被丢弃；flink中只有在watermark &gt; window_end + lateness的时候，data才会被丢弃。</p>
</li>
<li>
<p>watermark更新: spark中watermark是上个batch中的max event time，存在延迟；而在flink中是可以做到每条数据同步更新watermark。</p>
</li>
<li>
<p>window触发: flink中window计算会触发一次或多次，第一次在watermark &gt;= window_end后立刻触发（main fire），接着会在迟到数据到来后进行增量触发。spark只会在watermark（包含lateness）过了window_end之后才会触发，虽然计算结果一次性正确，但触发比flink起码多了一个lateness的延迟。</p>
</li>
</ul>
<p>上面三点可见flink在设计event time处理模型还是较优的：watermark的计算实时性高，输出延迟低，而且接受迟到数据没有spark那么受限。</p>
<h3 id="SQL-API方面的对比">SQL API方面的对比</h3>
<p>待续，网上资料都是较老版本的比对，不太准确，等自己实践过再来总结吧</p>
<h3 id="Kafka-集成">Kafka 集成</h3>
<p>待续</p>
<h3 id="静态数据操作">静态数据操作</h3>
<p>待续</p>
<h1>吞吐量</h1>
<p>待续</p>
<h1>总结</h1>
<p>待续</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2023/10/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%A6%82%E7%8E%87%E5%92%8C%E5%8F%91%E7%94%9F%E6%AF%94%E4%BB%A5%E5%8F%8ALogit/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/10/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%A6%82%E7%8E%87%E5%92%8C%E5%8F%91%E7%94%9F%E6%AF%94%E4%BB%A5%E5%8F%8ALogit/" class="post-title-link" itemprop="url">机器学习基础-概率和发生比以及Logit</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-10-31 18:57:17" itemprop="dateCreated datePublished" datetime="2023-10-31T18:57:17+08:00">2023-10-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-11-01 10:26:56" itemprop="dateModified" datetime="2023-11-01T10:26:56+08:00">2023-11-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">联邦学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">机器学习基础</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <hr>
<p>概率是机器学习中最常见的概念，在分类算法(Classification)中经常出现，而Logit也是逻辑回归（Logistic Regression）中的重要概念，本文将总结概率（Probability），发生比（Odds）和Logit（log(odds)）之间的关系。</p>
<h2 id="概率，发生比与Logit的定义">概率，发生比与Logit的定义</h2>
<h3 id="概率">概率</h3>
<p>概率是指一个事件发生的可能性，假设一个袋子里有若干红球和蓝球，用P来表示概率。</p>
<p>$P_{抽到红球}=\frac{红球数}{总个数}$</p>
<h3 id="发生比">发生比</h3>
<p>发生比指一件事发生与其不发生的概率之比，同样以红球和篮球为例，用Odds来表示概率，则：</p>
<p>$Odds_{抽到红球}=\frac {P_{抽到红球}}{1-P_{抽到红球}}$</p>
<h3 id="Logit">Logit</h3>
<p>Logit是给发生比取对数，即log(Odds)，其中log是自然对数，以红球篮球为例，其公式为:</p>
<p>$Logit=log(\frac {P_{抽到红球}}{1-P_{抽到红球}})$</p>
<h2 id="概率发生比与Logit的关系">概率发生比与Logit的关系</h2>
<p>概率与发生比的关系较为简单， 概率是指事件发生的可能性，而发生比是指该事件发生与不发生的概率之比。但发生比与Logit的关系就并没有那么直观了，从公式上我们确实可以知道Logit就是log(发生比),<strong>但我们该如何理解这一定义呢？</strong></p>
<p>通过下面的图表不难发现，概率与发生比有以下关系：</p>
<ol>
<li>概率取值为(0,1)，发生比取值为(0,Inf)</li>
<li>当概率为0.5时，发生比为1，即事件发生与不发生的概率相同。</li>
<li>当概率小于0.5时，发生比取值范围为(0,1)，当概率大于0.5时，发生比取值范围为(1,Inf)</li>
<li>当概率大于或小于0.5时，发生比的取值范围并不对称。</li>
</ol>
<p>发生比的不对称性，时常会带来一些困扰，比如：</p>
<ul>
<li>概率为0.3和0.7时，这两个概率的均值是0.5，他们是“对称”的；但这两个概率的发生比却分别是0.429和2.333,这使我们很难直观地感受到他们的对称关系。</li>
<li>当概率从0.1增加到0.2，时，发生比增加了0.139；当概率从0.8增加到0.9时，发生比却增加了5，虽然概率的增量相同，但发生比的增量却大大的不同。</li>
</ul>
<p>而Logit就能有效地解决发生比带来的困惑：</p>
<ul>
<li>概率为0.3和0.7时，Logit分别为-0.847和0.847，这两个值关于0对称,因此我们知道这两Logit值对应的发生比是对称的。</li>
<li>当概率从0.2减少至0.1时，Logit的增量是-0.811；当概率从0.8增加到0.9时，Logit的增量也是0.811，我们可以直观地感受到发生比增量也是对称的。</li>
<li>当发生比为1时，Logit为0；当发生比小于或大于1时，Logit取值范围为(-Inf,0)和(0,Inf).取值范围对称。</li>
</ul>
<p>下表列出了当概率取不同值时，发生比和Logit之间的关系：</p>
<table>
<thead>
<tr>
<th></th>
<th>Probability</th>
<th>Odds</th>
<th>Logit</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.0001</td>
<td>0.00010001</td>
<td>-9.21024</td>
</tr>
<tr>
<td>1</td>
<td>0.001</td>
<td>0.001001</td>
<td>-6.90675</td>
</tr>
<tr>
<td>2</td>
<td>0.01</td>
<td>0.010101</td>
<td>-4.59512</td>
</tr>
<tr>
<td>3</td>
<td>0.1</td>
<td>0.111111</td>
<td>-2.19722</td>
</tr>
<tr>
<td>4</td>
<td>0.2</td>
<td>0.25</td>
<td>-1.38629</td>
</tr>
<tr>
<td>5</td>
<td>0.3</td>
<td>0.428571</td>
<td>-0.847298</td>
</tr>
<tr>
<td>6</td>
<td>0.4</td>
<td>0.666667</td>
<td>-0.405465</td>
</tr>
<tr>
<td>7</td>
<td>0.5</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>8</td>
<td>0.6</td>
<td>1.5</td>
<td>0.405465</td>
</tr>
<tr>
<td>9</td>
<td>0.7</td>
<td>2.33333</td>
<td>0.847298</td>
</tr>
<tr>
<td>10</td>
<td>0.8</td>
<td>4</td>
<td>1.38629</td>
</tr>
<tr>
<td>11</td>
<td>0.9</td>
<td>9</td>
<td>2.19722</td>
</tr>
<tr>
<td>12</td>
<td>0.99</td>
<td>9</td>
<td>4.59512</td>
</tr>
<tr>
<td>13</td>
<td>0.999</td>
<td>999</td>
<td>6.90675</td>
</tr>
<tr>
<td>14</td>
<td>0.9999</td>
<td>9999</td>
<td>9.21024</td>
</tr>
</tbody>
</table>
<p>用图形表示，三者关系如下，其中x轴为概率，两条曲线分别为发生比和Logit，为了让图形更加好看，概率只取了[0.1, 0.9]这一部分：</p>
<p><img src="/2023/10/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%A6%82%E7%8E%87%E5%92%8C%E5%8F%91%E7%94%9F%E6%AF%94%E4%BB%A5%E5%8F%8ALogit/%E6%A6%82%E7%8E%87-%E5%8F%91%E7%94%9F%E6%AF%94-Logit.jpg" alt="概率和发生比以及Logit"></p>
<h2 id="小结">小结</h2>
<p>由于逻辑回归模型的需求，我们引入了发生比这一概念，但发生比本身的不对称性让我们难以直观地比较发生比，因此我们引入了Logit这一概念。<strong>Logit赋予了发生比更强的”对称“性</strong>，让我们更容易比较不同发生比之间的关系。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/" class="post-title-link" itemprop="url">PySpark基础</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-10-26 15:01:25" itemprop="dateCreated datePublished" datetime="2023-10-26T15:01:25+08:00">2023-10-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-11-10 10:20:21" itemprop="dateModified" datetime="2023-11-10T10:20:21+08:00">2023-11-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>简介</h1>
<p>PySpark是Python调用Spark的接口，可以通过调用Python API的方式来编写Spark程序，它支持了大多数的Spark功能，比如SparkDataFrame、Spark SQL、Streaming、MLlib等等。</p>
<h1>基础概念</h1>
<h2 id="RDD">RDD</h2>
<p>RDD的全称是Resilient Distributed Datasets，这是Spark的一种数据抽象集合，它可以被执行在分布式的集群上进行各种操作，而且有较强的容错机制。RDD可以被分为若干个分区，每一个分区就是一个数据集片段，从而可以支持分布式计算。</p>
<h2 id="RDD运行时相关的关键名词">RDD运行时相关的关键名词</h2>
<p>简单来说可以有Client、Job、Master、Worker、Driver、Stage、Task以及Executor，这几个东西在调优的时候也会经常遇到的。</p>
<ul>
<li>Client: 指的是客户端进程，主要负责提交job到Master；</li>
<li>Job: Job来自于我们编写的程序，Application包含一个或多个job，job包含各种RDD操作；</li>
<li>Master: 指的是Standalone模式中的主控节点，负责接收来自Client的job，并管理着worker，可以给worker分配任务和资源（主要是dirver和executor资源）；</li>
<li>Worker: 指的是Standalone模式中的slave节点，负责管理本节点的资源，同时受Master管理，需要定期给Master回报heartbeat（心跳），启动Driver和Executor；</li>
<li>Driver: 指的是job（作业）的主进程，一般每个Spark作业都会有一个Driver进程，负责整个作业的运行，包括job的解析、Stage的生成、调度Task到Executor上去执行；</li>
<li>Stage: 中文名是：阶段，是Job的基本调度单位，因为每个Job会分成若干组Task，每组task就被称为Stage；</li>
<li>Task: 任务，指的是直接运行在executor上的东西，是executor上的一个线程；</li>
<li>Executor: 指的是执行器，顾名思义就是真正执行任务的地方了，一个集群可以被配置若干个Executor，每个Executor接收来自Driver的Task，并执行它（可同时执行多个Task）。</li>
</ul>
<h2 id="DAG">DAG</h2>
<p>全称是Directed Acyclic Graph, 中文名是：有向无环图。Spark就是借用了DAG对RDD之间的关系进行了建模，用来描述RDD之间的因果依赖关系。因为在一个Spark作业调度中，多个作业任务之间也是相互依赖的，有些任务需要在一些任务执行完成了才可以执行的。在Spark调度中就是有DAGscheduler，它负责将job分成若干组Task组成的Stage。<br>
<img src="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/dag.jpg" alt="DAG"></p>
<h2 id="Spark运行模式">Spark运行模式</h2>
<p>主要有local模式、Standalone模式、Mesos模式、YARN模式。</p>
<ul>
<li>Standalone:  独立模式，Spark 原生的简单集群管理器， 自带完整的服务， 可单独部署到一个集群中，无需依赖任何其他资源管理系统， 使用 Standalone 可以很方便地搭建一个集群，一般在公司内部没有搭建其他资源管理框架的时候才会使用。</li>
<li>Mesos: 一个强大的分布式资源管理框架，它允许多种不同的框架部署在其上，包括 yarn，由于mesos这种方式目前应用的比较少。</li>
<li>YARN: 统一的资源管理机制， 在上面可以运行多套计算框架， 如map reduce、storm 等， 根据 driver 在集群中的位置不同，分为 yarn client 和 yarn cluster。<br>
实际上Spark内部为了方便用户测试，自身也提供了一些部署模式。由于在实际工厂环境下使用的绝大多数的集群管理器是 Hadoop YARN，因此我们关注的重点是 Hadoop YARN 模式下的 Spark 集群部署。</li>
</ul>
<p>Spark 的运行模式取决于传递给 SparkContext 的 MASTER 环境变量的值， 个别模式还需要辅助的程序接口来配合使用，目前支持的 Master 字符串及 URL 包括：</p>
<table>
<thead>
<tr>
<th>Master URL</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>local</td>
<td>在本地运行，只有一个工作进程，无并行计算能力</td>
</tr>
<tr>
<td>local[K]</td>
<td>在本地运行，只有K个工作进程，通常设置K为机器的CPU核心数量</td>
</tr>
<tr>
<td>local[*]</td>
<td>在本地运行，工作进程数量等于机器的CPU核心数量</td>
</tr>
<tr>
<td>spark://host:port</td>
<td>以 Standalone 模式运行，这是 Spark 自身提供的集群运行模式，默认端口号: 7077</td>
</tr>
<tr>
<td>mesos://host:port</td>
<td>在 Mesos 集群上运行，Driver 进程和 Worker 进程运行在 Mesos 集群上，部署模式必须使用固定值:–deploy-mode cluster</td>
</tr>
<tr>
<td>yarn-client</td>
<td>在 Yarn 集群上运行，Driver 进程在本地， Work 进程在 Yarn 集群上， 部署模式必须使用固定值:–deploy-modeclient。Yarn 集群地址必须在HADOOP_CONF_DIRorYARN_CONF_DIR 变量里定义。</td>
</tr>
<tr>
<td>yarn-cluster</td>
<td>yarn-cluster 效率比yarn-client高。在 Yarn 集群上运行，Driver 进程在 Yarn 集群上，Work 进程也在 Yarn 集群上，部署模式必须使用固定值:–deploy-mode cluster。Yarn 集群地址必须在HADOOP_CONF_DIR or YARN_CONF_DIR 变量里定义。</td>
</tr>
</tbody>
</table>
<p><strong>用户在提交任务给 Spark 处理时，以下两个参数共同决定了 Spark 的运行方式。</strong></p>
<ul>
<li>master MASTER_URL: 决定了 Spark 任务提交给哪种集群处理。</li>
<li>deploy-mode DEPLOY_MODE: 决定了 Driver 的运行方式，可选值为 Client<br>
或者 Cluster。</li>
</ul>
<h3 id="Standalone模式运行机制">Standalone模式运行机制</h3>
<p>Standalone 集群有四个重要组成部分，分别是：</p>
<ol>
<li>Driver： 是一个进程，我们编写的 Spark 应用程序就运行在 Driver 上， 由Driver 进程执行；</li>
<li>Master：是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责；</li>
<li>Worker：是一个进程，一个 Worker 运行在集群中的一台服务器上，主要负责两个职责，一个是用自己的内存存储 RDD 的某个或某些 partition；另一个是启动其他进程和线程（Executor） ，对 RDD 上的 partition 进行并行的处理和计算。</li>
<li>Executor：是一个进程， 一个 Worker 上可以运行多个 Executor， Executor 通过启动多个线程（task）来执行对 RDD 的 partition 进行并行计算，也就是执行我们对 RDD 定义的例如 map、flatMap、reduce 等算子操作。</li>
</ol>
<h4 id="Standalone-Client模式">Standalone Client模式</h4>
<p>在 Standalone Client 模式下，Driver 在任务提交的本地机器上运行，Driver 启动后向 Master 注册应用程序，Master 根据 submit 脚本的资源需求找到内部资源至少可以启动一个 Executor 的所有 Worker，然后在这些 Worker 之间分配 Executor，Worker 上的 Executor 启动后会向 Driver 反向注册，所有的 Executor 注册完成后，Driver 开始执行 main 函数，之后执行到 Action 算子时，开始划分 stage，每个 stage 生成对应的 taskSet，之后将 task 分发到各个 Executor 上执行。</p>
<h4 id="Standalone-Cluster模式">Standalone Cluster模式</h4>
<p>在 Standalone Cluster 模式下，任务提交后，Master 会找到一个 Worker 启动 Driver进程， Driver 启动后向 Master 注册应用程序， Master 根据 submit 脚本的资源需求找到内部资源至少可以启动一个 Executor 的所有 Worker，然后在这些 Worker 之间分配 Executor，Worker 上的 Executor 启动后会向 Driver 反向注册，所有的 Executor 注册完成后，Driver 开始执行 main 函数，之后执行到 Action 算子时，开始划分 stage，每个 stage 生成对应的 taskSet，之后将 task 分发到各个 Executor 上执行。</p>
<blockquote>
<p>注意， Standalone 的两种模式下（ client/cluster）， Master 在接到 Driver 注册<br>
Spark 应用程序的请求后，会获取其所管理的剩余资源能够启动一个 Executor 的所有 Worker， 然后在这些 Worker 之间分发 Executor，此时的分发只考虑 Worker 上的资源是否足够使用，直到当前应用程序所需的所有 Executor 都分配完毕， Executor 反向注册完毕后，Driver 开始执行 main 程序。</p>
</blockquote>
<h3 id="YARN模式运行机制">YARN模式运行机制</h3>
<h4 id="YARN-Client模式">YARN Client模式</h4>
<ol>
<li>Driver在任务提交的本地机器上运行，Driver启动后会和ResourceManager通讯申请启动ApplicationMaster；</li>
<li>随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster的功能相当于一个ExecutorLaucher，只负责向ResourceManager申 请Executor内存；</li>
<li>ResourceManager接到ApplicationMaster的资源申请后会分配Container，然后ApplicationMaster在资源分配指定的NodeManager上启动Executor进程；</li>
<li>Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数；</li>
<li>之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分Stage，每个Stage生成对应的TaskSet，之后将Task分发到各个Executor上执行。</li>
</ol>
<p><img src="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/yarn-client.png" alt="yarn-client模式"></p>
<h4 id="YARN-Cluster-模式">YARN Cluster 模式</h4>
<ol>
<li>任务提交后会和ResourceManager通讯申请启动ApplicationMaster;</li>
<li>随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster就是Driver；</li>
<li>Driver启动后向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster的资源申请后会分配Container,然后在合适的NodeManager上启动Executor进程;</li>
<li>Executor进程启动后会向Driver反向注册;</li>
<li>Executor全部注册完成后Driver开始执行main函数，之后执行到Action算子时，触发一个job，并根据宽依赖开始划分stage，每个stage生成对应的taskSet，之后将task分发到各个Executor上执行;</li>
</ol>
<p><img src="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/yarn-cluster.jpg" alt="yarn-cluster模式"></p>
<h2 id="Shuffle操作是什么">Shuffle操作是什么</h2>
<p>Shuffle指的是数据从Map端到Reduce端的数据传输过程，Shuffle性能的高低会直接影响程序的性能。因为Reduce task需要跨节点去拉分布在不同节点上的Map task计算结果，这一个过程是需要有磁盘IO消耗以及数据网络传输的消耗的，所以需要根据实际数据情况进行适当调整。另外，Shuffle可以分为两部分，分别是Map阶段的数据准备与Reduce阶段的数据拷贝处理，在Map端我们叫Shuffle Write，在Reduce端我们叫Shuffle Read。</p>
<h2 id="什么是惰性执行">什么是惰性执行</h2>
<p>这是RDD的一个特性，在RDD中的算子可以分为Transform算子和Action算子，其中Transform算子的操作都不会真正执行，只会记录一下依赖关系，直到遇见了Action算子，在这之前的所有Transform操作才会被触发计算，这就是所谓的惰性执行。具体哪些是Transform和Action算子，可以看下一节。</p>
<h2 id="常用函数">常用函数</h2>
<p>常用的算子大概可以分为以下几种：</p>
<h3 id="Transformations">Transformations</h3>
<ul>
<li>map</li>
<li>flatmap</li>
<li>filter</li>
<li>distinct</li>
<li>reduceByKey</li>
<li>mapPartitions</li>
<li>sortBy</li>
</ul>
<h3 id="Actions">Actions</h3>
<ul>
<li>collect</li>
<li>collectAsMap</li>
<li>reduce</li>
<li>countByKey/countByValue</li>
<li>take</li>
<li>first</li>
</ul>
<h3 id="代码示例">代码示例</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pyspark</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">&quot;test_spark_app&quot;</span>).setMaster(<span class="string">&quot;local[4]&quot;</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 parallelize方法直接实例化一个RDD</span></span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">11</span>),<span class="number">4</span>) <span class="comment"># 这里的 4 指的是分区数量</span></span><br><span class="line">rdd.take(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">----------------------------------------------</span></span><br><span class="line"><span class="string">                Transform算子解析</span></span><br><span class="line"><span class="string">----------------------------------------------</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 以下的操作由于是Transform操作，因为我们需要在最后加上一个collect算子用来触发计算。</span></span><br><span class="line"><span class="comment"># 1. map: 和python差不多，map转换就是对每一个元素进行一个映射</span></span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>), <span class="number">4</span>)</span><br><span class="line">rdd_map = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x*<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始数据：&quot;</span>, rdd.collect())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;扩大2倍：&quot;</span>, rdd_map.collect())</span><br><span class="line"><span class="comment"># 原始数据： [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</span></span><br><span class="line"><span class="comment"># 扩大2倍： [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. flatMap: 这个相比于map多一个flat（压平）操作，顾名思义就是要把高维的数组变成一维</span></span><br><span class="line">rdd2 = sc.parallelize([<span class="string">&quot;hello SamShare&quot;</span>, <span class="string">&quot;hello PySpark&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始数据：&quot;</span>, rdd2.collect())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;直接split之后的map结果：&quot;</span>, rdd2.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&quot; &quot;</span>)).collect())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;直接split之后的flatMap结果：&quot;</span>, rdd2.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">&quot; &quot;</span>)).collect())</span><br><span class="line"><span class="comment"># 直接split之后的map结果： [[&#x27;hello&#x27;, &#x27;SamShare&#x27;], [&#x27;hello&#x27;, &#x27;PySpark&#x27;]]</span></span><br><span class="line"><span class="comment"># 直接split之后的flatMap结果： [&#x27;hello&#x27;, &#x27;SamShare&#x27;, &#x27;hello&#x27;, &#x27;PySpark&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. filter: 过滤数据</span></span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>), <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始数据：&quot;</span>, rdd.collect())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;过滤奇数：&quot;</span>, rdd.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x % <span class="number">2</span> == <span class="number">0</span>).collect())</span><br><span class="line"><span class="comment"># 原始数据： [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</span></span><br><span class="line"><span class="comment"># 过滤奇数： [2, 4, 6, 8, 10]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. distinct: 去重元素</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始数据：&quot;</span>, rdd.collect())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;去重数据：&quot;</span>, rdd.distinct().collect())</span><br><span class="line"><span class="comment"># 原始数据： [2, 2, 4, 8, 8, 8, 8, 16, 32, 32]</span></span><br><span class="line"><span class="comment"># 去重数据： [4, 8, 16, 32, 2]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. reduceByKey: 根据key来映射数据</span></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</span><br><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始数据：&quot;</span>, rdd.collect())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始数据：&quot;</span>, rdd.reduceByKey(add).collect())</span><br><span class="line"><span class="comment"># 原始数据： [(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 1), (&#x27;a&#x27;, 1)]</span></span><br><span class="line"><span class="comment"># 原始数据： [(&#x27;b&#x27;, 1), (&#x27;a&#x27;, 2)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. mapPartitions: 根据分区内的数据进行映射操作</span></span><br><span class="line"><span class="comment"># 把数据集发成两个分区，在分区内进行求和操作</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], <span class="number">2</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">iterator</span>):</span><br><span class="line">    <span class="keyword">yield</span> <span class="built_in">sum</span>(iterator)</span><br><span class="line"><span class="built_in">print</span>(rdd.collect())</span><br><span class="line"><span class="built_in">print</span>(rdd.mapPartitions(f).collect())</span><br><span class="line"><span class="comment"># [1, 2, 3, 4]</span></span><br><span class="line"><span class="comment"># [3, 7]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. sortBy: 根据规则进行排序</span></span><br><span class="line">tmp = [(<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;1&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;d&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;2&#x27;</span>, <span class="number">5</span>)]</span><br><span class="line"><span class="built_in">print</span>(sc.parallelize(tmp).sortBy(<span class="keyword">lambda</span> x: x[<span class="number">0</span>]).collect())</span><br><span class="line"><span class="built_in">print</span>(sc.parallelize(tmp).sortBy(<span class="keyword">lambda</span> x: x[<span class="number">1</span>]).collect())</span><br><span class="line"><span class="comment"># [(&#x27;1&#x27;, 3), (&#x27;2&#x27;, 5), (&#x27;a&#x27;, 1), (&#x27;b&#x27;, 2), (&#x27;d&#x27;, 4)]</span></span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 2), (&#x27;1&#x27;, 3), (&#x27;d&#x27;, 4), (&#x27;2&#x27;, 5)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. subtract: 数据集相减, Return each value in self that is not contained in other.</span></span><br><span class="line">x = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>)])</span><br><span class="line">y = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;c&quot;</span>, <span class="literal">None</span>)])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(x.subtract(y).collect()))</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 4), (&#x27;b&#x27;, 5)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9. union: 合并两个RDD</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(rdd.union(rdd).collect())</span><br><span class="line"><span class="comment"># [1, 1, 2, 3, 1, 1, 2, 3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 10. intersection: 取两个RDD的交集，同时有去重的功效</span></span><br><span class="line">rdd1 = sc.parallelize([<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">rdd2 = sc.parallelize([<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line"><span class="built_in">print</span>(rdd1.intersection(rdd2).collect())</span><br><span class="line"><span class="comment"># [1, 2, 3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 11. cartesian: 生成笛卡尔积</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(rdd.cartesian(rdd).collect()))</span><br><span class="line"><span class="comment"># [(1, 1), (1, 2), (2, 1), (2, 2)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 12. zip: 拉链合并，需要两个RDD具有相同的长度以及分区数量</span></span><br><span class="line">x = sc.parallelize(<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">5</span>))</span><br><span class="line">y = sc.parallelize(<span class="built_in">range</span>(<span class="number">1000</span>, <span class="number">1005</span>))</span><br><span class="line"><span class="built_in">print</span>(x.collect())</span><br><span class="line"><span class="built_in">print</span>(y.collect())</span><br><span class="line"><span class="built_in">print</span>(x.<span class="built_in">zip</span>(y).collect())</span><br><span class="line"><span class="comment"># [0, 1, 2, 3, 4]</span></span><br><span class="line"><span class="comment"># [1000, 1001, 1002, 1003, 1004]</span></span><br><span class="line"><span class="comment"># [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 13. zipWithIndex: 将RDD和一个从0开始的递增序列按照拉链方式连接。</span></span><br><span class="line">rdd_name = sc.parallelize([<span class="string">&quot;LiLei&quot;</span>, <span class="string">&quot;Hanmeimei&quot;</span>, <span class="string">&quot;Lily&quot;</span>, <span class="string">&quot;Lucy&quot;</span>, <span class="string">&quot;Ann&quot;</span>, <span class="string">&quot;Dachui&quot;</span>, <span class="string">&quot;RuHua&quot;</span>])</span><br><span class="line">rdd_index = rdd_name.zipWithIndex()</span><br><span class="line"><span class="built_in">print</span>(rdd_index.collect())</span><br><span class="line"><span class="comment"># [(&#x27;LiLei&#x27;, 0), (&#x27;Hanmeimei&#x27;, 1), (&#x27;Lily&#x27;, 2), (&#x27;Lucy&#x27;, 3), (&#x27;Ann&#x27;, 4), (&#x27;Dachui&#x27;, 5), (&#x27;RuHua&#x27;, 6)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 14. groupByKey: 按照key来聚合数据</span></span><br><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="built_in">print</span>(rdd.collect())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(rdd.groupByKey().mapValues(<span class="built_in">len</span>).collect()))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(rdd.groupByKey().mapValues(<span class="built_in">list</span>).collect()))</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 1), (&#x27;a&#x27;, 1)]</span></span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 2), (&#x27;b&#x27;, 1)]</span></span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, [1, 1]), (&#x27;b&#x27;, [1])]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 15. sortByKey:</span></span><br><span class="line">tmp = [(<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;1&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;d&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;2&#x27;</span>, <span class="number">5</span>)]</span><br><span class="line"><span class="built_in">print</span>(sc.parallelize(tmp).sortByKey(<span class="literal">True</span>, <span class="number">1</span>).collect())</span><br><span class="line"><span class="comment"># [(&#x27;1&#x27;, 3), (&#x27;2&#x27;, 5), (&#x27;a&#x27;, 1), (&#x27;b&#x27;, 2), (&#x27;d&#x27;, 4)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 16. join:</span></span><br><span class="line">x = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)])</span><br><span class="line">y = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>)])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(x.join(y).collect()))</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, (1, 2)), (&#x27;a&#x27;, (1, 3))]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 17. leftOuterJoin/rightOuterJoin</span></span><br><span class="line">x = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)])</span><br><span class="line">y = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">2</span>)])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(x.leftOuterJoin(y).collect()))</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, (1, 2)), (&#x27;b&#x27;, (4, None))]</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">----------------------------------------------</span></span><br><span class="line"><span class="string">                Action算子解析</span></span><br><span class="line"><span class="string">----------------------------------------------</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 1. collect: 指的是把数据都汇集到driver端，便于后续的操作</span></span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">5</span>))</span><br><span class="line">rdd_collect = rdd.collect()</span><br><span class="line"><span class="built_in">print</span>(rdd_collect)</span><br><span class="line"><span class="comment"># [0, 1, 2, 3, 4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. first: 取第一个元素</span></span><br><span class="line">sc.parallelize([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]).first()</span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. collectAsMap: 转换为dict，使用这个要注意了，不要对大数据用，不然全部载入到driver端会爆内存</span></span><br><span class="line">m = sc.parallelize([(<span class="number">1</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">4</span>)]).collectAsMap()</span><br><span class="line">m</span><br><span class="line"><span class="comment"># &#123;1: 2, 3: 4&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. reduce: 逐步对两个元素进行操作</span></span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">10</span>),<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(rdd.reduce(<span class="keyword">lambda</span> x,y:x+y))</span><br><span class="line"><span class="comment"># 45</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. countByKey/countByValue:</span></span><br><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(rdd.countByKey().items()))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(rdd.countByValue().items()))</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 2), (&#x27;b&#x27;, 1)]</span></span><br><span class="line"><span class="comment"># [((&#x27;a&#x27;, 1), 2), ((&#x27;b&#x27;, 1), 1)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. take: 相当于取几个数据到driver端</span></span><br><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="built_in">print</span>(rdd.take(<span class="number">5</span>))</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 1), (&#x27;a&#x27;, 1)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. saveAsTextFile: 保存rdd成text文件到本地</span></span><br><span class="line">text_file_path = <span class="string">&quot;./data/rdd&quot;</span></span><br><span class="line">shutil.rmtree(text_file_path, <span class="literal">True</span>)</span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">5</span>))</span><br><span class="line">rdd.saveAsTextFile(text_file_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. takeSample: 随机取数</span></span><br><span class="line">rdd = sc.textFile(<span class="string">&quot;./samples/zipcodes.csv&quot;</span>, <span class="number">4</span>)  <span class="comment"># 这里的 4 指的是分区数量</span></span><br><span class="line">rdd_sample = rdd.takeSample(<span class="literal">True</span>, <span class="number">2</span>, <span class="number">0</span>)  <span class="comment"># withReplacement 参数1：代表是否是有放回抽样</span></span><br><span class="line"><span class="built_in">print</span>(rdd_sample)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9. foreach: 对每一个元素执行某种操作，不生成新的RDD</span></span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">10</span>), <span class="number">5</span>)</span><br><span class="line">accum = sc.accumulator(<span class="number">0</span>)</span><br><span class="line">rdd.foreach(<span class="keyword">lambda</span> x: accum.add(x))</span><br><span class="line"><span class="built_in">print</span>(accum.value)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Spark-SQL使用">Spark SQL使用</h2>
<p>在讲Spark SQL前，先解释下这个模块。这个模块是Spark中用来处理结构化数据的，提供一个叫SparkDataFrame的东西并且自动解析为分布式SQL查询数据。用过Python的Pandas库，也大致了解了DataFrame，这个其实和它没有太大的区别，只是调用的API可能有些不同罢了。</p>
<p>我们通过使用Spark SQL来处理数据，会让我们更加熟悉，比如可以用SQL语句、用SparkDataFrame的API或者Datasets API，我们可以按照需求随心转换，通过SparkDataFrame API 和 SQL 写的逻辑，会被Spark优化器Catalyst自动优化成RDD。</p>
<h2 id="创建SparkDataFrame">创建SparkDataFrame</h2>
<p>开始讲SparkDataFrame，我们先学习下几种创建的方法，分别是使用RDD来创建、使用pandas的DataFrame来创建、使用List来创建、读取数据文件来创建、通过读取数据库来创建。</p>
<h3 id="使用RDD来创建">使用RDD来创建</h3>
<p>主要使用RDD的toDF方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;Sam&quot;</span>, <span class="number">28</span>, <span class="number">88</span>), (<span class="string">&quot;Flora&quot;</span>, <span class="number">28</span>, <span class="number">90</span>), (<span class="string">&quot;Run&quot;</span>, <span class="number">1</span>, <span class="number">60</span>)])</span><br><span class="line">df = rdd.toDF([<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>, <span class="string">&quot;score&quot;</span>])</span><br><span class="line">df.show()</span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br><span class="line"><span class="comment"># | name|age|score|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment">#  |-- name: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- score: long (nullable = true)</span></span><br></pre></td></tr></table></figure>
<h3 id="使用pandas的DataFrame来创建">使用pandas的DataFrame来创建</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame([[<span class="string">&#x27;Sam&#x27;</span>, <span class="number">28</span>, <span class="number">88</span>], [<span class="string">&#x27;Flora&#x27;</span>, <span class="number">28</span>, <span class="number">90</span>], [<span class="string">&#x27;Run&#x27;</span>, <span class="number">1</span>, <span class="number">60</span>]],</span><br><span class="line">                  columns=[<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;score&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&gt;&gt; 打印DataFrame:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">Spark_df = spark.createDataFrame(df)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&gt;&gt; 打印SparkDataFrame:&quot;</span>)</span><br><span class="line">Spark_df.show()</span><br><span class="line"><span class="comment"># &gt;&gt; 打印DataFrame:</span></span><br><span class="line"><span class="comment">#     name  age  score</span></span><br><span class="line"><span class="comment"># 0    Sam   28     88</span></span><br><span class="line"><span class="comment"># 1  Flora   28     90</span></span><br><span class="line"><span class="comment"># 2    Run    1     60</span></span><br><span class="line"><span class="comment"># &gt;&gt; 打印SparkDataFrame:</span></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br><span class="line"><span class="comment"># | name|age|score|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br></pre></td></tr></table></figure>
<h3 id="使用List来创建">使用List来创建</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">list_values = [[<span class="string">&#x27;Sam&#x27;</span>, <span class="number">28</span>, <span class="number">88</span>], [<span class="string">&#x27;Flora&#x27;</span>, <span class="number">28</span>, <span class="number">90</span>], [<span class="string">&#x27;Run&#x27;</span>, <span class="number">1</span>, <span class="number">60</span>]]</span><br><span class="line">Spark_df = spark.createDataFrame(list_values, [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;score&#x27;</span>])</span><br><span class="line">Spark_df.show()</span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br><span class="line"><span class="comment"># | name|age|score|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br></pre></td></tr></table></figure>
<h3 id="读取数据文件来创建">读取数据文件来创建</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CSV文件</span></span><br><span class="line">df = spark.read.option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;delimiter&quot;</span>, <span class="string">&quot;,&quot;</span>) \</span><br><span class="line">    .csv(<span class="string">&quot;./samples/breast_homo_test.csv&quot;</span>)</span><br><span class="line">df.show(<span class="number">5</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># json文件</span></span><br><span class="line">df = spark.read.json(<span class="string">&quot;./samples/zipcodes.json&quot;</span>)</span><br><span class="line">df.show(<span class="number">5</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line">df = spark.read.option(<span class="string">&quot;multiline&quot;</span>, <span class="string">&quot;true&quot;</span>).json(<span class="string">&quot;./samples/multiline-zipcode.json&quot;</span>)</span><br><span class="line">df.show(<span class="number">5</span>)</span><br><span class="line">df.printSchema()</span><br></pre></td></tr></table></figure>
<h3 id="通过读取数据库来创建">通过读取数据库来创建</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取hive数据</span></span><br><span class="line">spark.sql(<span class="string">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;LOAD DATA LOCAL INPATH &#x27;data/kv1.txt&#x27; INTO TABLE src&quot;</span>)</span><br><span class="line">df = spark.sql(<span class="string">&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;</span>)</span><br><span class="line">df.show(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取mysql数据</span></span><br><span class="line">url = <span class="string">&quot;jdbc:mysql://localhost:3306/test&quot;</span></span><br><span class="line">df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;jdbc&quot;</span>) \</span><br><span class="line"> .option(<span class="string">&quot;url&quot;</span>, url) \</span><br><span class="line"> .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;runoob_tbl&quot;</span>) \</span><br><span class="line"> .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>) \</span><br><span class="line"> .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;8888&quot;</span>) \</span><br><span class="line"> .load()\</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>
<h2 id="常用的SparkDataFrame-API">常用的SparkDataFrame API</h2>
<p>这里我大概是分成了几部分来看这些APIs，分别是查看DataFrame的APIs、简单处理DataFrame的APIs、DataFrame的列操作APIs、DataFrame的一些思路变换操作APIs、DataFrame的一些统计操作APIs，这样子也有助于我们了解这些API的功能，以后遇见实际问题的时候可以解决。</p>
<p>首先我们这小节全局用到的数据集如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="comment"># SparkSQL的许多功能封装在SparkSession的方法接口中, SparkContext则不行的。</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">&quot;test_spark_app&quot;</span>) \</span><br><span class="line">    .config(<span class="string">&quot;master&quot;</span>, <span class="string">&quot;local[4]&quot;</span>) \</span><br><span class="line">    .enableHiveSupport() \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个SparkDataFrame</span></span><br><span class="line">rdd = sc.parallelize([(<span class="string">&quot;Sam&quot;</span>, <span class="number">28</span>, <span class="number">88</span>, <span class="string">&quot;M&quot;</span>),</span><br><span class="line">                      (<span class="string">&quot;Flora&quot;</span>, <span class="number">28</span>, <span class="number">90</span>, <span class="string">&quot;F&quot;</span>),</span><br><span class="line">                      (<span class="string">&quot;Run&quot;</span>, <span class="number">1</span>, <span class="number">60</span>, <span class="literal">None</span>),</span><br><span class="line">                      (<span class="string">&quot;Peter&quot;</span>, <span class="number">55</span>, <span class="number">100</span>, <span class="string">&quot;M&quot;</span>),</span><br><span class="line">                      (<span class="string">&quot;Mei&quot;</span>, <span class="number">54</span>, <span class="number">95</span>, <span class="string">&quot;F&quot;</span>)])</span><br><span class="line">df = rdd.toDF([<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>, <span class="string">&quot;score&quot;</span>, <span class="string">&quot;sex&quot;</span>])</span><br><span class="line">df.show()</span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"><span class="comment"># | name|age|score| sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|   M|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|   F|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|null|</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|   M|</span></span><br><span class="line"><span class="comment"># |  Mei| 54|   95|   F|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment">#  |-- name: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- score: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- sex: string (nullable = true)</span></span><br></pre></td></tr></table></figure>
<h3 id="查看DataFrame的APIs">查看DataFrame的APIs</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame.collect</span></span><br><span class="line"><span class="comment"># 以列表形式返回行</span></span><br><span class="line">df.collect()</span><br><span class="line"><span class="comment"># [Row(name=&#x27;Sam&#x27;, age=28, score=88, sex=&#x27;M&#x27;),</span></span><br><span class="line"><span class="comment"># Row(name=&#x27;Flora&#x27;, age=28, score=90, sex=&#x27;F&#x27;),</span></span><br><span class="line"><span class="comment"># Row(name=&#x27;Run&#x27;, age=1, score=60, sex=None),</span></span><br><span class="line"><span class="comment"># Row(name=&#x27;Peter&#x27;, age=55, score=100, sex=&#x27;M&#x27;),</span></span><br><span class="line"><span class="comment"># Row(name=&#x27;Mei&#x27;, age=54, score=95, sex=&#x27;F&#x27;)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.count</span></span><br><span class="line">df.count()</span><br><span class="line"><span class="comment"># 5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.columns</span></span><br><span class="line">df.columns</span><br><span class="line"><span class="comment"># [&#x27;name&#x27;, &#x27;age&#x27;, &#x27;score&#x27;, &#x27;sex&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.dtypes</span></span><br><span class="line">df.dtypes</span><br><span class="line"><span class="comment"># [(&#x27;name&#x27;, &#x27;string&#x27;), (&#x27;age&#x27;, &#x27;bigint&#x27;), (&#x27;score&#x27;, &#x27;bigint&#x27;), (&#x27;sex&#x27;, &#x27;string&#x27;)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.describe</span></span><br><span class="line"><span class="comment"># 返回列的基础统计信息</span></span><br><span class="line">df.describe([<span class="string">&#x27;age&#x27;</span>]).show()</span><br><span class="line"><span class="comment"># +-------+------------------+</span></span><br><span class="line"><span class="comment"># |summary|               age|</span></span><br><span class="line"><span class="comment"># +-------+------------------+</span></span><br><span class="line"><span class="comment"># |  count|                 5|</span></span><br><span class="line"><span class="comment"># |   mean|              33.2|</span></span><br><span class="line"><span class="comment"># | stddev|22.353970564532826|</span></span><br><span class="line"><span class="comment"># |    min|                 1|</span></span><br><span class="line"><span class="comment"># |    max|                55|</span></span><br><span class="line"><span class="comment"># +-------+------------------+</span></span><br><span class="line">df.describe().show()</span><br><span class="line"><span class="comment"># +-------+-----+------------------+------------------+----+</span></span><br><span class="line"><span class="comment"># |summary| name|               age|             score| sex|</span></span><br><span class="line"><span class="comment"># +-------+-----+------------------+------------------+----+</span></span><br><span class="line"><span class="comment"># |  count|    5|                 5|                 5|   4|</span></span><br><span class="line"><span class="comment"># |   mean| null|              33.2|              86.6|null|</span></span><br><span class="line"><span class="comment"># | stddev| null|22.353970564532826|15.582040944625966|null|</span></span><br><span class="line"><span class="comment"># |    min|Flora|                 1|                60|   F|</span></span><br><span class="line"><span class="comment"># |    max|  Sam|                55|               100|   M|</span></span><br><span class="line"><span class="comment"># +-------+-----+------------------+------------------+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.select</span></span><br><span class="line"><span class="comment"># 选定指定列并按照一定顺序呈现</span></span><br><span class="line">df.select(<span class="string">&quot;sex&quot;</span>, <span class="string">&quot;score&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.first</span></span><br><span class="line"><span class="comment"># DataFrame.head</span></span><br><span class="line"><span class="comment"># 查看第1条数据</span></span><br><span class="line">df.first()</span><br><span class="line"><span class="comment"># Row(name=&#x27;Sam&#x27;, age=28, score=88, sex=&#x27;M&#x27;)</span></span><br><span class="line">df.head(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># [Row(name=&#x27;Sam&#x27;, age=28, score=88, sex=&#x27;M&#x27;)]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.freqItems</span></span><br><span class="line"><span class="comment"># 查看指定列的枚举值</span></span><br><span class="line">df.freqItems([<span class="string">&quot;age&quot;</span>,<span class="string">&quot;sex&quot;</span>]).show()</span><br><span class="line"><span class="comment"># +---------------+-------------+</span></span><br><span class="line"><span class="comment"># |  age_freqItems|sex_freqItems|</span></span><br><span class="line"><span class="comment"># +---------------+-------------+</span></span><br><span class="line"><span class="comment"># |[55, 1, 28, 54]|      [M, F,]|</span></span><br><span class="line"><span class="comment"># +---------------+-------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.summary</span></span><br><span class="line">df.summary().show()</span><br><span class="line"><span class="comment"># +-------+-----+------------------+------------------+----+</span></span><br><span class="line"><span class="comment"># |summary| name|               age|             score| sex|</span></span><br><span class="line"><span class="comment"># +-------+-----+------------------+------------------+----+</span></span><br><span class="line"><span class="comment"># |  count|    5|                 5|                 5|   4|</span></span><br><span class="line"><span class="comment"># |   mean| null|              33.2|              86.6|null|</span></span><br><span class="line"><span class="comment"># | stddev| null|22.353970564532826|15.582040944625966|null|</span></span><br><span class="line"><span class="comment"># |    min|Flora|                 1|                60|   F|</span></span><br><span class="line"><span class="comment"># |    25%| null|                28|                88|null|</span></span><br><span class="line"><span class="comment"># |    50%| null|                28|                90|null|</span></span><br><span class="line"><span class="comment"># |    75%| null|                54|                95|null|</span></span><br><span class="line"><span class="comment"># |    max|  Sam|                55|               100|   M|</span></span><br><span class="line"><span class="comment"># +-------+-----+------------------+------------------+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.sample</span></span><br><span class="line"><span class="comment"># 按照一定规则从df随机抽样数据</span></span><br><span class="line">df.sample(<span class="number">0.5</span>).show()</span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"><span class="comment"># | name|age|score| sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|   M|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|null|</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|   M|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br></pre></td></tr></table></figure>
<h3 id="简单处理DataFrame的APIs">简单处理DataFrame的APIs</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame.distinct</span></span><br><span class="line"><span class="comment"># 对数据集进行去重</span></span><br><span class="line">df.distinct().show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.dropDuplicates</span></span><br><span class="line"><span class="comment"># 对指定列去重</span></span><br><span class="line">df.dropDuplicates([<span class="string">&quot;sex&quot;</span>]).show()</span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"><span class="comment"># | name|age|score| sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|   F|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|null|</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|   M|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.exceptAll</span></span><br><span class="line"><span class="comment"># DataFrame.subtract</span></span><br><span class="line"><span class="comment"># 根据指定的df对df进行去重</span></span><br><span class="line">df1 = spark.createDataFrame(</span><br><span class="line">        [(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>,  <span class="number">3</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>)], [<span class="string">&quot;C1&quot;</span>, <span class="string">&quot;C2&quot;</span>])</span><br><span class="line">df2 = spark.createDataFrame([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>)], [<span class="string">&quot;C1&quot;</span>, <span class="string">&quot;C2&quot;</span>])</span><br><span class="line">df3 = df1.exceptAll(df2)  <span class="comment"># 没有去重的功效</span></span><br><span class="line">df4 = df1.subtract(df2)  <span class="comment"># 有去重的奇效</span></span><br><span class="line">df1.show()</span><br><span class="line">df2.show()</span><br><span class="line">df3.show()</span><br><span class="line">df4.show()</span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># | C1| C2|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># |  a|  1|</span></span><br><span class="line"><span class="comment"># |  a|  1|</span></span><br><span class="line"><span class="comment"># |  b|  3|</span></span><br><span class="line"><span class="comment"># |  c|  4|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># | C1| C2|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># |  a|  1|</span></span><br><span class="line"><span class="comment"># |  b|  3|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># | C1| C2|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># |  a|  1|</span></span><br><span class="line"><span class="comment"># |  c|  4|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># | C1| C2|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># |  c|  4|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.intersectAll</span></span><br><span class="line"><span class="comment"># 返回两个DataFrame的交集</span></span><br><span class="line">df1 = spark.createDataFrame(</span><br><span class="line">        [(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>,  <span class="number">3</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>)], [<span class="string">&quot;C1&quot;</span>, <span class="string">&quot;C2&quot;</span>])</span><br><span class="line">df2 = spark.createDataFrame([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)], [<span class="string">&quot;C1&quot;</span>, <span class="string">&quot;C2&quot;</span>])</span><br><span class="line">df1.intersectAll(df2).show()</span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># | C1| C2|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># |  a|  1|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.drop</span></span><br><span class="line"><span class="comment"># 丢弃指定列</span></span><br><span class="line">df.drop(<span class="string">&#x27;age&#x27;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.withColumn</span></span><br><span class="line"><span class="comment"># 新增列</span></span><br><span class="line">df1 = df.withColumn(<span class="string">&quot;birth_year&quot;</span>, <span class="number">2021</span> - df.age)</span><br><span class="line">df1.show()</span><br><span class="line"><span class="comment"># +-----+---+-----+----+----------+</span></span><br><span class="line"><span class="comment"># | name|age|score| sex|birth_year|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+----------+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|   M|      1993|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|   F|      1993|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|null|      2020|</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|   M|      1966|</span></span><br><span class="line"><span class="comment"># |  Mei| 54|   95|   F|      1967|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+----------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.withColumnRenamed</span></span><br><span class="line"><span class="comment"># 重命名列名</span></span><br><span class="line">df1 = df.withColumnRenamed(<span class="string">&quot;sex&quot;</span>, <span class="string">&quot;gender&quot;</span>)</span><br><span class="line">df1.show()</span><br><span class="line"><span class="comment"># +-----+---+-----+------+</span></span><br><span class="line"><span class="comment"># | name|age|score|gender|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+------+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|     M|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|     F|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|  null|</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|     M|</span></span><br><span class="line"><span class="comment"># |  Mei| 54|   95|     F|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.dropna</span></span><br><span class="line"><span class="comment"># 丢弃空值，DataFrame.dropna(how=&#x27;any&#x27;, thresh=None, subset=None)</span></span><br><span class="line">df.dropna(how=<span class="string">&#x27;all&#x27;</span>, subset=[<span class="string">&#x27;sex&#x27;</span>]).show()</span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># | name|age|score|sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|  M|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|  F|</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|  M|</span></span><br><span class="line"><span class="comment"># |  Mei| 54|   95|  F|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.fillna</span></span><br><span class="line"><span class="comment"># 空值填充操作</span></span><br><span class="line">df1 = spark.createDataFrame(</span><br><span class="line">        [(<span class="string">&quot;a&quot;</span>, <span class="literal">None</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="literal">None</span>,  <span class="number">3</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>)], [<span class="string">&quot;C1&quot;</span>, <span class="string">&quot;C2&quot;</span>])</span><br><span class="line"><span class="comment"># df2 = df1.na.fill(&#123;&quot;C1&quot;: &quot;d&quot;, &quot;C2&quot;: 99&#125;)</span></span><br><span class="line">df2 = df1.fillna(&#123;<span class="string">&quot;C1&quot;</span>: <span class="string">&quot;d&quot;</span>, <span class="string">&quot;C2&quot;</span>: <span class="number">99</span>&#125;)</span><br><span class="line">df1.show()</span><br><span class="line">df2.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.filter</span></span><br><span class="line"><span class="comment"># 根据条件过滤</span></span><br><span class="line">df.<span class="built_in">filter</span>(df.age&gt;<span class="number">50</span>).show()</span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># | name|age|score|sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|  M|</span></span><br><span class="line"><span class="comment"># |  Mei| 54|   95|  F|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line">df.where(df.age==<span class="number">28</span>).show()</span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># | name|age|score|sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|  M|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|  F|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line">df.<span class="built_in">filter</span>(<span class="string">&quot;age&lt;18&quot;</span>).show()</span><br><span class="line"><span class="comment"># +----+---+-----+----+</span></span><br><span class="line"><span class="comment"># |name|age|score| sex|</span></span><br><span class="line"><span class="comment"># +----+---+-----+----+</span></span><br><span class="line"><span class="comment"># | Run|  1|   60|null|</span></span><br><span class="line"><span class="comment"># +----+---+-----+----+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.join</span></span><br><span class="line"><span class="comment"># 这个不用多解释了，直接上案例来看看具体的语法即可，DataFrame.join(other, on=None, how=None)</span></span><br><span class="line">df1 = spark.createDataFrame(</span><br><span class="line">        [(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>,  <span class="number">3</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>)], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;num1&quot;</span>])</span><br><span class="line">df2 = spark.createDataFrame([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>)], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;num2&quot;</span>])</span><br><span class="line">df1.join(df2, df1.<span class="built_in">id</span> == df2.<span class="built_in">id</span>, <span class="string">&#x27;left&#x27;</span>).select(df1.<span class="built_in">id</span>.alias(<span class="string">&quot;df1_id&quot;</span>),</span><br><span class="line">                                               df1.num1.alias(<span class="string">&quot;df1_num&quot;</span>),</span><br><span class="line">                                               df2.num2.alias(<span class="string">&quot;df2_num&quot;</span>)</span><br><span class="line">                                               ).sort([<span class="string">&quot;df1_id&quot;</span>], ascending=<span class="literal">False</span>)\</span><br><span class="line">    .show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.agg(*exprs)</span></span><br><span class="line"><span class="comment"># 聚合数据，可以写多个聚合方法，如果不写groupBy的话就是对整个DF进行聚合</span></span><br><span class="line"><span class="comment"># DataFrame.alias</span></span><br><span class="line"><span class="comment"># 设置列或者DataFrame别名</span></span><br><span class="line"><span class="comment"># DataFrame.groupBy</span></span><br><span class="line"><span class="comment"># 根据某几列进行聚合，如有多列用列表写在一起，如 df.groupBy([&quot;sex&quot;, &quot;age&quot;])</span></span><br><span class="line">df.groupBy(<span class="string">&quot;sex&quot;</span>).agg(F.<span class="built_in">min</span>(df.age).alias(<span class="string">&quot;最小年龄&quot;</span>),</span><br><span class="line">                      F.expr(<span class="string">&quot;avg(age)&quot;</span>).alias(<span class="string">&quot;平均年龄&quot;</span>),</span><br><span class="line">                      F.expr(<span class="string">&quot;collect_list(name)&quot;</span>).alias(<span class="string">&quot;姓名集合&quot;</span>)</span><br><span class="line">                      ).show()</span><br><span class="line"><span class="comment"># +----+--------+--------+------------+</span></span><br><span class="line"><span class="comment"># | sex|最小年龄|平均年龄|    姓名集合|</span></span><br><span class="line"><span class="comment"># +----+--------+--------+------------+</span></span><br><span class="line"><span class="comment"># |   F|      28|    41.0|[Flora, Mei]|</span></span><br><span class="line"><span class="comment"># |null|       1|     1.0|       [Run]|</span></span><br><span class="line"><span class="comment"># |   M|      28|    41.5|[Sam, Peter]|</span></span><br><span class="line"><span class="comment"># +----+--------+--------+------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.foreach</span></span><br><span class="line"><span class="comment"># 对每一行进行函数方法的应用</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">person</span>):</span><br><span class="line">    <span class="built_in">print</span>(person.name)</span><br><span class="line">df.foreach(f)</span><br><span class="line"><span class="comment"># Peter</span></span><br><span class="line"><span class="comment"># Run</span></span><br><span class="line"><span class="comment"># Sam</span></span><br><span class="line"><span class="comment"># Flora</span></span><br><span class="line"><span class="comment"># Mei</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.replace</span></span><br><span class="line"><span class="comment"># 修改df里的某些值</span></span><br><span class="line">df1 = df.na.replace(&#123;<span class="string">&quot;M&quot;</span>: <span class="string">&quot;Male&quot;</span>, <span class="string">&quot;F&quot;</span>: <span class="string">&quot;Female&quot;</span>&#125;)</span><br><span class="line">df1.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.union</span></span><br><span class="line"><span class="comment"># 相当于SQL里的union all操作</span></span><br><span class="line">df1 = spark.createDataFrame(</span><br><span class="line">        [(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>,  <span class="number">3</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>)], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;num&quot;</span>])</span><br><span class="line">df2 = spark.createDataFrame([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>)], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;num&quot;</span>])</span><br><span class="line">df1.union(df2).show()</span><br><span class="line">df1.unionAll(df2).show()</span><br><span class="line"><span class="comment"># 这里union没有去重，不知道为啥，有知道的朋友麻烦解释下，谢谢了。</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># | id|num|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># |  a|  1|</span></span><br><span class="line"><span class="comment"># |  d|  1|</span></span><br><span class="line"><span class="comment"># |  b|  3|</span></span><br><span class="line"><span class="comment"># |  c|  4|</span></span><br><span class="line"><span class="comment"># |  a|  1|</span></span><br><span class="line"><span class="comment"># |  b|  3|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.unionByName</span></span><br><span class="line"><span class="comment"># 根据列名来进行合并数据集</span></span><br><span class="line">df1 = spark.createDataFrame([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]], [<span class="string">&quot;col0&quot;</span>, <span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>])</span><br><span class="line">df2 = spark.createDataFrame([[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], [<span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>, <span class="string">&quot;col0&quot;</span>])</span><br><span class="line">df1.unionByName(df2).show()</span><br><span class="line"><span class="comment"># +----+----+----+</span></span><br><span class="line"><span class="comment"># |col0|col1|col2|</span></span><br><span class="line"><span class="comment"># +----+----+----+</span></span><br><span class="line"><span class="comment"># |   1|   2|   3|</span></span><br><span class="line"><span class="comment"># |   6|   4|   5|</span></span><br><span class="line"><span class="comment"># +----+----+----+</span></span><br></pre></td></tr></table></figure>
<h3 id="DataFrame的列操作APIs">DataFrame的列操作APIs</h3>
<p>这里主要针对的是列进行操作，比如说重命名、排序、空值判断、类型判断等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Column.alias(*alias, **kwargs)  <span class="comment"># 重命名列名</span></span><br><span class="line">Column.asc()  <span class="comment"># 按照列进行升序排序</span></span><br><span class="line">Column.desc()  <span class="comment"># 按照列进行降序排序</span></span><br><span class="line">Column.astype(dataType)  <span class="comment"># 类型转换</span></span><br><span class="line">Column.cast(dataType)  <span class="comment"># 强制转换类型</span></span><br><span class="line">Column.between(lowerBound, upperBound)  <span class="comment"># 返回布尔值，是否在指定区间范围内</span></span><br><span class="line">Column.contains(other)  <span class="comment"># 是否包含某个关键词</span></span><br><span class="line">Column.endswith(other)  <span class="comment"># 以什么结束的值，如 df.filter(df.name.endswith(&#x27;ice&#x27;)).collect()</span></span><br><span class="line">Column.isNotNull()  <span class="comment"># 筛选非空的行</span></span><br><span class="line">Column.isNull()</span><br><span class="line">Column.isin(*cols)  <span class="comment"># 返回包含某些值的行 df[df.name.isin(&quot;Bob&quot;, &quot;Mike&quot;)].collect()</span></span><br><span class="line">Column.like(other)  <span class="comment"># 返回含有关键词的行</span></span><br><span class="line">Column.when(condition, value)  <span class="comment"># 给True的赋值</span></span><br><span class="line">Column.otherwise(value)  <span class="comment"># 与when搭配使用，df.select(df.name, F.when(df.age &gt; 3, 1).otherwise(0)).show()</span></span><br><span class="line">Column.rlike(other)  <span class="comment"># 可以使用正则的匹配 df.filter(df.name.rlike(&#x27;ice$&#x27;)).collect()</span></span><br><span class="line">Column.startswith(other)  <span class="comment"># df.filter(df.name.startswith(&#x27;Al&#x27;)).collect()</span></span><br><span class="line">Column.substr(startPos, length)  <span class="comment"># df.select(df.name.substr(1, 3).alias(&quot;col&quot;)).collect()</span></span><br></pre></td></tr></table></figure>
<h3 id="DataFrame的一些思路变换操作APIs">DataFrame的一些思路变换操作APIs</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame.createOrReplaceGlobalTempView</span></span><br><span class="line"><span class="comment"># DataFrame.dropGlobalTempView</span></span><br><span class="line"><span class="comment"># 创建全局的试图，注册后可以使用sql语句来进行操作，生命周期取决于Spark application本身</span></span><br><span class="line">df.createOrReplaceGlobalTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;select * from global_temp.people where sex = &#x27;M&#x27; &quot;</span>).show()</span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># | name|age|score|sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|  M|</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|  M|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.createOrReplaceTempView</span></span><br><span class="line"><span class="comment"># DataFrame.dropTempView</span></span><br><span class="line"><span class="comment"># 创建本地临时试图，生命周期取决于用来创建此数据集的SparkSession</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;tmp_people&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;select * from tmp_people where sex = &#x27;F&#x27; &quot;</span>).show()</span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># | name|age|score|sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|  F|</span></span><br><span class="line"><span class="comment"># |  Mei| 54|   95|  F|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.cache\DataFrame.persist</span></span><br><span class="line"><span class="comment"># 可以把一些数据放入缓存中，default storage level (MEMORY_AND_DISK).</span></span><br><span class="line">df.cache()</span><br><span class="line">df.persist()</span><br><span class="line">df.unpersist()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.crossJoin</span></span><br><span class="line"><span class="comment"># 返回两个DataFrame的笛卡尔积关联的DataFrame</span></span><br><span class="line">df1 = df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;sex&quot;</span>)</span><br><span class="line">df2 = df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;sex&quot;</span>)</span><br><span class="line">df3 = df1.crossJoin(df2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;表1的记录数&quot;</span>, df1.count())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;表2的记录数&quot;</span>, df2.count())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;笛卡尔积后的记录数&quot;</span>, df3.count())</span><br><span class="line"><span class="comment"># 表1的记录数 5</span></span><br><span class="line"><span class="comment"># 表2的记录数 5</span></span><br><span class="line"><span class="comment"># 笛卡尔积后的记录数 25</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.toPandas</span></span><br><span class="line"><span class="comment"># 把SparkDataFrame转为 Pandas的DataFrame</span></span><br><span class="line">df.toPandas()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.rdd</span></span><br><span class="line"><span class="comment"># 把SparkDataFrame转为rdd，这样子可以用rdd的语法来操作数据</span></span><br><span class="line">df.rdd</span><br></pre></td></tr></table></figure>
<h3 id="DataFrame的一些统计操作APIs">DataFrame的一些统计操作APIs</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame.cov</span></span><br><span class="line"><span class="comment"># 计算指定两列的样本协方差</span></span><br><span class="line">df.cov(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;score&quot;</span>)</span><br><span class="line"><span class="comment"># 324.59999999999997</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.corr</span></span><br><span class="line"><span class="comment"># 计算指定两列的相关系数，DataFrame.corr(col1, col2, method=None)，目前method只支持Pearson相关系数</span></span><br><span class="line">df.corr(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;score&quot;</span>, method=<span class="string">&quot;pearson&quot;</span>)</span><br><span class="line"><span class="comment"># 0.9319004030498815</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.cube</span></span><br><span class="line"><span class="comment"># 创建多维度聚合的结果，通常用于分析数据，比如我们指定两个列进行聚合，比如name和age，那么这个函数返回的聚合结果会</span></span><br><span class="line"><span class="comment"># groupby(&quot;name&quot;, &quot;age&quot;)</span></span><br><span class="line"><span class="comment"># groupby(&quot;name&quot;)</span></span><br><span class="line"><span class="comment"># groupby(&quot;age&quot;)</span></span><br><span class="line"><span class="comment"># groupby(all)</span></span><br><span class="line"><span class="comment"># 四个聚合结果的union all 的结果</span></span><br><span class="line"></span><br><span class="line">df1 = df.<span class="built_in">filter</span>(df.name != <span class="string">&quot;Run&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df1.show())</span><br><span class="line">df1.cube(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;sex&quot;</span>).count().show()</span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># | name|age|score|sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|  M|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|  F|</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|  M|</span></span><br><span class="line"><span class="comment"># |  Mei| 54|   95|  F|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># cube 聚合之后的结果</span></span><br><span class="line"><span class="comment"># +-----+----+-----+</span></span><br><span class="line"><span class="comment"># | name| sex|count|</span></span><br><span class="line"><span class="comment"># +-----+----+-----+</span></span><br><span class="line"><span class="comment"># | null|   F|    2|</span></span><br><span class="line"><span class="comment"># | null|null|    4|</span></span><br><span class="line"><span class="comment"># |Flora|null|    1|</span></span><br><span class="line"><span class="comment"># |Peter|null|    1|</span></span><br><span class="line"><span class="comment"># | null|   M|    2|</span></span><br><span class="line"><span class="comment"># |Peter|   M|    1|</span></span><br><span class="line"><span class="comment"># |  Sam|   M|    1|</span></span><br><span class="line"><span class="comment"># |  Sam|null|    1|</span></span><br><span class="line"><span class="comment"># |  Mei|   F|    1|</span></span><br><span class="line"><span class="comment"># |  Mei|null|    1|</span></span><br><span class="line"><span class="comment"># |Flora|   F|    1|</span></span><br><span class="line"><span class="comment"># +-----+----+-----+</span></span><br></pre></td></tr></table></figure>
<h2 id="保存数据-写入数据库">保存数据/写入数据库</h2>
<p>这里的保存数据主要是保存到Hive中的栗子，主要包括了overwrite、append等方式。</p>
<h3 id="当结果集为SparkDataFrame的时候">当结果集为SparkDataFrame的时候</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> HiveContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf()\</span><br><span class="line">      .setAppName(<span class="string">&quot;test&quot;</span>)\</span><br><span class="line">      .<span class="built_in">set</span>(<span class="string">&quot;hive.exec.dynamic.partition.mode&quot;</span>, <span class="string">&quot;nonstrict&quot;</span>) <span class="comment"># 动态写入hive分区表</span></span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line">hc = HiveContext(sc)</span><br><span class="line">sc.setLogLevel(<span class="string">&quot;ERROR&quot;</span>)</span><br><span class="line">    </span><br><span class="line">list_values = [[<span class="string">&#x27;Sam&#x27;</span>, <span class="number">28</span>, <span class="number">88</span>], [<span class="string">&#x27;Flora&#x27;</span>, <span class="number">28</span>, <span class="number">90</span>], [<span class="string">&#x27;Run&#x27;</span>, <span class="number">1</span>, <span class="number">60</span>]]</span><br><span class="line">Spark_df = spark.createDataFrame(list_values, [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;score&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(Spark_df.show())</span><br><span class="line">save_table = <span class="string">&quot;tmp.samshare_pyspark_savedata&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式1:直接写入到Hive</span></span><br><span class="line">Spark_df.write.<span class="built_in">format</span>(<span class="string">&quot;hive&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).saveAsTable(save_table) <span class="comment"># 或者改成append模式</span></span><br><span class="line"><span class="built_in">print</span>(datetime.now().strftime(<span class="string">&quot;%y/%m/%d %H:%M:%S&quot;</span>), <span class="string">&quot;测试数据写入到表&quot;</span> + save_table)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2:注册为临时表，使用SparkSQL来写入分区表</span></span><br><span class="line">Spark_df.createOrReplaceTempView(<span class="string">&quot;tmp_table&quot;</span>)</span><br><span class="line">write_sql = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">insert overwrite table &#123;0&#125; partitions (pt_date=&#x27;&#123;1&#125;&#x27;)</span></span><br><span class="line"><span class="string">select * from tmp_table</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>.<span class="built_in">format</span>(save_table, <span class="string">&quot;20210520&quot;</span>)</span><br><span class="line">hc.sql(write_sql)</span><br><span class="line"><span class="built_in">print</span>(datetime.now().strftime(<span class="string">&quot;%y/%m/%d %H:%M:%S&quot;</span>), <span class="string">&quot;测试数据写入到表&quot;</span> + save_table)</span><br></pre></td></tr></table></figure>
<h3 id="当结果集为Python的DataFrame的时候">当结果集为Python的DataFrame的时候</h3>
<p>如果是Python的DataFrame，我们就需要多做一步把它转换为SparkDataFrame，其余操作就一样了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> HiveContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf()\</span><br><span class="line">      .setAppName(<span class="string">&quot;test&quot;</span>)\</span><br><span class="line">      .<span class="built_in">set</span>(<span class="string">&quot;hive.exec.dynamic.partition.mode&quot;</span>, <span class="string">&quot;nonstrict&quot;</span>) <span class="comment"># 动态写入hive分区表</span></span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line">hc = HiveContext(sc)</span><br><span class="line">sc.setLogLevel(<span class="string">&quot;ERROR&quot;</span>)</span><br><span class="line">    </span><br><span class="line">result_df = pd.DataFrame([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], columns=[<span class="string">&#x27;a&#x27;</span>])</span><br><span class="line">save_table = <span class="string">&quot;tmp.samshare_pyspark_savedata&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取DataFrame的schema</span></span><br><span class="line">c1 = <span class="built_in">list</span>(result_df.columns)</span><br><span class="line"><span class="comment"># 转为SparkDataFrame</span></span><br><span class="line">result = hc.createDataFrame(result_df.astype(<span class="built_in">str</span>), c1)</span><br><span class="line">result.write.<span class="built_in">format</span>(<span class="string">&quot;hive&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).saveAsTable(save_table) <span class="comment"># 或者改成append模式</span></span><br><span class="line"><span class="built_in">print</span>(datetime.now().strftime(<span class="string">&quot;%y/%m/%d %H:%M:%S&quot;</span>), <span class="string">&quot;测试数据写入到表&quot;</span> + save_table)</span><br></pre></td></tr></table></figure>
<h2 id="Spark-调优思路">Spark 调优思路</h2>
<p><img src="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/%E8%B0%83%E4%BC%98%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.jpg" alt="调优"></p>
<h3 id="开发习惯优化">开发习惯优化</h3>
<h4 id="尽可能使用同一RDD，避免重复创建，并且适当持久化数据">尽可能使用同一RDD，避免重复创建，并且适当持久化数据</h4>
<p>这种开发习惯是需要我们对于即将要开发的应用逻辑有比较深刻的思考，并且可以通过code review来发现的，讲白了就是要记得我们创建过啥数据集，可以复用的尽量广播（broadcast）下，能很好提升性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最低级写法，相同数据集重复创建。</span></span><br><span class="line">rdd1 = sc.textFile(<span class="string">&quot;./test/data/hello_samshare.txt&quot;</span>, <span class="number">4</span>) <span class="comment"># 这里的 4 指的是分区数量</span></span><br><span class="line">rdd2 = sc.textFile(<span class="string">&quot;./test/data/hello_samshare.txt&quot;</span>, <span class="number">4</span>) <span class="comment"># 这里的 4 指的是分区数量</span></span><br><span class="line"><span class="built_in">print</span>(rdd1.take(<span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(rdd2.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:x[<span class="number">0</span>:<span class="number">1</span>]).take(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 稍微进阶一些，复用相同数据集，但因中间结果没有缓存，数据会重复计算</span></span><br><span class="line">rdd1 = sc.textFile(<span class="string">&quot;./test/data/hello_samshare.txt&quot;</span>, <span class="number">4</span>) <span class="comment"># 这里的 4 指的是分区数量</span></span><br><span class="line"><span class="built_in">print</span>(rdd1.take(<span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(rdd1.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:x[<span class="number">0</span>:<span class="number">1</span>]).take(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相对比较高效，使用缓存来持久化数据</span></span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>), <span class="number">4</span>).cache()  <span class="comment"># 或者persist()</span></span><br><span class="line">rdd_map = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x*<span class="number">2</span>)</span><br><span class="line">rdd_reduce = rdd.reduce(<span class="keyword">lambda</span> x, y: x+y)</span><br><span class="line"><span class="built_in">print</span>(rdd_map.take(<span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(rdd_reduce)</span><br></pre></td></tr></table></figure>
<p>下面我们就来对比一下使用缓存能给我们的Spark程序带来多大的效率提升吧，我们先构造一个程序运行时长测量器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># 统计程序运行时间</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">time_me</span>(<span class="params">info=<span class="string">&quot;used&quot;</span></span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_time_me</span>(<span class="params">fn</span>):</span><br><span class="line"><span class="meta">        @functools.wraps(<span class="params">fn</span>)</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">            start = time.time()</span><br><span class="line">            fn(*args, **kwargs)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;%s %s %s&quot;</span> % (fn.__name__, info, time.time() - start), <span class="string">&quot;second&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> _wrapper</span><br><span class="line">    <span class="keyword">return</span> _time_me</span><br></pre></td></tr></table></figure>
<p>下面我们运行下面的代码，看下使用了cache带来的效率提升：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@time_me()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">types=<span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> types == <span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;使用持久化缓存&quot;</span>)</span><br><span class="line">        rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10000000</span>), <span class="number">4</span>)</span><br><span class="line">        rdd1 = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x*x + <span class="number">2</span>*x + <span class="number">1</span>).cache()  <span class="comment"># 或者 persist(StorageLevel.MEMORY_AND_DISK_SER)</span></span><br><span class="line">        <span class="built_in">print</span>(rdd1.take(<span class="number">10</span>))</span><br><span class="line">        rdd2 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x+y)</span><br><span class="line">        rdd3 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">        rdd4 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">        rdd5 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">        <span class="built_in">print</span>(rdd5)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;不使用持久化缓存&quot;</span>)</span><br><span class="line">        rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10000000</span>), <span class="number">4</span>)</span><br><span class="line">        rdd1 = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x * x + <span class="number">2</span> * x + <span class="number">1</span>)</span><br><span class="line">        <span class="built_in">print</span>(rdd1.take(<span class="number">10</span>))</span><br><span class="line">        rdd2 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">        rdd3 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">        rdd4 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">        rdd5 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">        <span class="built_in">print</span>(rdd5)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">test()   <span class="comment"># 不使用持久化缓存</span></span><br><span class="line">time.sleep(<span class="number">10</span>)</span><br><span class="line">test(<span class="number">1</span>)  <span class="comment"># 使用持久化缓存</span></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># 使用持久化缓存</span></span><br><span class="line"><span class="comment"># [4, 9, 16, 25, 36, 49, 64, 81, 100, 121]</span></span><br><span class="line"><span class="comment"># 333333383333334999999</span></span><br><span class="line"><span class="comment"># test used 26.36529278755188 second</span></span><br><span class="line"><span class="comment"># 使用持久化缓存</span></span><br><span class="line"><span class="comment"># [4, 9, 16, 25, 36, 49, 64, 81, 100, 121]</span></span><br><span class="line"><span class="comment"># 333333383333334999999</span></span><br><span class="line"><span class="comment"># test used 17.49532413482666 second</span></span><br></pre></td></tr></table></figure>
<p>因为我们的代码是需要重复调用RDD1的，当没有对RDD1进行持久化的时候，每次当它被action算子消费了之后，就释放了，等下一个算子计算的时候要用，就从头开始计算一下RDD1。代码中需要重复调用RDD1 五次，所以没有缓存的话，差不多每次都要6秒，总共需要耗时26秒左右，但是，做了缓存，每次就只需要3s不到，总共需要耗时17秒左右。</p>
<p>另外，这里需要提及一下一个知识点，那就是持久化的级别，一般cache的话就是放入内存中，就没有什么好说的，需要讲一下的就是另外一个 persist()，它的持久化级别是可以被我们所配置的：</p>
<table>
<thead>
<tr>
<th>存储级别</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>MEMORY_ONLY</td>
<td>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。</td>
</tr>
<tr>
<td>MEMORY_AND_DISK</td>
<td>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</td>
</tr>
<tr>
<td>MEMORY_ONLY_SER</td>
<td>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_SER</td>
<td>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</td>
</tr>
<tr>
<td>MEMORY_ONLY_2，MEMORY_AND_DISK_2，等等</td>
<td>对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。</td>
</tr>
</tbody>
</table>
<h4 id="尽量避免使用低性能算子">尽量避免使用低性能算子</h4>
<p>shuffle类算子算是低性能算子的一种代表，如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。</p>
<p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</p>
<p>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p>
<h6 id="Broadcast与map进行join代码示例">Broadcast与map进行join代码示例</h6>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统的join操作会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Broadcast+map的join操作，不会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></span><br><span class="line"><span class="keyword">val</span> rdd2Data = rdd2.collect()</span><br><span class="line"><span class="keyword">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></span><br><span class="line"><span class="comment">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span></span><br><span class="line"><span class="comment">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></span><br><span class="line"><span class="comment">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原则2：尽量避免使用低性能算子</span></span><br><span class="line">rdd1 = sc.parallelize([(<span class="string">&#x27;A1&#x27;</span>, <span class="number">211</span>), (<span class="string">&#x27;A1&#x27;</span>, <span class="number">212</span>), (<span class="string">&#x27;A2&#x27;</span>, <span class="number">22</span>), (<span class="string">&#x27;A4&#x27;</span>, <span class="number">24</span>), (<span class="string">&#x27;A5&#x27;</span>, <span class="number">25</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&#x27;A1&#x27;</span>, <span class="number">11</span>), (<span class="string">&#x27;A2&#x27;</span>, <span class="number">12</span>), (<span class="string">&#x27;A3&#x27;</span>, <span class="number">13</span>), (<span class="string">&#x27;A4&#x27;</span>, <span class="number">14</span>)])</span><br><span class="line"><span class="comment"># 低效的写法，也是传统的写法，直接join</span></span><br><span class="line">rdd_join = rdd1.join(rdd2)</span><br><span class="line"><span class="built_in">print</span>(rdd_join.collect())</span><br><span class="line"><span class="comment"># [(&#x27;A4&#x27;, (24, 14)), (&#x27;A2&#x27;, (22, 12)), (&#x27;A1&#x27;, (211, 11)), (&#x27;A1&#x27;, (212, 11))]</span></span><br><span class="line">rdd_left_join = rdd1.leftOuterJoin(rdd2)</span><br><span class="line"><span class="built_in">print</span>(rdd_left_join.collect())</span><br><span class="line"><span class="comment"># [(&#x27;A4&#x27;, (24, 14)), (&#x27;A2&#x27;, (22, 12)), (&#x27;A5&#x27;, (25, None)), (&#x27;A1&#x27;, (211, 11)), (&#x27;A1&#x27;, (212, 11))]</span></span><br><span class="line">rdd_full_join = rdd1.fullOuterJoin(rdd2)</span><br><span class="line"><span class="built_in">print</span>(rdd_full_join.collect())</span><br><span class="line"><span class="comment"># [(&#x27;A4&#x27;, (24, 14)), (&#x27;A3&#x27;, (None, 13)), (&#x27;A2&#x27;, (22, 12)), (&#x27;A5&#x27;, (25, None)), (&#x27;A1&#x27;, (211, 11)), (&#x27;A1&#x27;, (212, 11))]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 高效的写法，使用广播+map来实现相同效果</span></span><br><span class="line"><span class="comment"># tips1: 这里需要注意的是，用来broadcast的RDD不可以太大，最好不要超过1G</span></span><br><span class="line"><span class="comment"># tips2: 这里需要注意的是，用来broadcast的RDD不可以有重复的key的</span></span><br><span class="line">rdd1 = sc.parallelize([(<span class="string">&#x27;A1&#x27;</span>, <span class="number">11</span>), (<span class="string">&#x27;A2&#x27;</span>, <span class="number">12</span>), (<span class="string">&#x27;A3&#x27;</span>, <span class="number">13</span>), (<span class="string">&#x27;A4&#x27;</span>, <span class="number">14</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&#x27;A1&#x27;</span>, <span class="number">211</span>), (<span class="string">&#x27;A1&#x27;</span>, <span class="number">212</span>), (<span class="string">&#x27;A2&#x27;</span>, <span class="number">22</span>), (<span class="string">&#x27;A4&#x27;</span>, <span class="number">24</span>), (<span class="string">&#x27;A5&#x27;</span>, <span class="number">25</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># step1： 先将小表进行广播，也就是collect到driver端，然后广播到每个Executor中去。</span></span><br><span class="line">rdd_small_bc = sc.broadcast(rdd1.collect())</span><br><span class="line"></span><br><span class="line"><span class="comment"># step2：从Executor中获取存入字典便于后续map操作</span></span><br><span class="line">rdd_small_dict = <span class="built_in">dict</span>(rdd_small_bc.value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step3：定义join方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">broadcast_join</span>(<span class="params">line, rdd_small_dict, join_type</span>):</span><br><span class="line">    k = line[<span class="number">0</span>]</span><br><span class="line">    v = line[<span class="number">1</span>]</span><br><span class="line">    small_table_v = rdd_small_dict[k] <span class="keyword">if</span> k <span class="keyword">in</span> rdd_small_dict <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> join_type == <span class="string">&#x27;join&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> (k, (v, small_table_v)) <span class="keyword">if</span> k <span class="keyword">in</span> rdd_small_dict <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">elif</span> join_type == <span class="string">&#x27;left_join&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> (k, (v, small_table_v <span class="keyword">if</span> small_table_v <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;not support join type!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step4：使用 map 实现 两个表join的功能</span></span><br><span class="line">rdd_join = rdd2.<span class="built_in">map</span>(<span class="keyword">lambda</span> line: broadcast_join(line, rdd_small_dict, <span class="string">&quot;join&quot;</span>)).<span class="built_in">filter</span>(<span class="keyword">lambda</span> line: line <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)</span><br><span class="line">rdd_left_join = rdd2.<span class="built_in">map</span>(<span class="keyword">lambda</span> line: broadcast_join(line, rdd_small_dict, <span class="string">&quot;left_join&quot;</span>)).<span class="built_in">filter</span>(<span class="keyword">lambda</span> line: line <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)</span><br><span class="line"><span class="built_in">print</span>(rdd_join.collect())</span><br><span class="line"><span class="built_in">print</span>(rdd_left_join.collect())</span><br><span class="line"><span class="comment"># [(&#x27;A1&#x27;, (211, 11)), (&#x27;A1&#x27;, (212, 11)), (&#x27;A2&#x27;, (22, 12)), (&#x27;A4&#x27;, (24, 14))]</span></span><br><span class="line"><span class="comment"># [(&#x27;A1&#x27;, (211, 11)), (&#x27;A1&#x27;, (212, 11)), (&#x27;A2&#x27;, (22, 12)), (&#x27;A4&#x27;, (24, 14)), (&#x27;A5&#x27;, (25, None))]</span></span><br></pre></td></tr></table></figure>
<p>上面的RDD join被改写为 broadcast+map的PySpark版本实现，不过里面有两个点需要注意：</p>
<ul>
<li>tips1: 用来broadcast的RDD不可以太大，最好不要超过1G</li>
<li>tips2: 用来broadcast的RDD不可以有重复的key的</li>
</ul>
<h4 id="使用map-side预聚合的shuffle操作">使用map-side预聚合的shuffle操作</h4>
<p>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。</p>
<p>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p>
<p>比如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p>
<p><img src="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/map-side-shuffle0.png" alt="groupByKey原理"><br>
<img src="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/map-side-shuffle1.png" alt="reduceByKey原理"></p>
<h4 id="尽量使用高性能算子">尽量使用高性能算子</h4>
<p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p>
<h5 id="使用reduceByKey-aggregateByKey替代groupByKey">使用reduceByKey/aggregateByKey替代groupByKey</h5>
<p>详情见：使用map-side预聚合的shuffle操作。</p>
<h5 id="使用mapPartitions替代普通map">使用mapPartitions替代普通map</h5>
<p>mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p>
<h5 id="使用foreachPartitions替代foreach">使用foreachPartitions替代foreach</h5>
<p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p>
<h5 id="使用filter之后进行coalesce操作">使用filter之后进行coalesce操作</h5>
<p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p>
<h5 id="使用repartitionAndSortWithinPartitions替代repartition与sort类操作">使用repartitionAndSortWithinPartitions替代repartition与sort类操作</h5>
<p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p>
<h4 id="广播大变量">广播大变量</h4>
<p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p>
<p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p>
<p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p>
<h5 id="python示例">python示例</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原则4：广播大变量</span></span><br><span class="line">rdd1 = sc.parallelize([(<span class="string">&#x27;A1&#x27;</span>, <span class="number">11</span>), (<span class="string">&#x27;A2&#x27;</span>, <span class="number">12</span>), (<span class="string">&#x27;A3&#x27;</span>, <span class="number">13</span>), (<span class="string">&#x27;A4&#x27;</span>, <span class="number">14</span>)])</span><br><span class="line">rdd1_broadcast = sc.broadcast(rdd1.collect())</span><br><span class="line"><span class="built_in">print</span>(rdd1.collect())</span><br><span class="line"><span class="built_in">print</span>(rdd1_broadcast.value)</span><br><span class="line"><span class="comment"># [(&#x27;A1&#x27;, 11), (&#x27;A2&#x27;, 12), (&#x27;A3&#x27;, 13), (&#x27;A4&#x27;, 14)]</span></span><br><span class="line"><span class="comment"># [(&#x27;A1&#x27;, 11), (&#x27;A2&#x27;, 12), (&#x27;A3&#x27;, 13), (&#x27;A4&#x27;, 14)]</span></span><br></pre></td></tr></table></figure>
<h5 id="scala示例">scala示例</h5>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 以下代码在算子函数中，使用了外部的变量。</span></span><br><span class="line"><span class="comment">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line">rdd1.map(list1...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下代码将list1封装成了Broadcast类型的广播变量。</span></span><br><span class="line"><span class="comment">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span></span><br><span class="line"><span class="comment">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span></span><br><span class="line"><span class="comment">// 每个Executor内存中，就只会驻留一份广播变量副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line"><span class="keyword">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class="line">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure>
<h4 id="使用Kryo优化序列化性能">使用Kryo优化序列化性能</h4>
<p>在Spark中，主要有三个地方涉及到了序列化：</p>
<ul>
<li>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见：广播大变量”中的讲解）。</li>
<li>将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。</li>
<li>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</li>
</ul>
<p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p>
<p>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建SparkConf对象。</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line"><span class="comment">// 设置序列化器为KryoSerializer。</span></span><br><span class="line">conf.set(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line"><span class="comment">// 注册要序列化的自定义类型。</span></span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br></pre></td></tr></table></figure>
<h4 id="Spark作业基本原理">Spark作业基本原理</h4>
<p>如果要进行资源调优，我们就必须先知道Spark运行的机制与流程。</p>
<p><img src="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/Spark%E4%BD%9C%E4%B8%9A%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86.png" alt="Spark作业基本运行原理"></p>
<p>详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。</p>
<p>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。**task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。**一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。</p>
<p>**Spark是根据shuffle类算子来进行stage的划分。**如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，<strong>它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。</strong></p>
<p>当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。</p>
<p>因此Executor的内存主要分为三块：</p>
<ul>
<li>第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；</li>
<li>第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；</li>
<li>第三块是让RDD持久化时使用，默认占Executor总内存的60%。</li>
</ul>
<p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</p>
<p>以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。</p>
<h4 id="资源参数调优">资源参数调优</h4>
<p>了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p>
<h5 id="num-executors">num-executors</h5>
<ul>
<li>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li>
<li>参数调优建议：每个Spark作业的运行一般设置<code>50~100</code>个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li>
</ul>
<h5 id="executor-memory">executor-memory</h5>
<ul>
<li>参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li>
<li>参数调优建议：每个Executor进程的内存设置<code>4G~8G</code>较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的<code>1/3~1/2</code>，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li>
</ul>
<h5 id="executor-cores">executor-cores</h5>
<ul>
<li>参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li>
<li>参数调优建议：Executor的CPU core数量设置为<code>2~4</code>个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的<code>1/3~1/2</code>左右比较合适，也是避免影响其它业务的作业运行。</li>
</ul>
<h5 id="driver-memory">driver-memory</h5>
<ul>
<li>参数说明：该参数用于设置Driver进程的内存。</li>
<li>参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</li>
</ul>
<h5 id="spark-default-parallelism">spark.default.parallelism</h5>
<ul>
<li>参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。</li>
<li>参数调优建议：Spark作业的默认task数量为<code>500~1000</code>个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的<code>2~3</code>倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</li>
</ul>
<h5 id="spark-storage-memoryFraction">spark.storage.memoryFraction</h5>
<ul>
<li>参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li>
<li>参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li>
</ul>
<h5 id="spark-shuffle-memoryFraction">spark.shuffle.memoryFraction</h5>
<ul>
<li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li>
<li>参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li>
</ul>
<blockquote>
<p>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</p>
</blockquote>
<h4 id="资源参数参考示例">资源参数参考示例</h4>
<p>以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --master yarn-cluster \</span><br><span class="line">  --num-executors 100 \</span><br><span class="line">  --executor-memory 6G \</span><br><span class="line">  --executor-cores 4 \</span><br><span class="line">  --driver-memory 1G \</span><br><span class="line">  --conf spark.default.parallelism=1000 \</span><br><span class="line">  --conf spark.storage.memoryFraction=0.5 \</span><br><span class="line">  --conf spark.shuffle.memoryFraction=0.3 \</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2023/10/19/Spark%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/10/19/Spark%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/" class="post-title-link" itemprop="url">Spark部署模式</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-10-19 17:00:27" itemprop="dateCreated datePublished" datetime="2023-10-19T17:00:27+08:00">2023-10-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-10-26 14:15:30" itemprop="dateModified" datetime="2023-10-26T14:15:30+08:00">2023-10-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>简介</h1>
<p><code>Spark Application</code>提交运行时的部署模式<code>Deploy Mode</code>，表示的是<code>Driver Program</code>运行的地方。</p>
<ul>
<li><code>client</code>:Driver Program 运行在<code>提交应用的Client</code>上</li>
<li><code>cluster</code>:要么就是集群中从节点<code>(Standalone：Worker，YARN：NodeManager)</code>。</li>
</ul>
<p>默认值为<code>client</code>，当在实际的开发环境中，尤其是生产环境，使用<code>cluster</code>部署模式提交应用运行。</p>
<h1>Client 模式</h1>
<p>以<code>Spark Application</code>运行到<code>Standalone</code>集群上为例，前面提交运行圆周率PI或者词频统计<code>WordCount程序</code>时，默认 <code>DeployMode</code>为<code>Client</code>，表示应用<code>Driver Program</code>运行在提交应用的<code>Client 主机</code>上（启动 JVM Process 进程），示意图如下：<br>
<img src="/2023/10/19/Spark%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/Deploy-Mode-Client.jpg" alt="Client模式"></p>
<h1>Cluster 模式</h1>
<p>如果采用cluster模式运行应用，应用Driver Program运行在集群从节点Worker某台机器上，示意图如下：<br>
<img src="/2023/10/19/Spark%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/Deploy-Mode-Cluster.jpg" alt="Cluster模式"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.kaizi.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于Scala语言使用SparkCore编程实现词频统计：WordCount</span></span><br><span class="line"><span class="comment"> * 从HDFS上读取数据，统计WordCount，将结果保存到HDFS上</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> 当应用运行在集群上的时候，MAIN函数就是Driver Program，必须创建SparkContext对象</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建SparkConf对象，设置应用的配置信息，比如应用名称和应用运行模式</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setMaster(<span class="string">&quot;local[2]&quot;</span>)  <span class="comment">// 设置运行本地模式</span></span><br><span class="line">      .setAppName(<span class="string">&quot;SparkWordCount&quot;</span>)</span><br><span class="line">    <span class="comment">// 构建SparkContext上下文实例对象，读取数据和调度Job执行</span></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ************第一步、读取数据:在Spark的Executor中执行***************</span></span><br><span class="line">    <span class="comment">// 封装到RDD集合，认为列表List</span></span><br><span class="line">    <span class="keyword">val</span> inputRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;/datas/wordcount.data&quot;</span>)</span><br><span class="line">    <span class="comment">// ***************************************************************</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// ************第二步、处理数据:在Spark的Executor中执行***************</span></span><br><span class="line">    <span class="comment">// 调用RDD中函数，认为调用列表中的函数</span></span><br><span class="line">    <span class="comment">// a. 每行数据分割为单词</span></span><br><span class="line">    <span class="keyword">val</span> wordsRDD = inputRDD.flatMap(line =&gt; line.split(<span class="string">&quot;\\s+&quot;</span>))</span><br><span class="line">    <span class="comment">// b. 转换为二元组，表示每个单词出现一次</span></span><br><span class="line">    <span class="keyword">val</span> tuplesRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordsRDD.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">// c. 按照Key分组聚合</span></span><br><span class="line">    <span class="keyword">val</span> wordCountsRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = tuplesRDD.reduceByKey((tmp, item) =&gt; tmp + item)</span><br><span class="line">    <span class="comment">// ***************************************************************</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// ************第三步、输出数据:在Spark的Executor中执行****************</span></span><br><span class="line">    wordCountsRDD.foreach(println)</span><br><span class="line">    <span class="comment">// 保存到为存储系统，比如HDFS</span></span><br><span class="line">    wordCountsRDD.saveAsTextFile(<span class="string">s&quot;/datas/swc-output-<span class="subst">$&#123;System.currentTimeMillis()&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment">// ***************************************************************</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 为了测试，线程休眠，查看WEB UI界面</span></span><br><span class="line">    <span class="type">Thread</span>.sleep(<span class="number">10000000</span>)</span><br><span class="line">    <span class="comment">// TODO：应用程序运行接收，关闭资源</span></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>main方法中一开始的创建SparkContext对象和最后的关闭SparkContext资源，都是在Driver Program中执行的，代码中的第一步加载数据、第二步处理数据、第三步输出数据都是在Executor上执行。</p>
</blockquote>
<p>综上所述Spark Application中Job执行有两个主要点：</p>
<ul>
<li>RDD输出函数分类两类
<ul>
<li>第一类：返回值给<code>Driver Progam</code>，比如<code>count</code>、<code>first</code>、<code>take</code>、<code>collect</code>等</li>
<li>第二类：没有返回值，比如直接打印结果、保存至外部存储系统（HDFS文件）等</li>
</ul>
</li>
<li>在<code>Job</code>中从读取数据封装为<code>RDD</code>和一切<code>RDD调用方法</code>都是在<code>Executor</code>中执行，其他代码都是在<code>Driver Program</code>中执行
<ul>
<li><code>SparkContext</code>创建与关闭、其他变量创建等在<code>Driver Program</code>中执行</li>
<li><code>RDD调用函数</code>都是在Executors中执行</li>
</ul>
</li>
</ul>
<h1>Client模式和Cluster模式的区别</h1>
<p>Cluster和Client模式最本质的区别是：Driver程序运行在哪里。</p>
<ul>
<li>cluster模式：生产环境中使用该模式
<ul>
<li>Driver程序在YARN集群当中</li>
<li>应用的运行结果不能在客户端显示</li>
</ul>
</li>
<li>client模式：学习测试时使用（也不一定，个人觉得生产环境也可以用）
<ul>
<li>Driver运行在Client上的SparkSubmit进程中</li>
<li>应用程序运行结果会在客户端显示</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2023/08/06/Linux%E7%B3%BB%E7%BB%9F%E4%B8%ADPycharm%E7%95%8C%E9%9D%A2%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/08/06/Linux%E7%B3%BB%E7%BB%9F%E4%B8%ADPycharm%E7%95%8C%E9%9D%A2%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">Linux系统中Pycharm界面显示问题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-06 19:07:22" itemprop="dateCreated datePublished" datetime="2023-08-06T19:07:22+08:00">2023-08-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-10-27 16:47:33" itemprop="dateModified" datetime="2023-10-27T16:47:33+08:00">2023-10-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>前言</h1>
<p>打开<code>Pycharm</code>，激活界面中的中文显示不了，因为缺少了字体，<code>文泉驿正黑</code>，我用的是<code>Fedora</code>，记录一下<code>Fedora</code>下安装<code>文泉驿正黑</code>这个字体</p>
<h1>安装<code>文泉驿正黑</code></h1>
<h2 id="安装">安装</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dnf install -y wqy-zenhei-fonts</span><br></pre></td></tr></table></figure>
<blockquote>
<p>也可以使用<code>Fedora</code>的软件包管理器安装<code>ttf-wqy-zenhei</code>字体包</p>
</blockquote>
<h2 id="刷新字体">刷新字体</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fc-cache -f -v</span><br></pre></td></tr></table></figure>
<h2 id="验证">验证</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fc-list | grep &quot;文泉驿正黑&quot;</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2023/07/14/Python%E4%B8%AD%E7%9A%84args%E5%92%8Ckwargs%E5%8F%82%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/14/Python%E4%B8%AD%E7%9A%84args%E5%92%8Ckwargs%E5%8F%82%E6%95%B0/" class="post-title-link" itemprop="url">Python中的args和kwargs参数</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-07-14 17:38:31 / 修改时间：17:54:20" itemprop="dateCreated datePublished" datetime="2023-07-14T17:38:31+08:00">2023-07-14</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">基础</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>*args &amp;&amp; **kwargs</h1>
<p>在Python项目的代码中经常会见到这两个词 args 和 kwargs，前面通常还会加上一个或者两个星号。其实这只是编程人员约定的变量名字，args 是 arguments 的缩写，表示位置参数；kwargs 是 keyword arguments 的缩写，表示关键字参数。这其实就是 Python 中可变参数的两种形式，并且 *args 必须放在 **kwargs 的前面，因为位置参数在关键字参数的前面。</p>
<h1>在函数定义中的用法</h1>
<h2 id="args的用法">*args的用法</h2>
<p>*args就是就是传递一个可变参数列表给函数实参，这个参数列表的数目未知，甚至长度可以为0。下面这段代码演示了如何使用args</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_args</span>(<span class="params">first, *args</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Required argument: &#x27;</span>, first)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(args))</span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> args:</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;Optional argument: &#x27;</span>, v)</span><br><span class="line"></span><br><span class="line">test_args(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>第一个参数是必须要传入的参数，所以使用了第一个形参，而后面三个参数则作为可变参数列表传入了实参，并且是作为元组tuple来使用的。代码的运行结果如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Required argument:  1</span><br><span class="line">&lt;class &#x27;tuple&#x27;&gt;</span><br><span class="line">Optional argument:  2</span><br><span class="line">Optional argument:  3</span><br><span class="line">Optional argument:  4</span><br></pre></td></tr></table></figure>
<h2 id="kwargs用法">**kwargs用法</h2>
<p>**kwargs则是将一个可变的关键字参数的字典传给函数实参，同样参数列表长度可以为0或为其他值。下面这段代码演示了如何使用kwargs：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_kwargs</span>(<span class="params">first, *args, **kwargs</span>):</span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&#x27;Required argument: &#x27;</span>, first)</span><br><span class="line">   <span class="built_in">print</span>(<span class="built_in">type</span>(kwargs))</span><br><span class="line">   <span class="keyword">for</span> v <span class="keyword">in</span> args:</span><br><span class="line">      <span class="built_in">print</span> (<span class="string">&#x27;Optional argument (args): &#x27;</span>, v)</span><br><span class="line">   <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">      <span class="built_in">print</span> (<span class="string">&#x27;Optional argument %s (kwargs): %s&#x27;</span> % (k, v))</span><br><span class="line"></span><br><span class="line">test_kwargs(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, k1=<span class="number">5</span>, k2=<span class="number">6</span>)</span><br></pre></td></tr></table></figure>
<p>正如前面所说的，args类型是一个tuple，而kwargs则是一个字典dict，并且args只能位于kwargs的前面。代码的运行结果如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Required argument:  1</span><br><span class="line">&lt;class &#x27;dict&#x27;&gt;</span><br><span class="line">Optional argument (args):  2</span><br><span class="line">Optional argument (args):  3</span><br><span class="line">Optional argument (args):  4</span><br><span class="line">Optional argument k2 (kwargs): 6</span><br><span class="line">Optional argument k1 (kwargs): 5</span><br></pre></td></tr></table></figure>
<h1>调用函数中的用法</h1>
<p>args和kwargs不仅可以在函数定义中使用，还可以在函数调用中使用。在调用时使用就相当于pack（打包）和unpack（解包），类似于元组的打包和解包。</p>
<p>首先来看一下使用args来解包调用函数的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_args_kwargs</span>(<span class="params">arg1, arg2, arg3</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;arg1:&quot;</span>, arg1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;arg2:&quot;</span>, arg2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;arg3:&quot;</span>, arg3)</span><br><span class="line"></span><br><span class="line">args = (<span class="string">&quot;two&quot;</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">test_args_kwargs(*args)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arg1: two</span><br><span class="line">arg2: 3</span><br><span class="line">arg3: 5</span><br></pre></td></tr></table></figure>
<blockquote>
<p>将元组解包后传给对应的实参，kwargs的用法与其类似。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kwargs = &#123;<span class="string">&quot;arg3&quot;</span>: <span class="number">3</span>, <span class="string">&quot;arg2&quot;</span>: <span class="string">&quot;two&quot;</span>, <span class="string">&quot;arg1&quot;</span>: <span class="number">5</span>&#125;</span><br><span class="line">test_args_kwargs(**kwargs)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arg1: 5</span><br><span class="line">arg2: two</span><br><span class="line">arg3: 3</span><br></pre></td></tr></table></figure>
<h1>总结</h1>
<p>args和kwargs组合起来可以传入任意的参数，这在参数未知的情况下是很有效的，同时加强了函数的可拓展性。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2023/07/14/Python%E4%B8%AD%E8%A3%85%E9%A5%B0%E5%99%A8%E7%9A%84wraps%E7%9A%84%E4%BD%9C%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/14/Python%E4%B8%AD%E8%A3%85%E9%A5%B0%E5%99%A8%E7%9A%84wraps%E7%9A%84%E4%BD%9C%E7%94%A8/" class="post-title-link" itemprop="url">Python中装饰器的wraps的作用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-07-14 15:50:52 / 修改时间：16:15:45" itemprop="dateCreated datePublished" datetime="2023-07-14T15:50:52+08:00">2023-07-14</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">基础</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>装饰器</h1>
<p><code>Python</code>装饰器<code>decorator</code>在实现的时候，被装饰后的函数其实已经是另外一个函数了（函数名等函数属性会发生改变），就相当于产生了<code>副作用</code>，<code>Python</code>的<code>functools</code>包中提供了一个叫<code>wraps</code>的<code>decorator</code>来消除这样的<code>副作用</code>。写一个<code>decorator</code>的时候，最好在实现之前加上<code>functools</code>的<code>wrap</code>，它能保留原有函数的<code>__name__</code>和<code>__doc__</code>。</p>
<h1>示例1（副作用）</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_decorator</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;decorator&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Calling decorated function...&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@my_decorator</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;DocString&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Called example function.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(example.__name__, example.__doc__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    example()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>打印现来的是：wrapper decorator 而不是 example DocString</p>
</blockquote>
<h1>示例2（消除副作用）</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_decorator</span>(<span class="params">func</span>):</span><br><span class="line"><span class="meta">    @wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Decorator&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Calling decorated function...&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@my_decorator</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;DocString&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Called example function&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    example()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>无毒副作用, 这就是<code>@wraps</code>的作用</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2023/06/30/%E6%80%8E%E4%B9%88%E6%8A%8AK8S%E4%B8%AD%E9%83%A8%E7%BD%B2%E7%9A%84%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E5%87%BA%E6%9D%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/30/%E6%80%8E%E4%B9%88%E6%8A%8AK8S%E4%B8%AD%E9%83%A8%E7%BD%B2%E7%9A%84%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E5%87%BA%E6%9D%A5/" class="post-title-link" itemprop="url">怎么把K8S中部署的服务端口映射出来</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-30 17:56:12 / 修改时间：17:59:22" itemprop="dateCreated datePublished" datetime="2023-06-30T17:56:12+08:00">2023-06-30</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/" itemprop="url" rel="index"><span itemprop="name">云原生</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/" itemprop="url" rel="index"><span itemprop="name">基础设施</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ol>
<li>
<p>修改fate-10001命名空间下的mysql的端口映射模式</p>
<ul>
<li>先查看fate-10001下有哪些服务</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc -n fate-10001</span><br></pre></td></tr></table></figure>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                               AGE</span><br><span class="line">spark-master         ClusterIP      None            &lt;none&gt;          8080/TCP,7077/TCP,6066/TCP            16d</span><br><span class="line">fateflow             ClusterIP      None            &lt;none&gt;          9360/TCP,9380/TCP                     16d</span><br><span class="line">spark-worker-1       ClusterIP      None            &lt;none&gt;          8081/TCP                              16d</span><br><span class="line">fateflow-client      ClusterIP      10.43.80.240    &lt;none&gt;          9360/TCP,9380/TCP                     16d</span><br><span class="line">datanode             ClusterIP      10.43.247.69    &lt;none&gt;          9000/TCP,9864/TCP                     16d</span><br><span class="line">frontend             NodePort       10.43.24.110    &lt;none&gt;          8080:31925/TCP,8443:31194/TCP         16d</span><br><span class="line">notebook             ClusterIP      10.43.41.5      &lt;none&gt;          20000/TCP                             16d</span><br><span class="line">fateboard            ClusterIP      10.43.3.44      &lt;none&gt;          8080/TCP                              16d</span><br><span class="line">namenode             ClusterIP      10.43.146.160   &lt;none&gt;          9000/TCP,9870/TCP                     16d</span><br><span class="line">pulsar               ClusterIP      10.43.42.170    &lt;none&gt;          6650/TCP,6651/TCP,8080/TCP,8081/TCP   16d</span><br><span class="line">postgres             ClusterIP      10.43.182.72    &lt;none&gt;          5432/TCP                              16d</span><br><span class="line">mysql                ClusterIP      10.43.42.233    &lt;none&gt;          3306/TCP                              16d</span><br><span class="line">nginx                NodePort       10.43.101.234   &lt;none&gt;          9300:31960/TCP,9310:32603/TCP         16d</span><br><span class="line">spark-client         ClusterIP      10.43.69.60     &lt;none&gt;          8080/TCP,7077/TCP,6066/TCP            16d</span><br><span class="line">site-portal-server   ClusterIP      10.43.237.196   &lt;none&gt;          8080/TCP,8443/TCP                     16d</span><br><span class="line">pulsar-public-tls    LoadBalancer   10.43.144.61    192.168.11.71   6651:32593/TCP                        16d</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意msyql 的type 是 ClusterIP，这样就只能被k8s集群内部访问而不能被外部访问到，所以这里要把它改为NodePort模式</p>
</blockquote>
<ul>
<li>修改mysql的type</li>
</ul>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit svc mysql -n fate-10001</span><br></pre></td></tr></table></figure>
 <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">meta.helm.sh/release-name:</span> <span class="string">party10001</span></span><br><span class="line">    <span class="attr">meta.helm.sh/release-namespace:</span> <span class="string">fate-10001</span></span><br><span class="line"><span class="attr">creationTimestamp:</span> <span class="string">&quot;2023-06-13T05:46:12Z&quot;</span></span><br><span class="line"><span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/managed-by:</span> <span class="string">Helm</span></span><br><span class="line">    <span class="attr">chart:</span> <span class="string">fate</span></span><br><span class="line">    <span class="attr">cluster:</span> <span class="string">fate</span></span><br><span class="line">    <span class="attr">fateMoudle:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="attr">heritage:</span> <span class="string">Helm</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">party10001</span></span><br><span class="line">    <span class="attr">owner:</span> <span class="string">kubefate</span></span><br><span class="line">    <span class="attr">partyId:</span> <span class="string">&quot;10001&quot;</span></span><br><span class="line">    <span class="attr">release:</span> <span class="string">party10001</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">mysql</span></span><br><span class="line"><span class="attr">namespace:</span> <span class="string">fate-10001</span></span><br><span class="line"><span class="attr">resourceVersion:</span> <span class="string">&quot;33441&quot;</span></span><br><span class="line"><span class="attr">uid:</span> <span class="string">08d5e8fe-7853-4f85-b17c-9315afdd6055</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">clusterIP:</span> <span class="number">10.43</span><span class="number">.42</span><span class="number">.233</span></span><br><span class="line"><span class="attr">clusterIPs:</span></span><br><span class="line"><span class="bullet">-</span> <span class="number">10.43</span><span class="number">.42</span><span class="number">.233</span></span><br><span class="line"><span class="attr">internalTrafficPolicy:</span> <span class="string">Cluster</span></span><br><span class="line"><span class="attr">ipFamilies:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">IPv4</span></span><br><span class="line"><span class="attr">ipFamilyPolicy:</span> <span class="string">SingleStack</span></span><br><span class="line"><span class="attr">ports:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tcp-mysql</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">3306</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">3306</span></span><br><span class="line"><span class="attr">selector:</span></span><br><span class="line">    <span class="attr">fateMoudle:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">party10001</span></span><br><span class="line">    <span class="attr">partyId:</span> <span class="string">&quot;10001&quot;</span></span><br><span class="line"><span class="attr">sessionAffinity:</span> <span class="string">None</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line"><span class="attr">loadBalancer:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>
<p>修改后的内容为：</p>
 <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">meta.helm.sh/release-name:</span> <span class="string">party10001</span></span><br><span class="line">    <span class="attr">meta.helm.sh/release-namespace:</span> <span class="string">fate-10001</span></span><br><span class="line"><span class="attr">creationTimestamp:</span> <span class="string">&quot;2023-06-13T05:46:12Z&quot;</span></span><br><span class="line"><span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/managed-by:</span> <span class="string">Helm</span></span><br><span class="line">    <span class="attr">chart:</span> <span class="string">fate</span></span><br><span class="line">    <span class="attr">cluster:</span> <span class="string">fate</span></span><br><span class="line">    <span class="attr">fateMoudle:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="attr">heritage:</span> <span class="string">Helm</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">party10001</span></span><br><span class="line">    <span class="attr">owner:</span> <span class="string">kubefate</span></span><br><span class="line">    <span class="attr">partyId:</span> <span class="string">&quot;10001&quot;</span></span><br><span class="line">    <span class="attr">release:</span> <span class="string">party10001</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">mysql</span></span><br><span class="line"><span class="attr">namespace:</span> <span class="string">fate-10001</span></span><br><span class="line"><span class="attr">resourceVersion:</span> <span class="string">&quot;33441&quot;</span></span><br><span class="line"><span class="attr">uid:</span> <span class="string">08d5e8fe-7853-4f85-b17c-9315afdd6055</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">clusterIP:</span> <span class="number">10.43</span><span class="number">.42</span><span class="number">.233</span></span><br><span class="line"><span class="attr">clusterIPs:</span></span><br><span class="line"><span class="bullet">-</span> <span class="number">10.43</span><span class="number">.42</span><span class="number">.233</span></span><br><span class="line"><span class="attr">internalTrafficPolicy:</span> <span class="string">Cluster</span></span><br><span class="line"><span class="attr">externalTrafficPolicy:</span> <span class="string">Cluster</span></span><br><span class="line"><span class="attr">ipFamilies:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">IPv4</span></span><br><span class="line"><span class="attr">ipFamilyPolicy:</span> <span class="string">SingleStack</span></span><br><span class="line"><span class="attr">ports:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tcp-mysql</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">3306</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">3306</span></span><br><span class="line">    <span class="attr">nodePort:</span> <span class="number">31306</span></span><br><span class="line"><span class="attr">selector:</span></span><br><span class="line">    <span class="attr">fateMoudle:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">party10001</span></span><br><span class="line">    <span class="attr">partyId:</span> <span class="string">&quot;10001&quot;</span></span><br><span class="line"><span class="attr">sessionAffinity:</span> <span class="string">None</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line"><span class="attr">loadBalancer:</span> &#123;&#125;</span><br></pre></td></tr></table></figure></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2023/06/16/vim%E5%B8%B8%E7%94%A8%E6%8F%92%E4%BB%B6%E7%9A%84%E4%BD%BF%E7%94%A8%E5%A4%87%E5%BF%98%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/16/vim%E5%B8%B8%E7%94%A8%E6%8F%92%E4%BB%B6%E7%9A%84%E4%BD%BF%E7%94%A8%E5%A4%87%E5%BF%98%E5%BD%95/" class="post-title-link" itemprop="url">vim常用插件的使用备忘录</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-16 20:50:11 / 修改时间：20:55:27" itemprop="dateCreated datePublished" datetime="2023-06-16T20:50:11+08:00">2023-06-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1><code>vim-surround</code> 的用法</h1>
<ul>
<li><code>cs'&quot;</code>用双引号替换单引号</li>
<li><code>cs&quot;'</code>用单引号替换双引号</li>
<li><code>ysiw&lt;q&gt;</code>给当前单词加上标签<code>&lt;q&gt;</code></li>
<li><code>cst&quot;</code>用双引号替换当前的标签</li>
<li><code>ds'</code>删除单引号</li>
<li><code>ysiw]</code>在当前单词外加上方括号</li>
<li><code>yss]</code>在当前句子处加上方括号</li>
</ul>
<h1>vim编辑<code>markdown</code>在浏览器中预览效果</h1>
<ul>
<li>安装<code>coc-nvim</code>插件</li>
<li>使用<code>coc-nvim</code>插件安装<code>coc-markdown</code>插件</li>
<li>使用<code>:CocCommand markdown-preview-enhanced.openPreview</code>命令在浏览器中打开预览</li>
</ul>
<h1><code>coc-vim</code>的使用</h1>
<ul>
<li>可以先安装一个<code>coc-marketplace</code>插件:<code>:CocInstall coc-marketplace</code></li>
<li><code>CocList marketplace</code>列出所有可用的插件</li>
<li>在选中的插件上可以使用<code>tab</code>来进行<code>install</code>,<code>uninstall</code>,<code>homepage</code>等动作</li>
</ul>
<h1><code>vim-easy-align</code>的使用</h1>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  Lorem   = ipsum</span><br><span class="line">  dolor   = sit</span><br><span class="line">   amet  += consectetur == adipiscing</span><br><span class="line">   elit  -= sed           != do</span><br><span class="line">eiusmod   = tempor        = incididunt</span><br><span class="line">     ut &amp;&amp;= labore</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>normal模式下：<code>gaip</code></p>
<ul>
<li><code>=</code>Around the 1st occurrentces</li>
<li><code>2=</code>Around the 2nd occurrences</li>
<li><code>*=</code>Around all occurrences</li>
<li><code>**=</code>Left/Right alternating alignment around all occurrences</li>
<li><code>&lt;Enter&gt;</code>Switching between left/right/center alignment modes</li>
</ul>
</li>
<li>
<p><code>visual line</code> 模式: <code>vip</code></p>
<ul>
<li><code>ga</code>进入<code>easyalign</code>模式</li>
<li><code>&lt;Enter&gt;</code> 切换对齐模式</li>
<li>输入<code>对齐目标字符</code>, 再<code>&lt;Space&gt;</code> 完成对齐操作</li>
</ul>
</li>
</ul>
<h1><code>nerdtree</code>的使用</h1>
<ul>
<li><code>j</code>, <code>k</code> 在目录下上移动</li>
<li><code>o</code>展开目录或打开文件，焦点会跑到右侧文件视图中</li>
<li><code>ctrl + w + h</code>可以让焦点从文件中回到左侧目录视图中</li>
<li><code>:NERDTreeToggle</code>命令是<code>NERDTree</code>打开或关闭的开关,在<code>.vimrc</code>中设置快捷键把它用<code>F3</code>来控制  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot; 设置NerdTree</span><br><span class="line">map &lt;F3&gt; :NERDTreeMirror&lt;CR&gt;</span><br><span class="line">map &lt;F3&gt; :NERDTreeToggle&lt;CR&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1><code>TagBar</code>的使用</h1>
<ul>
<li><code>:TagbarToggle</code></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2023/06/14/k8s%E4%B8%ADconfigMap%E7%BC%96%E8%BE%91%E7%A4%BA%E4%BE%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/14/k8s%E4%B8%ADconfigMap%E7%BC%96%E8%BE%91%E7%A4%BA%E4%BE%8B/" class="post-title-link" itemprop="url">k8s中configMap编辑示例</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-14 14:27:16 / 修改时间：14:50:24" itemprop="dateCreated datePublished" datetime="2023-06-14T14:27:16+08:00">2023-06-14</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/" itemprop="url" rel="index"><span itemprop="name">云原生</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/" itemprop="url" rel="index"><span itemprop="name">基础设施</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1>背景</h1>
<p>部署在k3s中的一个应用，在前端页面中上传文件，出现报错，提示是文件太大。需要修改nginx配置来解决这个问题。这个应用部署在fate-10000命名空间下</p>
<h1>解决</h1>
<ul>
<li>
<p>查看一下fate-10000这个命名空间</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get all  -n fate-10000</span><br></pre></td></tr></table></figure>
<p><img src="/2023/06/14/k8s%E4%B8%ADconfigMap%E7%BC%96%E8%BE%91%E7%A4%BA%E4%BE%8B/fate-10000.jpg" alt="查看命名空间"></p>
</li>
<li>
<p>先查看一下<code>site-portal</code>前端<code>pod</code>的描述信息：例子中为<code>fate-10000</code>命名空间下的<code>frontend-85b6fdffc4-4lkpc</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod frontend-85b6fdffc4-4lkpc -n fate-10000</span><br></pre></td></tr></table></figure>
<p><img src="/2023/06/14/k8s%E4%B8%ADconfigMap%E7%BC%96%E8%BE%91%E7%A4%BA%E4%BE%8B/frontend.jpg" alt="frontend描述信息"></p>
</li>
</ul>
<blockquote>
<p>注意图中nginx的配置文件会从nginx-conf-https中获取一些运维自定义的配置，nginx-conf-http是一个configMap, configMap的作用我理解的就是给运维人员后期动态去修改配置，pod从configMap中获取这些自定义的值后就达到了动态修改配置的目的，当然configMap修改后，相关的pod需要重启</p>
</blockquote>
<ul>
<li>
<p>查看命名空间下的configMap</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get cm -n fate-10000</span><br></pre></td></tr></table></figure>
<p><img src="/2023/06/14/k8s%E4%B8%ADconfigMap%E7%BC%96%E8%BE%91%E7%A4%BA%E4%BE%8B/get-cm.jpg" alt="configMap"></p>
</li>
<li>
<p>编辑configMap中的内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit cm nginx-conf-https -n fate-10000</span><br></pre></td></tr></table></figure>
<p><img src="/2023/06/14/k8s%E4%B8%ADconfigMap%E7%BC%96%E8%BE%91%E7%A4%BA%E4%BE%8B/edit-cm.jpg" alt="编辑configMap"></p>
</li>
<li>
<p>获取deployment</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get deploy -n fate-10000</span><br></pre></td></tr></table></figure>
<p><img src="/2023/06/14/k8s%E4%B8%ADconfigMap%E7%BC%96%E8%BE%91%E7%A4%BA%E4%BE%8B/deployment.jpg" alt="deployment"></p>
</li>
<li>
<p>用另外一种方式确认某个pod是否和某个configMap有关联</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get deploy frontend -n fate-10000 -o yaml</span><br></pre></td></tr></table></figure>
<p>返回如下：</p>
 <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">annotations:</span></span><br><span class="line">   <span class="attr">deployment.kubernetes.io/revision:</span> <span class="string">&quot;2&quot;</span></span><br><span class="line">   <span class="attr">meta.helm.sh/release-name:</span> <span class="string">party10000</span></span><br><span class="line">   <span class="attr">meta.helm.sh/release-namespace:</span> <span class="string">fate-10000</span></span><br><span class="line"><span class="attr">creationTimestamp:</span> <span class="string">&quot;2023-06-14T01:09:35Z&quot;</span></span><br><span class="line"><span class="attr">generation:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">labels:</span></span><br><span class="line">   <span class="attr">app.kubernetes.io/managed-by:</span> <span class="string">Helm</span></span><br><span class="line">   <span class="attr">chart:</span> <span class="string">fate</span></span><br><span class="line">   <span class="attr">cluster:</span> <span class="string">fate</span></span><br><span class="line">   <span class="attr">fateMoudle:</span> <span class="string">frontend</span></span><br><span class="line">   <span class="attr">heritage:</span> <span class="string">Helm</span></span><br><span class="line">   <span class="attr">name:</span> <span class="string">party10000</span></span><br><span class="line">   <span class="attr">owner:</span> <span class="string">kubefate</span></span><br><span class="line">   <span class="attr">partyId:</span> <span class="string">&quot;10000&quot;</span></span><br><span class="line">   <span class="attr">release:</span> <span class="string">party10000</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">namespace:</span> <span class="string">fate-10000</span></span><br><span class="line"><span class="attr">resourceVersion:</span> <span class="string">&quot;319024&quot;</span></span><br><span class="line"><span class="attr">uid:</span> <span class="string">e62cf903-86b2-4799-96e1-f366ed4e3395</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">progressDeadlineSeconds:</span> <span class="number">600</span></span><br><span class="line"><span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">revisionHistoryLimit:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">selector:</span></span><br><span class="line">   <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">fateMoudle:</span> <span class="string">frontend</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">party10000</span></span><br><span class="line">      <span class="attr">partyId:</span> <span class="string">&quot;10000&quot;</span></span><br><span class="line"><span class="attr">strategy:</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">Recreate</span></span><br><span class="line"><span class="attr">template:</span></span><br><span class="line">   <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">      <span class="attr">kubectl.kubernetes.io/restartedAt:</span> <span class="string">&quot;2023-06-14T10:41:20+08:00&quot;</span></span><br><span class="line">      <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">chart:</span> <span class="string">fate</span></span><br><span class="line">      <span class="attr">cluster:</span> <span class="string">fate</span></span><br><span class="line">      <span class="attr">fateMoudle:</span> <span class="string">frontend</span></span><br><span class="line">      <span class="attr">heritage:</span> <span class="string">Helm</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">party10000</span></span><br><span class="line">      <span class="attr">owner:</span> <span class="string">kubefate</span></span><br><span class="line">      <span class="attr">partyId:</span> <span class="string">&quot;10000&quot;</span></span><br><span class="line">      <span class="attr">release:</span> <span class="string">party10000</span></span><br><span class="line">   <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">env:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">SITEPORTAL_SERVER_HOST</span></span><br><span class="line">         <span class="attr">value:</span> <span class="string">site-portal-server</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">federatedai/site-portal-frontend:v0.3.0</span></span><br><span class="line">      <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">frontend</span></span><br><span class="line">      <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8443</span></span><br><span class="line">         <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">resources:</span> &#123;&#125;</span><br><span class="line">      <span class="attr">terminationMessagePath:</span> <span class="string">/dev/termination-log</span></span><br><span class="line">      <span class="attr">terminationMessagePolicy:</span> <span class="string">File</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/var/lib/site-portal/cert</span></span><br><span class="line">         <span class="attr">name:</span> <span class="string">site-portal-cert</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/nginx/conf.d/nginx.conf.template</span></span><br><span class="line">         <span class="attr">name:</span> <span class="string">nginx-conf-https</span></span><br><span class="line">         <span class="attr">subPath:</span> <span class="string">nginx.conf.template</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">ClusterFirst</span></span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br><span class="line">      <span class="attr">schedulerName:</span> <span class="string">default-scheduler</span></span><br><span class="line">      <span class="attr">securityContext:</span> &#123;&#125;</span><br><span class="line">      <span class="attr">serviceAccount:</span> <span class="string">default</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">default</span></span><br><span class="line">      <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">30</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">site-portal-cert</span></span><br><span class="line">      <span class="attr">secret:</span></span><br><span class="line">         <span class="attr">defaultMode:</span> <span class="number">420</span></span><br><span class="line">         <span class="attr">secretName:</span> <span class="string">site-portal-cert</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">configMap:</span></span><br><span class="line">         <span class="attr">defaultMode:</span> <span class="number">420</span></span><br><span class="line">         <span class="attr">name:</span> <span class="string">nginx-conf-https</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx-conf-https</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line"><span class="attr">availableReplicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">conditions:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">lastTransitionTime:</span> <span class="string">&quot;2023-06-14T02:41:23Z&quot;</span></span><br><span class="line">   <span class="attr">lastUpdateTime:</span> <span class="string">&quot;2023-06-14T02:41:23Z&quot;</span></span><br><span class="line">   <span class="attr">message:</span> <span class="string">Deployment</span> <span class="string">has</span> <span class="string">minimum</span> <span class="string">availability.</span></span><br><span class="line">   <span class="attr">reason:</span> <span class="string">MinimumReplicasAvailable</span></span><br><span class="line">   <span class="attr">status:</span> <span class="string">&quot;True&quot;</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">Available</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">lastTransitionTime:</span> <span class="string">&quot;2023-06-14T01:09:35Z&quot;</span></span><br><span class="line">   <span class="attr">lastUpdateTime:</span> <span class="string">&quot;2023-06-14T02:41:23Z&quot;</span></span><br><span class="line">   <span class="attr">message:</span> <span class="string">ReplicaSet</span> <span class="string">&quot;frontend-85b6fdffc4&quot;</span> <span class="string">has</span> <span class="string">successfully</span> <span class="string">progressed.</span></span><br><span class="line">   <span class="attr">reason:</span> <span class="string">NewReplicaSetAvailable</span></span><br><span class="line">   <span class="attr">status:</span> <span class="string">&quot;True&quot;</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">Progressing</span></span><br><span class="line"><span class="attr">observedGeneration:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">readyReplicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">updatedReplicas:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意看其中的 configMap, 就是: nginx-conf-https，所以说明我们可以去改nginx-config-https这个configMap中的值，修改会作用于frontend</p>
</blockquote>
</li>
<li>
<p>编辑configMap</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit cm nginx-conf-https -n fate-10000</span><br></pre></td></tr></table></figure>
<p>上述命令会使用nano打开configMap,可以对configMap进行编辑<br>
<img src="/2023/06/14/k8s%E4%B8%ADconfigMap%E7%BC%96%E8%BE%91%E7%A4%BA%E4%BE%8B/edit-cm.jpg" alt="编辑configMap"></p>
<ul>
<li>重启frontend</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout restart deployment/frontend -n fate-10000</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意这个命令中的 <code>deployment/frontend</code>, 命令中需要有<code>deployment/</code>加在前面</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">暂留白</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">81</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">38</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">172</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">暂留白</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>
-->

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
