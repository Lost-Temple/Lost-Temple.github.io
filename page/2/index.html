<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.blockchainof.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="自留地">
<meta property="og:url" content="http://www.blockchainof.com/page/2/index.html">
<meta property="og:site_name" content="自留地">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="暂留白">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://www.blockchainof.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>自留地</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">自留地</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-主页">

    <a href="/" rel="section"><i class="fas fa-home fa-fw"></i>主页</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fas fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fas fa-folder-open fa-fw"></i>分类</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2024/05/17/Docker%E7%9F%A5%E8%AF%86%E7%82%B902/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/05/17/Docker%E7%9F%A5%E8%AF%86%E7%82%B902/" class="post-title-link" itemprop="url">Docker知识点02</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-05-17 10:21:25" itemprop="dateCreated datePublished" datetime="2024-05-17T10:21:25+08:00">2024-05-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-05-24 16:30:40" itemprop="dateModified" datetime="2024-05-24T16:30:40+08:00">2024-05-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/" itemprop="url" rel="index"><span itemprop="name">云原生</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/" itemprop="url" rel="index"><span itemprop="name">基础设施</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="前言">前言</h1>
<p>需要搭建spark集群以及hadoop集群，并且使它间之间进行交互。目标：编写一个yaml，用于一键部署。本文先试验一个个部署，最终形成一个一键部署的方案。</p>
<h1 id="实践">实践</h1>
<h2 id="hadoop集群单机部署试验">hadoop集群单机部署试验</h2>
<ol type="1">
<li><p>拉取appache官方镜像</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull apache/hadoop</span><br></pre></td></tr></table></figure></li>
<li><p>在服务器本地目录创建所需文件</p>
<ul>
<li><p>创建<code>docker-compose.yaml</code>文件</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&quot;3&quot;</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">   <span class="attr">namenode:</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">apache/hadoop:3</span></span><br><span class="line">      <span class="attr">networks:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">hadoop_network</span></span><br><span class="line">      <span class="attr">hostname:</span> <span class="string">namenode</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&quot;hdfs&quot;</span>, <span class="string">&quot;namenode&quot;</span>]</span><br><span class="line">      <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">9870</span><span class="string">:9870</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">8020</span><span class="string">:8020</span></span><br><span class="line">      <span class="attr">env_file:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">./config</span></span><br><span class="line">      <span class="attr">environment:</span></span><br><span class="line">          <span class="attr">ENSURE_NAMENODE_DIR:</span> <span class="string">&quot;/tmp/hadoop-root/dfs/name&quot;</span></span><br><span class="line">   <span class="attr">datanode:</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">apache/hadoop:3</span></span><br><span class="line">      <span class="attr">networks:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">hadoop_network</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&quot;hdfs&quot;</span>, <span class="string">&quot;datanode&quot;</span>]</span><br><span class="line">      <span class="attr">env_file:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">./config</span></span><br><span class="line">   <span class="attr">resourcemanager:</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">apache/hadoop:3</span></span><br><span class="line">      <span class="attr">networks:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">hadoop_network</span></span><br><span class="line">      <span class="attr">hostname:</span> <span class="string">resourcemanager</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&quot;yarn&quot;</span>, <span class="string">&quot;resourcemanager&quot;</span>]</span><br><span class="line">      <span class="attr">ports:</span></span><br><span class="line">         <span class="bullet">-</span> <span class="number">8088</span><span class="string">:8088</span></span><br><span class="line">      <span class="attr">env_file:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">./config</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">./test.sh:/opt/test.sh</span></span><br><span class="line">   <span class="attr">nodemanager:</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">apache/hadoop:3</span></span><br><span class="line">      <span class="attr">networks:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">hadoop_network</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&quot;yarn&quot;</span>, <span class="string">&quot;nodemanager&quot;</span>]</span><br><span class="line">      <span class="attr">env_file:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">./config</span></span><br><span class="line"><span class="attr">networks:</span></span><br><span class="line">  <span class="attr">hadoop_network:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">hadoop-net</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>自定义网络，这样定义是会自动创建这个网络的</p>
</blockquote>
<p>如果网络已经存在，那可以这样指定(在后续我们一键部署时这个很重要，需要让每个容器在同一网络内，<code>docker stack</code>
部署还需要这个网络为<code>overlay</code>网络)</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">networks:</span></span><br><span class="line">  <span class="attr">hadoop_network:</span></span><br><span class="line">    <span class="attr">external:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">hadoop-net</span></span><br></pre></td></tr></table></figure></li>
<li><p>上述文件中有一个<code>config</code>文件用来设置一些环境变量，所以我们再创建一个<code>config</code>文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">CORE-SITE.XML_fs.default.name=hdfs://namenode</span><br><span class="line">CORE-SITE.XML_fs.defaultFS=hdfs://namenode</span><br><span class="line">HDFS-SITE.XML_dfs.namenode.rpc-address=namenode:8020</span><br><span class="line">HDFS-SITE.XML_dfs.replication=1</span><br><span class="line">MAPRED-SITE.XML_mapreduce.framework.name=yarn</span><br><span class="line">MAPRED-SITE.XML_yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=$HADOOP_HOME</span><br><span class="line">MAPRED-SITE.XML_mapreduce.map.env=HADOOP_MAPRED_HOME=$HADOOP_HOME</span><br><span class="line">MAPRED-SITE.XML_mapreduce.reduce.env=HADOOP_MAPRED_HOME=$HADOOP_HOME</span><br><span class="line">YARN-SITE.XML_yarn.resourcemanager.hostname=resourcemanager</span><br><span class="line">YARN-SITE.XML_yarn.nodemanager.pmem-check-enabled=false</span><br><span class="line">YARN-SITE.XML_yarn.nodemanager.delete.debug-delay-sec=600</span><br><span class="line">YARN-SITE.XML_yarn.nodemanager.vmem-check-enabled=false</span><br><span class="line">YARN-SITE.XML_yarn.nodemanager.aux-services=mapreduce_shuffle</span><br><span class="line">CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-applications=10000</span><br><span class="line">CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-am-resource-percent=0.1</span><br><span class="line">CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.resource-calculator=org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</span><br><span class="line">CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.queues=default</span><br><span class="line">CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.capacity=100</span><br><span class="line">CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.user-limit-factor=1</span><br><span class="line">CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.maximum-capacity=100</span><br><span class="line">CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.state=RUNNING</span><br><span class="line">CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl_submit_applications=*</span><br><span class="line">CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl_administer_queue=*</span><br><span class="line">CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.node-locality-delay=40</span><br><span class="line">CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings=</span><br><span class="line">CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings-override.enable=false</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里面没有指定<code>HADOOP_HOME</code>的值，部署的时候会有一个警告，但也不影响使用，后来我就在这个文件中，这个变量使用前，添加了一行<code>HADOOP_HOME=/opt/hadoop</code></p>
</blockquote></li>
</ul></li>
<li><p>检查所需文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[centos@maodaoming-1 hadoop]$ ll</span><br><span class="line">total 8</span><br><span class="line">-rw-rw-r--. 1 centos centos 1789 May 17 09:21 config</span><br><span class="line">-rw-rw-r--. 1 centos centos  984 May 17 11:04 docker-compose.yaml</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="spark-集群单机部署">spark 集群单机部署</h1>
<p>因为是搭建FATE 2.1.0
版本的spark环境，所以使用的是使用FATE-Builder打包出来的spark镜像，包含：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yunpcds/spark-master        2.1.0-release   115b928c3b3d   21 hours ago    4.68GB</span><br><span class="line">yunpcds/spark-worker        2.1.0-release   371c1de5aabe   21 hours ago    4.68GB</span><br></pre></td></tr></table></figure>
<ol type="1">
<li>创建<code>docker-compose.yaml</code>文件</li>
</ol>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&quot;3&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">spark-master:</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">spark-master</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">$&#123;IMAGE_PREFIX&#125;/spark-master:$&#123;IMAGE_TAG&#125;</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">spark-master</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8080:8080&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;7077:7077&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;6066:6066&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">spark-slave:</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">spark-slave</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">$&#123;IMAGE_PREFIX&#125;/spark-worker:$&#123;IMAGE_TAG&#125;</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">spark-slave</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8781:8081&quot;</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/data/projects/:/data/projects/</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">SERVICE_PRECONDITION:</span> <span class="string">&quot;spark-master:7077&quot;</span></span><br><span class="line">      <span class="attr">EGGROLL_HOME:</span> <span class="string">&quot;/data/projects/fate/eggroll&quot;</span></span><br><span class="line">      <span class="attr">FATE_PROJECT_BASE:</span> <span class="string">&quot;/data/projects/fate&quot;</span></span><br><span class="line">      <span class="attr">VIRTUAL_ENV:</span> <span class="string">&quot;/data/projects/python/venv&quot;</span></span><br><span class="line">      <span class="attr">PYTHONPATH:</span> <span class="string">&quot;/data/projects/fate/:/data/projects/fate/eggroll/python:/data/projects/fate/fate/python:/data/projects/fate/fateflow/python:/data/projects/fate/python&quot;</span></span><br><span class="line">      <span class="attr">PATH:</span> <span class="string">&quot;/data/projects/python/venv/bin:/opt/hadoop-3.2.3/bin:/opt/hive/bin:/opt/spark-3.1.3-bin-hadoop3.2/bin:/opt/hadoop-3.2.3/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;</span></span><br><span class="line"><span class="attr">networks:</span></span><br><span class="line">  <span class="attr">spark_network:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">spark-net</span></span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>变量设置文件:<font size="5"><code>.env</code></font>文件</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">IMAGE_PREFIX=yunpcds</span><br><span class="line">IMAGE_TAG=2.1.0-release</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里面的变量会被传入到<code>docker-compose.yaml</code>中去,上面的PYTHONPATH,
PATH根据实际情况再调整；还有<font size="5">volumes</font>的映射，需要把fate的代码拷贝到宿主机的对应目录<code>/data/projects/</code></p>
</blockquote>
<ol start="3" type="1">
<li><p>查看所需文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[centos@maodaoming-2 spark]$ ll -a</span><br><span class="line">total 8</span><br><span class="line">drwxrwxr-x. 2 centos centos   45 May 17 15:33 .</span><br><span class="line">drwxrwxr-x. 4 centos centos   33 May 17 14:38 ..</span><br><span class="line">-rw-rw-r--. 1 centos centos 1103 May 17 14:56 docker-compose.yaml</span><br><span class="line">-rw-rw-r--. 1 centos centos   45 May 17 14:39 .env</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="一键部署">一键部署</h1>
<p>把<code>docker-compose.yaml</code>合并成一份，使用docker
stack部署：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&quot;3&quot;</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">namenode:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">apache/hadoop:3</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">namenode</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;hdfs&quot;</span>, <span class="string">&quot;namenode&quot;</span>]</span><br><span class="line">    <span class="attr">networks:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">bigdata-network</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">target:</span> <span class="number">9870</span></span><br><span class="line">        <span class="attr">published:</span> <span class="number">9870</span></span><br><span class="line">        <span class="attr">protocol:</span> <span class="string">tcp</span></span><br><span class="line">        <span class="attr">mode:</span> <span class="string">host</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">target:</span> <span class="number">9000</span></span><br><span class="line">        <span class="attr">published:</span> <span class="number">9000</span></span><br><span class="line">        <span class="attr">protocol:</span> <span class="string">tcp</span></span><br><span class="line">        <span class="attr">mode:</span> <span class="string">host</span></span><br><span class="line">    <span class="attr">env_file:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./config</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">ENSURE_NAMENODE_DIR:</span> <span class="string">&quot;/tmp/hadoop-root/dfs/name&quot;</span></span><br><span class="line">    <span class="attr">deploy:</span></span><br><span class="line">      <span class="attr">endpoint_mode:</span> <span class="string">dnsrr</span></span><br><span class="line">      <span class="attr">placement:</span></span><br><span class="line">        <span class="attr">constraints:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&#x27;node.labels.nodename == node55&#x27;</span></span><br><span class="line">  <span class="attr">datanode:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">apache/hadoop:3</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;hdfs&quot;</span>, <span class="string">&quot;datanode&quot;</span>]</span><br><span class="line">    <span class="attr">networks:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">bigdata-network</span></span><br><span class="line">    <span class="attr">env_file:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./config</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">target:</span> <span class="number">1004</span></span><br><span class="line">        <span class="attr">published:</span> <span class="number">1004</span></span><br><span class="line">        <span class="attr">protocol:</span> <span class="string">tcp</span></span><br><span class="line">        <span class="attr">mode:</span> <span class="string">host</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">target:</span> <span class="number">1006</span></span><br><span class="line">        <span class="attr">published:</span> <span class="number">1006</span></span><br><span class="line">        <span class="attr">protocol:</span> <span class="string">tcp</span></span><br><span class="line">        <span class="attr">mode:</span> <span class="string">host</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">target:</span> <span class="number">9866</span></span><br><span class="line">        <span class="attr">published:</span> <span class="number">9864</span></span><br><span class="line">        <span class="attr">protocol:</span> <span class="string">tcp</span></span><br><span class="line">        <span class="attr">mode:</span> <span class="string">host</span></span><br><span class="line">    <span class="attr">deploy:</span></span><br><span class="line">      <span class="attr">endpoint_mode:</span> <span class="string">dnsrr</span></span><br><span class="line">      <span class="attr">placement:</span></span><br><span class="line">        <span class="attr">constraints:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&#x27;node.labels.nodename == node55&#x27;</span></span><br><span class="line">  <span class="attr">resourcemanager:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">apache/hadoop:3</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">resourcemanager</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;yarn&quot;</span>, <span class="string">&quot;resourcemanager&quot;</span>]</span><br><span class="line">    <span class="attr">networks:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">bigdata-network</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">8088</span><span class="string">:8088</span></span><br><span class="line">    <span class="attr">env_file:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./config</span></span><br><span class="line">    <span class="attr">deploy:</span></span><br><span class="line">      <span class="attr">placement:</span></span><br><span class="line">        <span class="attr">constraints:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&#x27;node.labels.nodename == node55&#x27;</span></span><br><span class="line">  <span class="attr">nodemanager:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">apache/hadoop:3</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;yarn&quot;</span>, <span class="string">&quot;nodemanager&quot;</span>]</span><br><span class="line">    <span class="attr">networks:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">bigdata-network</span></span><br><span class="line">    <span class="attr">env_file:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./config</span></span><br><span class="line">    <span class="attr">deploy:</span></span><br><span class="line">      <span class="attr">placement:</span></span><br><span class="line">        <span class="attr">constraints:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&#x27;node.labels.nodename == node55&#x27;</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">spark-master:</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">spark-master</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">$&#123;IMAGE_PREFIX&#125;/spark-master:$&#123;IMAGE_TAG&#125;</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">spark-master</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line">    <span class="attr">networks:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">bigdata-network</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8080:8080&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;7077:7077&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;6066:6066&quot;</span></span><br><span class="line">    <span class="attr">deploy:</span></span><br><span class="line">      <span class="attr">placement:</span></span><br><span class="line">        <span class="attr">constraints:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&#x27;node.role == manager&#x27;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&#x27;node.labels.nodename == node91&#x27;</span></span><br><span class="line">  <span class="attr">spark-slave:</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">spark-worker</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">$&#123;IMAGE_PREFIX&#125;/spark-worker:$&#123;IMAGE_TAG&#125;</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">spark-worker</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line">    <span class="attr">networks:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">bigdata-network</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8781:8081&quot;</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/data/projects/:/data/projects/</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">SERVICE_PRECONDITION:</span> <span class="string">&quot;spark-master:7077&quot;</span></span><br><span class="line">      <span class="attr">EGGROLL_HOME:</span> <span class="string">&quot;/data/projects/fate/eggroll&quot;</span></span><br><span class="line">      <span class="attr">FATE_PROJECT_BASE:</span> <span class="string">&quot;/data/projects/fate&quot;</span></span><br><span class="line">      <span class="attr">VIRTUAL_ENV:</span> <span class="string">&quot;/data/projects/python/venv&quot;</span></span><br><span class="line">      <span class="attr">PYTHONPATH:</span> <span class="string">&quot;/data/projects/fate/:/data/projects/fate/eggroll/python:/data/projects/fate/fate/python:/data/projects/fate/fateflow/python:/data/projects/fate/python&quot;</span></span><br><span class="line">      <span class="attr">PATH:</span> <span class="string">&quot;/data/projects/python/venv/bin:/opt/hadoop-3.2.3/bin:/opt/hive/bin:/opt/spark-3.1.3-bin-hadoop3.2/bin:/opt/hadoop-3.2.3/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;</span></span><br><span class="line">    <span class="attr">deploy:</span></span><br><span class="line">      <span class="attr">placement:</span></span><br><span class="line">        <span class="attr">constraints:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&#x27;node.role == manager&#x27;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&#x27;node.labels.nodename == node91&#x27;</span></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line">  <span class="attr">hadoop_namenode:</span></span><br><span class="line">  <span class="attr">hadoop_datanode1:</span></span><br><span class="line">  <span class="attr">hadoop_datanode2:</span></span><br><span class="line">  <span class="attr">hadoop_datanode3:</span></span><br><span class="line">  <span class="attr">hadoop_historyserver:</span></span><br><span class="line">  <span class="attr">kerberos_db:</span></span><br><span class="line">  <span class="attr">kerberos_keytab:</span></span><br><span class="line">    <span class="attr">driver_opts:</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">nfs</span></span><br><span class="line">      <span class="attr">o:</span> <span class="string">addr=192.168.11.74,nfsvers=4,minorversion=0,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport</span></span><br><span class="line">      <span class="attr">device:</span> <span class="string">:/home/centos/nfs_share/volume/bigdata/keytab</span></span><br><span class="line">  <span class="attr">hive_metastore:</span></span><br><span class="line">  <span class="attr">mysql_data:</span></span><br><span class="line">  <span class="attr">linkis_log:</span></span><br><span class="line">  <span class="attr">linkis_runtime:</span></span><br><span class="line"><span class="attr">networks:</span></span><br><span class="line">  <span class="attr">bigdata-network:</span></span><br><span class="line">    <span class="attr">external:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">bigdata-network</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：有volumes映射的一定要注意，如果路径不存在就启动失败</p>
</blockquote>
<p>启动脚本编写：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker stack rm bigdata</span><br><span class="line"></span><br><span class="line">sleep 1s</span><br><span class="line"></span><br><span class="line">echo &quot;请注意清理nodedata的volume!!!&quot;</span><br><span class="line"></span><br><span class="line">[ -f .env ] &amp;&amp; export $(sed &#x27;/^#/d&#x27; .env)</span><br><span class="line">docker stack deploy -c docker-stack-deploy.yaml bigdata</span><br></pre></td></tr></table></figure>
<p>启动后检查是否成功：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[centos@maodaoming-2 flow-spark-v2-env]$ docker service ls</span><br><span class="line"></span><br><span class="line">gzpqsvib86y6   bigdata_datanode          replicated   1/1        apache/hadoop:3</span><br><span class="line">proujh710xqf   bigdata_namenode          replicated   1/1        apache/hadoop:3</span><br><span class="line">fyok561q2qiv   bigdata_nodemanager       replicated   1/1        apache/hadoop:3</span><br><span class="line">72gzt56c9yrp   bigdata_resourcemanager   replicated   0/1        apache/hadoop:3                      *:8088-&gt;8088/tcp</span><br><span class="line">opo2n1jaa0bj   bigdata_spark-master      replicated   1/1        yunpcds/spark-master:2.1.0-release   *:6066-&gt;6066/tcp, *:7077-&gt;7077/tcp, *:8080-&gt;8080/tcp</span><br><span class="line">rsxhqoanjd3y   bigdata_spark-worker      replicated   1/1        yunpcds/spark-worker:2.1.0-release   *:8781-&gt;8081/tcp</span><br><span class="line">chwttx82rp8u   busybox                   replicated   1/1        busybox:latest</span><br></pre></td></tr></table></figure>
<p>可以查看到<code>bigdata_resourcemanager</code>是启动失败的。可以用以下命令查看日志：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[centos@maodaoming-2 flow-spark-v2-env]$ docker service ps  --no-trunc bigdata_resourcemanager</span><br><span class="line"></span><br><span class="line">z8i8jbqpzz8zbmkh0w9k8i91o   bigdata_resourcemanager.1       apache/hadoop:3@sha256:af361b20bec0dfb13f03279328572ba764926e918c4fe716e197b8be2b08e37f   maodaoming-1.novalocal   Ready           Rejected 1 second ago     &quot;invalid mount config for type &quot;bind&quot;: bind source path does not exist: /home/centos/flow-spark-v2-env/test.sh&quot;</span><br><span class="line">spmxmabdm0gikj33y4bxoot51    \_ bigdata_resourcemanager.1   apache/hadoop:3@sha256:af361b20bec0dfb13f03279328572ba764926e918c4fe716e197b8be2b08e37f   maodaoming-1.novalocal   Shutdown        Rejected 6 seconds ago    &quot;invalid mount config for type &quot;bind&quot;: bind source path does not exist: /home/centos/flow-spark-v2-env/test.sh&quot;</span><br><span class="line">zo68mawn5flwwejynq54iwus0    \_ bigdata_resourcemanager.1   apache/hadoop:3@sha256:af361b20bec0dfb13f03279328572ba764926e918c4fe716e197b8be2b08e37f   maodaoming-1.novalocal   Shutdown        Rejected 11 seconds ago   &quot;invalid mount config for type &quot;bind&quot;: bind source path does not exist: /home/centos/flow-spark-v2-env/test.sh&quot;</span><br><span class="line">60ew6r1iqf9wppbc3dojq0566    \_ bigdata_resourcemanager.1   apache/hadoop:3@sha256:af361b20bec0dfb13f03279328572ba764926e918c4fe716e197b8be2b08e37f   maodaoming-1.novalocal   Shutdown        Rejected 17 seconds ago   &quot;invalid mount config for type &quot;bind&quot;: bind source path does not exist: /home/centos/flow-spark-v2-env/test.sh&quot;</span><br><span class="line">nk9rzlaqqw2e1rr4ali2634mp    \_ bigdata_resourcemanager.1   apache/hadoop:3@sha256:af361b20bec0dfb13f03279328572ba764926e918c4fe716e197b8be2b08e37f   maodaoming-1.novalocal   Shutdown        Rejected 21 seconds ago   &quot;invalid mount config for type &quot;bind&quot;: bind source path does not exist: /home/centos/flow-spark-v2-env/test.sh&quot;</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2024/04/24/vcpkg%E7%9A%84%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/04/24/vcpkg%E7%9A%84%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/" class="post-title-link" itemprop="url">vcpkg的使用记录</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-04-24 08:24:30 / 修改时间：13:59:30" itemprop="dateCreated datePublished" datetime="2024-04-24T08:24:30+08:00">2024-04-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/C/" itemprop="url" rel="index"><span itemprop="name">C++</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/C/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/" itemprop="url" rel="index"><span itemprop="name">开发环境</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="安装vcpkg">安装vcpkg</h1>
<h2 id="clone-vcpkg">clone vcpkg</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/microsoft/vcpkg</span><br></pre></td></tr></table></figure>
<h2 id="install-vcpkg">install vcpkg</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd vcpkg</span><br><span class="line">./bootstrap-vcpkg.sh</span><br></pre></td></tr></table></figure>
<h2 id="设置环境变量">设置环境变量</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">vcpkg</span></span><br><span class="line">export VCPKG_ROOT=~/vcpkg</span><br><span class="line">export PATH=$PATH:$VCPKG_ROOT</span><br><span class="line">export VCPKG_DEFAULT_TRIPLET=arm64-osx</span><br></pre></td></tr></table></figure>
<blockquote>
<p>VCPKG_DEFAULT_TRIPLET 设置了默认的TRIPLET，vcpkg install
时就无需指定triplet</p>
</blockquote>
<h1 id="vcpkg-install">vcpkg install</h1>
<h2 id="安装boost">安装boost</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vcpkg install boost</span><br></pre></td></tr></table></figure>
<p>在最后会出错，有警告信息如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">CMake Warning at ports/python3/portfile.cmake:7 (message):</span><br><span class="line">  python3 currently requires the following programs from the system package</span><br><span class="line">  manager:</span><br><span class="line"></span><br><span class="line">      autoconf automake autoconf-archive</span><br><span class="line"></span><br><span class="line">  On Debian and Ubuntu derivatives:</span><br><span class="line"></span><br><span class="line">      sudo apt-get install autoconf automake autoconf-archive</span><br><span class="line"></span><br><span class="line">  On recent Red Hat and Fedora derivatives:</span><br><span class="line"></span><br><span class="line">      sudo dnf install autoconf automake autoconf-archive</span><br><span class="line"></span><br><span class="line">  On Arch Linux and derivatives:</span><br><span class="line"></span><br><span class="line">      sudo pacman -S autoconf automake autoconf-archive</span><br><span class="line"></span><br><span class="line">  On Alpine:</span><br><span class="line"></span><br><span class="line">      apk add autoconf automake autoconf-archive</span><br><span class="line"></span><br><span class="line">  On macOS:</span><br><span class="line"></span><br><span class="line">      brew install autoconf automake autoconf-archive</span><br></pre></td></tr></table></figure>
<blockquote>
<p>按照上述警告信息操作后再重新执行vcpkg install boost，成功</p>
</blockquote>
<h2 id="安装nlohmann-json">安装nlohmann-json</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vcpkg install nlohmann-json</span><br></pre></td></tr></table></figure>
<h1 id="创建c项目">创建c++项目</h1>
<h3 id="vcpkg-创建项目">vcpkg 创建项目</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir helloworld &amp;&amp; cd helloworld</span><br><span class="line">vcpkg new --application</span><br></pre></td></tr></table></figure>
<blockquote>
<p>会生成vcpkg.json文件，项目中的依赖库信息会保存在里面</p>
</blockquote>
<h3 id="添加依赖">添加依赖</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vcpkg add port nlohmann-json</span><br></pre></td></tr></table></figure>
<p>执行后在<code>vcpkg.json</code>文件中会添加这个依赖：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;dependencies&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="string">&quot;nlohmann-json&quot;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="创建cmakelists.txt文件">创建<code>CMakeLists.txt</code>文件</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cmake_minimum_required(VERSION 3.10)</span><br><span class="line">project(HelloWorld)</span><br><span class="line">add_executable(HelloWorld helloworld.cpp)</span><br><span class="line">find_package(nlohmann_json CONFIG REQUIRED)</span><br><span class="line">target_link_libraries(HelloWorld PRIVATE nlohmann_json::nlohmann_json)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>CMakeLists.txt</code>中并没有指定nlohmann_json包的路径，那CMake怎么找到这些依赖包呢？那就要用到<code>CMakePresets.json</code>文件</p>
</blockquote>
<h2 id="创建cmakepresets.json文件">创建<code>CMakePresets.json</code>文件</h2>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;configurePresets&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;default&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;generator&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Ninja&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;binaryDir&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;sourceDir&#125;/build&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;cacheVariables&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;CMAKE_TOOLCHAIN_FILE&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$env&#123;VCPKG_ROOT&#125;/scripts/buildsystems/vcpkg.cmake&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里就需要在环境变量中设置<code>VCPKG_ROOT</code></p>
</blockquote>
<p>有了<code>CMakePresets.json</code>后，使用以下命令，会对cmake build
做一下一些前置的设置操作：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake --preset=default</span><br></pre></td></tr></table></figure>
<h2 id="编译">编译</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake --build build</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2024/03/05/%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85nasm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/03/05/%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85nasm/" class="post-title-link" itemprop="url">源码编译安装nasm</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-03-05 11:26:47 / 修改时间：11:36:40" itemprop="dateCreated datePublished" datetime="2024-03-05T11:26:47+08:00">2024-03-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B1%87%E7%BC%96/" itemprop="url" rel="index"><span itemprop="name">汇编</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="从源代码编译安装-nasm">从源代码编译安装 Nasm</h1>
<h2 id="下载-nasm-源代码">下载 Nasm 源代码</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://www.nasm.us/pub/nasm/releasebuilds/nasm-2.15.05.tar.gz</span><br></pre></td></tr></table></figure>
<h2 id="解压源代码">解压源代码</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -xvzf nasm-2.15.05.tar.gz</span><br></pre></td></tr></table></figure>
<h2 id="进入源代码目录">进入源代码目录</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd nasm-2.15.05</span><br></pre></td></tr></table></figure>
<h2 id="配置编译环境">配置编译环境</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./configure</span><br></pre></td></tr></table></figure>
<h2 id="编译安装">编译安装</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make &amp;&amp; sudo make install</span><br></pre></td></tr></table></figure>
<h2 id="验证-nasm-版本">验证 Nasm 版本</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nasm --version</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2024/02/19/Docker%E7%9F%A5%E8%AF%86%E7%82%B901/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/02/19/Docker%E7%9F%A5%E8%AF%86%E7%82%B901/" class="post-title-link" itemprop="url">Docker知识点01</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-02-19 09:16:00" itemprop="dateCreated datePublished" datetime="2024-02-19T09:16:00+08:00">2024-02-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-05-24 16:30:34" itemprop="dateModified" datetime="2024-05-24T16:30:34+08:00">2024-05-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/" itemprop="url" rel="index"><span itemprop="name">云原生</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/" itemprop="url" rel="index"><span itemprop="name">基础设施</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="名词">名词</h1>
<table>
<colgroup>
<col style="width: 4%">
<col style="width: 95%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">名词</th>
<th style="text-align: left;">说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Bridge 网络</td>
<td style="text-align: left;">1. Bridge 网络是 Docker
默认创建的网络模式。当您在 Docker
中创建一个新的容器时，默认会将容器连接到名为 bridge 的网络中 <br> 2.
Bridge
网络允许容器与宿主机以及同一主机上的其他容器进行通信，但默认情况下不允许容器之间跨主机通信
<br> 3. Bridge 网络使用 Linux
系统上的桥接技术，在宿主机上创建一个虚拟的网桥设备，用于连接容器和主机的物理网络接口
<br> 4. Bridge 网络适用于单个主机上的容器通信</td>
</tr>
<tr class="even">
<td style="text-align: left;">Overlay 网络</td>
<td style="text-align: left;">1. Overlay 网络用于跨多个 Docker
主机构建容器集群，并允许集群中的容器之间进行跨主机通信 <br> 2. Overlay
网络使用了 VxLAN 技术，在不同主机上创建虚拟的 Overlay
网络，容器可以通过该网络进行通信 <br> 3. Overlay 网络通常用于 Docker
Swarm 集群或 Kubernetes 集群中，用于构建分布式应用和微服务架构 <br> 4.
Overlay 网络适用于跨多个主机的容器通信</td>
</tr>
<tr class="odd">
<td style="text-align: left;">MTU</td>
<td style="text-align: left;">最大传输单元MTU（Maximum Transmission
Unit，MTU），是指网络能够传输的最大数据包大小，以字节为单位。MTU的大小决定了发送端一次能够发送报文的最大字节数。如果MTU超过了接收端所能够承受的最大值，或者是超过了发送路径上途经的某台设备所能够承受的最大值，就会造成报文分片甚至丢弃，加重网络传输的负担。如果太小，那实际传送的数据量就会过小，影响传输效率。</td>
</tr>
</tbody>
</table>
<h1 id="overlay网络">Overlay网络</h1>
<p>docker
swarm在启动的过程中会创建两个默认的网络：docker_gwbridge和ingress.</p>
<ul>
<li><strong>docker_gwbridge</strong>：通过这个网络，容器可以连接到宿主机。（它的driver就是bridge)</li>
<li><strong>ingress</strong>：由docker
swarm创建的overlay网络，这个网络用于将服务暴露给外部访问，docker
swarm就是通过它实现的routing
mesh（将外部请求路由到不同主机的容器）。</li>
</ul>
<p>例子如下： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[centos@bd-2 ~]$ docker network ls</span><br><span class="line">NETWORK ID     NAME                           DRIVER    SCOPE</span><br><span class="line">69477a3560e9   bridge                         bridge    local</span><br><span class="line">320cd9ae8475   centos_default                 bridge    local</span><br><span class="line">d344065cc4cc   docker_gwbridge                bridge    local</span><br><span class="line">da5d24e5d251   flow-admin-backend_admin-net   bridge    local</span><br><span class="line">4d5b92424942   flow-admin-front_admin-net     bridge    local</span><br><span class="line">qqfrntz0lrex   hadoop-com                     overlay   swarm</span><br><span class="line">5cd8e1bc97a5   host                           host      local</span><br><span class="line">mtysig457puz   ingress                        overlay   swarm</span><br><span class="line">7ad98d08bd3c   none                           null      local  </span><br></pre></td></tr></table></figure></p>
<h1 id="docker0的mtu">docker0的mtu</h1>
<p>mtu查看 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig | grep mtu</span><br></pre></td></tr></table></figure>
如果宿主机的mtu比docker0的mtu还小，网络会存在问题，所以是需要修改docker0的mtu</p>
<p>我直接通过修改/etc/docker/daemon.json文件： <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;log-driver&quot;</span><span class="punctuation">:</span><span class="string">&quot;json-file&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;log-opts&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;max-size&quot;</span><span class="punctuation">:</span><span class="string">&quot;200m&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;max-file&quot;</span><span class="punctuation">:</span><span class="string">&quot;3&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;live-restore&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;mtu&quot;</span><span class="punctuation">:</span> <span class="number">1450</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;insecure-registries&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;192.168.10.166:88&quot;</span><span class="punctuation">,</span><span class="string">&quot;192.168.9.68&quot;</span><span class="punctuation">,</span><span class="string">&quot;192.168.11.149:8888&quot;</span><span class="punctuation">,</span><span class="string">&quot;192.168.11.168:8888&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></p>
<p>重启docker</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure>
<p>重新查看mtu，如果docker中此时没有容器在运行，那会发现docker0的mtu还是原先的值。这里有坑：当docker中有容器在运行时，再去查看docker0的mtu就生效了。
强迫症可以修改daemon.json配置，重启docker服务后，再使用命令临时修改docker0的mtu：
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip link set docker0 mtu 1450</span><br></pre></td></tr></table></figure></p>
<h1 id="swarm集群搭建">Swarm集群搭建</h1>
<p>初始化manager节点 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker swarm init --advertise-addr 192.168.10.91:2377</span><br></pre></td></tr></table></figure> 正常情况输出类似以下的提示：
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Swarm initialized: current node (tg66scjfm2kgw5z7pb7vpnrye) is now a manager.</span><br><span class="line"></span><br><span class="line">To add a worker to this swarm, run the following command:</span><br><span class="line"></span><br><span class="line">    docker swarm join --token SWMTKN-1-4e22sso1h4dofwqzq25a6crgc9fzk4it6237jdd6ezuzqknsw5-79qgv3b9sc88wa8jk5etylpty 192.168.10.91:2377</span><br><span class="line"></span><br><span class="line">To add a manager to this swarm, run &#x27;docker swarm join-token manager&#x27; and follow the instructions.</span><br></pre></td></tr></table></figure></p>
<p>后续想要查看怎么加入到swarm集群，使用以下命令: <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取Manager节点加入命令</span></span><br><span class="line">docker swarm join-token manager</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取Worker节点加入命令</span></span><br><span class="line">docker swarm join-token worker</span><br></pre></td></tr></table></figure></p>
<p>可以到work节点上执行： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker swarm join --token SWMTKN-1-4e22sso1h4dofwqzq25a6crgc9fzk4it6237jdd6ezuzqknsw5-79qgv3b9sc88wa8jk5etylpty 192.168.10.91:2377</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>如果是多manager的话，可以在其它manager节点执行： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker swarm join --token SWMTKN-1-4e22sso1h4dofwqzq25a6crgc9fzk4it6237jdd6ezuzqknsw5-2ccx8qd17rc0ydci0kcw9qfy2 192.168.10.91:2377</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>这里有坑，就是docker的daemon.json中配置的mtu，对于swarm的overlay网络是不起作用的。</p>
</blockquote>
<p>修改overlay网络的mtu和eth0,docker0的mtu保持一致 1.
在manager节点中先获取子网信息 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network inspect -f &#x27;&#123;&#123;json .IPAM&#125;&#125;&#x27; docker_gwbridge</span><br></pre></td></tr></table></figure> 返回： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;Driver&quot;:&quot;default&quot;,&quot;Options&quot;:null,&quot;Config&quot;:[&#123;&quot;Subnet&quot;:&quot;172.18.0.0/16&quot;,&quot;Gateway&quot;:&quot;172.18.0.1&quot;&#125;]&#125; </span><br></pre></td></tr></table></figure> 2.
manager节点退出swarm集群(自定义docker_gwbridge网络，则必须在将 Docker
主机加入 swarm 之前或暂时从 swarm 中移除后进行) <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker swarm leave --force</span><br></pre></td></tr></table></figure> 3.
manager节点停掉docker服务 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl stop docker.service</span><br></pre></td></tr></table></figure> 4.
manager节点中删掉虚拟网卡docker_gwbridge <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo ip link set docker_gwbridge down</span><br><span class="line">sudo ip link del dev docker_gwbridge</span><br></pre></td></tr></table></figure> 5.
manager节点中启动docker <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl start docker.service</span><br></pre></td></tr></table></figure> 6.
manager节点中重建docker_gwbridge（这一步用到了第1步获取的子网信息以及设置我们要的mtu值）
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">docker network rm docker_gwbridge</span><br><span class="line">docker network create \</span><br><span class="line">  --subnet 172.18.0.0/16 \</span><br><span class="line">  --gateway 172.18.0.1 \</span><br><span class="line">  --opt com.docker.network.bridge.name=docker_gwbridge \</span><br><span class="line">  --opt com.docker.network.bridge.enable_icc=false \</span><br><span class="line">  --opt com.docker.network.bridge.enable_ip_masquerade=true \</span><br><span class="line">  --opt com.docker.network.driver.mtu=1450 \</span><br><span class="line">  docker_gwbridge</span><br></pre></td></tr></table></figure>
<strong>再到work节点上执行相同的命令执行1~6步骤</strong>完成docker_gwbridge网络的自定义创建</p>
<ol start="7" type="1">
<li><p>manager节点中查看ingress网络信息 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network inspect -f &#x27;&#123;&#123;json .IPAM&#125;&#125;&#x27; ingress</span><br></pre></td></tr></table></figure> 返回
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;Driver&quot;:&quot;default&quot;,&quot;Options&quot;:null,&quot;Config&quot;:[&#123;&quot;Subnet&quot;:&quot;10.0.0.0/24&quot;,&quot;Gateway&quot;:&quot;10.0.0.1&quot;&#125;]&#125;</span><br></pre></td></tr></table></figure></p></li>
<li><p>manager节点中删除ingress network <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network rm ingress</span><br></pre></td></tr></table></figure> &gt;
这一步会有个警告</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WARNING! Before removing the routing-mesh network, make sure all the nodes in your swarm run the same docker engine version. Otherwise, removal may not be effective and functionality of newly create ingress networks will be impaired.</span><br></pre></td></tr></table></figure>
<blockquote>
<p>所以swarm中的节点的docker版本最好保持一致</p>
</blockquote></li>
<li><p>manager节点中重建ingress(记得使用之前查看的ingress网络中的子网以及网关，mtu自定义修改)
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker network create \</span><br><span class="line">  --driver overlay \</span><br><span class="line">  --ingress \</span><br><span class="line">  --subnet=10.0.0.0/24 \</span><br><span class="line">  --gateway=10.0.0.1 \</span><br><span class="line">  --opt com.docker.network.driver.mtu=1450 \</span><br><span class="line">  ingress</span><br></pre></td></tr></table></figure></p></li>
<li><p>然后就是其它的manager节点/worker节点的加入了 &gt;
<strong>注意：新机器在join到swarm之前，得先重建docker_gwbridge(mtu得保持一致）</strong></p></li>
<li><p>验证mtu是否都按我们定义的修改了 启动一个swarm service
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker service create -td --name busybox busybox</span><br></pre></td></tr></table></figure> 查看mtu <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig | grep mtu</span><br></pre></td></tr></table></figure> 返回 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1450</span><br><span class="line">docker_gwbridge: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1450</span><br><span class="line">eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1450</span><br><span class="line">lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536</span><br><span class="line">veth21384b6: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1450</span><br><span class="line">veth41d39c5: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1450</span><br></pre></td></tr></table></figure></p></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2024/02/18/nfs%E5%85%B1%E4%BA%AB%E7%9B%AE%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/02/18/nfs%E5%85%B1%E4%BA%AB%E7%9B%AE%E5%BD%95/" class="post-title-link" itemprop="url">nfs共享目录</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-02-18 15:17:59 / 修改时间：15:49:14" itemprop="dateCreated datePublished" datetime="2024-02-18T15:17:59+08:00">2024-02-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="nfs-服务">nfs 服务</h1>
<ol type="1">
<li><p>安装 NFS 服务器软件包：首先，在 Fedora 系统上安装 NFS
服务器软件包。NFS
服务器软件包通常是<code>nfs-utils</code>，你可以使用以下命令安装：
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo dnf install nfs-utils</span><br><span class="line"></span><br></pre></td></tr></table></figure></p></li>
<li><p>配置<code>NFS</code>服务器：<code>NFS</code>
服务器的配置文件是<code>/etc/exports</code>，你需要编辑这个文件来指定要共享的目录和相关的共享选项。比如，你可以使用<code>vi</code>或者其他文本编辑器来编辑<code>/etc/exports</code>文件：</p>
<p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/exports</span><br></pre></td></tr></table></figure></p>
<p>在打开的文件中，你可以添加类似如下的行来指定共享的目录和相关选项：
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/home/maodaoming/nfs_share/volume *(rw,sync,no_root_squash,no_subtree_check,insecure)</span><br></pre></td></tr></table></figure> 这行表示将 /home/maodaoming/nfs_share/volume
目录共享给所有客户端,还定义了相应的权限</p></li>
<li><p>重载 NFS 服务器配置：编辑完成 /etc/exports 文件后，你需要重新加载
NFS 服务器配置使之生效。你可以使用以下命令来重新加载： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo exportfs -r</span><br></pre></td></tr></table></figure>
这个命令会重新加载 /etc/exports
文件中的配置，并使新的共享目录生效。</p></li>
<li><p>启动 NFS 服务器：使用以下命令启动 NFS 服务器：
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl start nfs-server</span><br></pre></td></tr></table></figure></p></li>
<li><p>设置开机启动 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl enable nfs-server</span><br></pre></td></tr></table></figure></p></li>
</ol>
<h1 id="客户端">客户端</h1>
<ol type="1">
<li><p>创建本地挂载点：在 macOS 上创建一个本地目录，用于挂载 NFS
共享。例如，你可以在 macOS 上创建一个名为 /mnt/nfs_share 的目录：
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /mnt/nfs_share</span><br></pre></td></tr></table></figure></p></li>
<li><p>挂载 NFS 共享：使用 mount 命令挂载 NFS 共享。你需要指定 NFS
服务器的 IP 地址（或主机名）以及共享的远程目录和本地挂载点。例如：
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t nfs &lt;NFS服务器IP&gt;:/home/maodaoming/nfs_share/volume /mnt/nfs_share</span><br></pre></td></tr></table></figure></p></li>
<li><p><del>卸载挂载点</del> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo umount /mnt/nfs_share</span><br></pre></td></tr></table></figure></p></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2023/12/14/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Window-%E6%9C%BA%E5%88%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/14/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Window-%E6%9C%BA%E5%88%B6/" class="post-title-link" itemprop="url">Flink原理与实现:Window机制</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-12-14 10:58:46 / 修改时间：17:54:52" itemprop="dateCreated datePublished" datetime="2023-12-14T10:58:46+08:00">2023-12-14</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/" itemprop="url" rel="index"><span itemprop="name">Flink</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="简介">简介</h1>
<p>Flink 认为 Batch 是 Streaming 的一个特例，所以 Flink
底层引擎是一个流式引擎，在上面实现了流处理和批处理。而窗口（window）就是从
Streaming 到 Batch 的一个桥梁。Flink
提供了非常完善的窗口机制，这是我认为的 Flink
最大的亮点之一（其他的亮点包括消息乱序处理，和 checkpoint
机制）。本文我们将介绍流式处理中的窗口概念，介绍 Flink 内建的一些窗口和
Window API，最后讨论下窗口在底层是如何实现的。</p>
<h1 id="什么是-window">什么是 Window</h1>
<p>在流处理应用中，数据是连续不断的，因此我们不可能等到所有数据都到了才开始处理。当然我们可以每来一个消息就处理一次，但是有时我们需要做一些聚合类的处理，例如：在过去的1分钟内有多少用户点击了我们的网页。在这种情况下，我们必须定义一个窗口，用来收集最近一分钟内的数据，并对这个窗口内的数据进行计算。</p>
<p>窗口可以是时间驱动的（Time
Window，例如：每30秒钟），也可以是数据驱动的（Count
Window，例如：每一百个元素）。一种经典的窗口分类可以分成：翻滚窗口（Tumbling
Window，无重叠），滚动窗口（Sliding
Window，有重叠），和会话窗口（Session Window，活动间隙）。</p>
<p>我们举个具体的场景来形象地理解不同窗口的概念。假设，淘宝网会记录每个用户每次购买的商品个数，我们要做的是统计不同窗口中用户购买商品的总数。下图给出了几种经典的窗口切分概述图：</p>
<figure>
<img src="/2023/12/14/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Window-%E6%9C%BA%E5%88%B6/Snipaste_2023-12-14_11-17-27.png" alt="窗口切分概述图">
<figcaption aria-hidden="true">窗口切分概述图</figcaption>
</figure>
<p>上图中，raw data stream
代表用户的购买行为流，圈中的数字代表该用户本次购买的商品个数，事件是按时间分布的，所以可以看出事件之间是有time
gap的。Flink 提供了上图中所有的窗口类型，下面我们会逐一进行介绍。</p>
<h1 id="time-window">Time Window</h1>
<p>就如名字所说的，Time Window
是根据时间对数据流进行分组的。这里我们涉及到了流处理中的时间问题，时间问题和消息乱序问题是紧密关联的，这是流处理中现存的难题之一，我们将在后续的
EventTime 和消息乱序处理 中对这部分问题进行深入探讨。这里我们只需要知道
Flink 提出了三种时间的概念，分别是event
time（事件时间：事件发生时的时间），ingestion
time（摄取时间：事件进入流处理系统的时间），processing
time（处理时间：消息被计算处理的时间）。Flink
中窗口机制和时间类型是完全解耦的，也就是说当需要改变时间类型时不需要更改窗口逻辑相关的代码。</p>
<h2 id="tumbling-time-window">Tumbling Time Window</h2>
<p>如上图，我们需要统计每一分钟中用户购买的商品的总数，需要将用户的行为事件按每一分钟进行切分，这种切分被成为翻滚时间窗口（Tumbling
Time
Window）。翻滚窗口能将数据流切分成不重叠的窗口，每一个事件只能属于一个窗口。通过使用
DataStream API，我们可以这样实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Stream of (userId, buyCnt)</span></span><br><span class="line"><span class="keyword">val</span> buyCnts: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> tumblingCnts: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = buyCnts</span><br><span class="line">  <span class="comment">// key stream by userId</span></span><br><span class="line">  .keyBy(<span class="number">0</span>) </span><br><span class="line">  <span class="comment">// tumbling time window of 1 minute length</span></span><br><span class="line">  .timeWindow(<span class="type">Time</span>.minutes(<span class="number">1</span>))</span><br><span class="line">  <span class="comment">// compute sum over buyCnt</span></span><br><span class="line">  .sum(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="sliding-time-window">Sliding Time Window</h2>
<p>但是对于某些应用，它们需要的窗口是不间断的，需要平滑地进行窗口聚合。比如，我们可以每30秒计算一次最近一分钟用户购买的商品总数。这种窗口我们称为滑动时间窗口（Sliding
Time Window）。在滑窗中，一个元素可以对应多个窗口。通过使用 DataStream
API，我们可以这样实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> slidingCnts: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = buyCnts</span><br><span class="line">  .keyBy(<span class="number">0</span>) </span><br><span class="line">  <span class="comment">// sliding time window of 1 minute length and 30 secs trigger interval</span></span><br><span class="line">  .timeWindow(<span class="type">Time</span>.minutes(<span class="number">1</span>), <span class="type">Time</span>.seconds(<span class="number">30</span>))</span><br><span class="line">  .sum(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="count-window">Count Window</h1>
<p>Count Window 是根据元素个数对数据流进行分组的。</p>
<h2 id="tumbling-count-window">Tumbling Count Window</h2>
<p>当我们想要每100个用户购买行为事件统计购买总数，那么每当窗口中填满100个元素了，就会对窗口进行计算，这种窗口我们称之为翻滚计数窗口（Tumbling
Count Window），上图所示窗口大小为3个。通过使用 DataStream
API，我们可以这样实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Stream of (userId, buyCnts)</span></span><br><span class="line"><span class="keyword">val</span> buyCnts: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> tumblingCnts: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = buyCnts</span><br><span class="line">  <span class="comment">// key stream by sensorId</span></span><br><span class="line">  .keyBy(<span class="number">0</span>)</span><br><span class="line">  <span class="comment">// tumbling count window of 100 elements size</span></span><br><span class="line">  .countWindow(<span class="number">100</span>)</span><br><span class="line">  <span class="comment">// compute the buyCnt sum </span></span><br><span class="line">  .sum(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="sliding-count-window">Sliding Count Window</h2>
<p>当然Count Window 也支持 Sliding
Window，虽在上图中未描述出来，但和Sliding Time
Window含义是类似的，例如每10个元素计算一次最近100个元素的总和，代码示例如下。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> slidingCnts: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = vehicleCnts</span><br><span class="line">  .keyBy(<span class="number">0</span>)</span><br><span class="line">  <span class="comment">// sliding count window of 100 elements size and 10 elements trigger interval</span></span><br><span class="line">  .countWindow(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">  .sum(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="session-window">Session Window</h1>
<p>在这种用户交互事件流中，我们首先想到的是将事件聚合到会话窗口中（一段用户持续活跃的周期），由非活跃的间隙分隔开。如上图所示，就是需要计算每个用户在活跃期间总共购买的商品数量，如果用户30秒没有活动则视为会话断开（假设raw
data stream是单个用户的购买行为流）。Session Window 的示例代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Stream of (userId, buyCnts)</span></span><br><span class="line"><span class="keyword">val</span> buyCnts: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = ...</span><br><span class="line">    </span><br><span class="line"><span class="keyword">val</span> sessionCnts: <span class="type">DataStream</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = vehicleCnts</span><br><span class="line">    .keyBy(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">// session window based on a 30 seconds session gap interval </span></span><br><span class="line">    .window(<span class="type">ProcessingTimeSessionWindows</span>.withGap(<span class="type">Time</span>.seconds(<span class="number">30</span>)))</span><br><span class="line">    .sum(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>一般而言，window
是在无限的流上定义了一个有限的元素集合。这个集合可以是基于时间的，元素个数的，时间和个数结合的，会话间隙的，或者是自定义的。Flink
的 DataStream API
提供了简洁的算子来满足常用的窗口操作，同时提供了通用的窗口机制来允许用户自己定义窗口分配逻辑。下面我们会对
Flink 窗口相关的 API 进行剖析。</p>
<h1 id="剖析-window-api">剖析 Window API</h1>
<p>得益于 Flink Window API
松耦合设计，我们可以非常灵活地定义符合特定业务的窗口。Flink
中定义一个窗口主要需要以下三个组件。</p>
<ul>
<li><p>Window Assigner：用来决定某个元素被分配到哪个/哪些窗口中去。</p>
<p>如下类图展示了目前内置实现的 Window Assigners：</p>
<figure>
<img src="/2023/12/14/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Window-%E6%9C%BA%E5%88%B6/WindowAssigners.png" alt="WindowAssigners">
<figcaption aria-hidden="true">WindowAssigners</figcaption>
</figure></li>
<li><p>Trigger：触发器。决定了一个窗口何时能够被计算或清除，每个窗口都会拥有一个自己的Trigger。</p>
<p>如下类图展示了目前内置实现的 Triggers： <img src="/2023/12/14/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Window-%E6%9C%BA%E5%88%B6/Triggers.png" alt="Triggers"></p></li>
<li><p>Evictor：可以译为“驱逐者”。在Trigger触发之后，在窗口被处理之前，Evictor（如果有Evictor的话）会用来剔除窗口中不需要的元素，相当于一个filter。</p>
<p>如下类图展示了目前内置实现的 Evictors： <img src="/2023/12/14/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Window-%E6%9C%BA%E5%88%B6/Evictors.png" alt="Evictors"></p></li>
</ul>
<p>上述三个组件的不同实现的不同组合，可以定义出非常复杂的窗口。Flink
中内置的窗口也都是基于这三个组件构成的，当然内置窗口有时候无法解决用户特殊的需求，所以
Flink
也暴露了这些窗口机制的内部接口供用户实现自定义的窗口。下面我们将基于这三者探讨窗口的实现机制。</p>
<h1 id="window-的实现">Window 的实现</h1>
<p>下图描述了 Flink 的窗口机制以及各组件之间是如何相互工作的。</p>
<figure>
<img src="/2023/12/14/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Window-%E6%9C%BA%E5%88%B6/组件交互.png" alt="组件交互">
<figcaption aria-hidden="true">组件交互</figcaption>
</figure>
<p>首先上图中的组件都位于一个算子（window
operator）中，数据流源源不断地进入算子，每一个到达的元素都会被交给
WindowAssigner。WindowAssigner
会决定元素被放到哪个或哪些窗口（window），可能会创建新窗口。因为一个元素可以被放入多个窗口中，所以同时存在多个窗口是可能的。注意，Window本身只是一个ID标识符，其内部可能存储了一些元数据，如TimeWindow中有开始和结束时间，但是并不会存储窗口中的元素。窗口中的元素实际存储在
Key/Value State
中，key为Window，value为元素集合（或聚合值）。为了保证窗口的容错性，该实现依赖了
Flink 的 State 机制（参见 <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-master/apis/streaming/state.html?spm=a2c6h.12873639.article-detail.8.571d3dd5xKjhTf">state
文档</a>）。</p>
<p>每一个窗口都拥有一个属于自己的
Trigger，Trigger上会有定时器，用来决定一个窗口何时能够被计算或清除。每当有元素加入到该窗口，或者之前注册的定时器超时了，那么Trigger都会被调用。Trigger的返回结果可以是
continue（不做任何操作），fire（处理窗口数据），purge（移除窗口和窗口中的数据），或者
fire +
purge。一个Trigger的调用结果只是fire的话，那么会计算窗口并保留窗口原样，也就是说窗口中的数据仍然保留不变，等待下次Trigger
fire的时候再次执行计算。一个窗口可以被重复计算多次直到它被 purge
了。在purge之前，窗口会一直占用着内存。</p>
<p>当Trigger
fire了，窗口中的元素集合就会交给Evictor（如果指定了的话）。Evictor
主要用来遍历窗口中的元素列表，并决定最先进入窗口的多少个元素需要被移除。剩余的元素会交给用户指定的函数进行窗口的计算。如果没有
Evictor 的话，窗口中的所有元素会一起交给函数进行计算。</p>
<p>计算函数收到了窗口的元素（可能经过了 Evictor
的过滤），并计算出窗口的结果值，并发送给下游。窗口的结果值可以是一个也可以是多个。DataStream
API 上可以接收不同类型的计算函数，包括预定义的sum(),min(),max()，还有
ReduceFunction，FoldFunction，还有WindowFunction。WindowFunction
是最通用的计算函数，其他的预定义的函数基本都是基于该函数实现的。</p>
<p>Flink
对于一些聚合类的窗口计算（如sum,min）做了优化，因为聚合类的计算不需要将窗口中的所有数据都保存下来，只需要保存一个result值就可以了。每个进入窗口的元素都会执行一次聚合函数并修改result值。这样可以大大降低内存的消耗并提升性能。但是如果用户定义了
Evictor，则不会启用对聚合窗口的优化，因为 Evictor
需要遍历窗口中的所有元素，必须要将窗口中所有元素都存下来。</p>
<h1 id="源码分析">源码分析</h1>
<p>上述的三个组件构成了 Flink
的窗口机制。为了更清楚地描述窗口机制，以及解开一些疑惑（比如 purge 和
Evictor 的区别和用途），我们将一步步地解释 Flink 内置的一些窗口（Time
Window，Count Window，Session Window）是如何实现的。</p>
<h2 id="count-window-实现">Count Window 实现</h2>
<p>Count Window 是使用三组件的典范，我们可以在 KeyedStream 上创建 Count
Window，其源码如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// tumbling count window</span></span><br><span class="line"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title function_">countWindow</span><span class="params">(<span class="type">long</span> size)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create())  <span class="comment">// create window stream using GlobalWindows</span></span><br><span class="line">        .trigger(PurgingTrigger.of(CountTrigger.of(size))); <span class="comment">// trigger is window size</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// sliding count window</span></span><br><span class="line"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title function_">countWindow</span><span class="params">(<span class="type">long</span> size, <span class="type">long</span> slide)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> window(GlobalWindows.create())</span><br><span class="line">        .evictor(CountEvictor.of(size))  <span class="comment">// evictor is window size</span></span><br><span class="line">        .trigger(CountTrigger.of(slide)); <span class="comment">// trigger is slide size</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第一个函数是申请翻滚计数窗口，参数为窗口大小。第二个函数是申请滑动计数窗口，参数分别为窗口大小和滑动大小。它们都是基于
GlobalWindows 这个 WindowAssigner
来创建的窗口，该assigner会将所有元素都分配到同一个global
window中，所有GlobalWindows的返回值一直是 GlobalWindow
单例。基本上自定义的窗口都会基于该assigner实现。</p>
<p>翻滚计数窗口并不带evictor，只注册了一个trigger。该trigger是带purge功能的
CountTrigger。也就是说每当窗口中的元素数量达到了
window-size，trigger就会返回fire+purge，窗口就会执行计算并清空窗口中的所有元素，再接着储备新的元素。从而实现了tumbling的窗口之间无重叠。</p>
<p>滑动计数窗口的各窗口之间是有重叠的，但我们用的 GlobalWindows assinger
从始至终只有一个窗口，不像 sliding time assigner
可以同时存在多个窗口。所以trigger结果不能带purge，也就是说计算完窗口后窗口中的数据要保留下来（供下个滑窗使用）。另外，trigger的间隔是slide-size，evictor的保留的元素个数是window-size。也就是说，每个滑动间隔就触发一次窗口计算，并保留下最新进入窗口的window-size个元素，剔除旧元素。</p>
<p>假设有一个滑动计数窗口，每2个元素计算一次最近4个元素的总和，那么窗口工作示意图如下所示：</p>
<figure>
<img src="/2023/12/14/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Window-%E6%9C%BA%E5%88%B6/窗口工作示意图.png" alt="窗口工作示意图">
<figcaption aria-hidden="true">窗口工作示意图</figcaption>
</figure>
<p>图中所示的各个窗口逻辑上是不同的窗口，但在物理上是同一个窗口。该滑动计数窗口，trigger的触发条件是元素个数达到2个（每进入2个元素就会触发一次），evictor保留的元素个数是4个，每次计算完窗口总和后会保留剩余的元素。所以第一次触发trigger是当元素5进入，第三次触发trigger是当元素2进入，并驱逐5和2，计算剩余的4个元素的总和（22）并发送出去，保留下2,4,9,7元素供下个逻辑窗口使用。</p>
<h2 id="time-window-实现">Time Window 实现</h2>
<p>同样的，我们也可以在 KeyedStream 上申请 Time
Window，其源码如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// tumbling time window</span></span><br><span class="line"><span class="keyword">public</span> WindowedStream&lt;T, KEY, TimeWindow&gt; <span class="title function_">timeWindow</span><span class="params">(Time size)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">        <span class="keyword">return</span> window(TumblingProcessingTimeWindows.of(size));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> window(TumblingEventTimeWindows.of(size));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// sliding time window</span></span><br><span class="line"><span class="keyword">public</span> WindowedStream&lt;T, KEY, TimeWindow&gt; <span class="title function_">timeWindow</span><span class="params">(Time size, Time slide)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">        <span class="keyword">return</span> window(SlidingProcessingTimeWindows.of(size, slide));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> window(SlidingEventTimeWindows.of(size, slide));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在方法体内部会根据当前环境注册的时间类型，使用不同的WindowAssigner创建window。可以看到，EventTime和IngestTime都使用了XXXEventTimeWindows这个assigner，因为EventTime和IngestTime在底层的实现上只是在Source处为Record打时间戳的实现不同，在window
operator中的处理逻辑是一样的。</p>
<p>这里我们主要分析sliding process time window，如下是相关源码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SlidingProcessingTimeWindows</span> <span class="keyword">extends</span> <span class="title class_">WindowAssigner</span>&lt;Object, TimeWindow&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> size;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> slide;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="title function_">SlidingProcessingTimeWindows</span><span class="params">(<span class="type">long</span> size, <span class="type">long</span> slide)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.size = size;</span><br><span class="line">        <span class="built_in">this</span>.slide = slide;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Collection&lt;TimeWindow&gt; <span class="title function_">assignWindows</span><span class="params">(Object element, <span class="type">long</span> timestamp)</span> &#123;</span><br><span class="line">        timestamp = System.currentTimeMillis();</span><br><span class="line">        List&lt;TimeWindow&gt; windows = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;((<span class="type">int</span>) (size / slide));</span><br><span class="line">        <span class="comment">// 对齐时间戳</span></span><br><span class="line">        <span class="type">long</span> <span class="variable">lastStart</span> <span class="operator">=</span> timestamp - timestamp % slide;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> lastStart;</span><br><span class="line">            start &gt; timestamp - size;</span><br><span class="line">            start -= slide) &#123;</span><br><span class="line">            <span class="comment">// 当前时间戳对应了多个window</span></span><br><span class="line">            windows.add(<span class="keyword">new</span> <span class="title class_">TimeWindow</span>(start, start + size));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> windows;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProcessingTimeTrigger</span> <span class="keyword">extends</span> <span class="title class_">Trigger</span>&lt;Object, TimeWindow&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 每个元素进入窗口都会调用该方法</span></span><br><span class="line">    <span class="keyword">public</span> TriggerResult <span class="title function_">onElement</span><span class="params">(Object element, <span class="type">long</span> timestamp, TimeWindow window, TriggerContext ctx)</span> &#123;</span><br><span class="line">        <span class="comment">// 注册定时器，当系统时间到达window end timestamp时会回调该trigger的onProcessingTime方法</span></span><br><span class="line">        ctx.registerProcessingTimeTimer(window.getEnd());</span><br><span class="line">        <span class="keyword">return</span> TriggerResult.CONTINUE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 返回结果表示执行窗口计算并清空窗口</span></span><br><span class="line">    <span class="keyword">public</span> TriggerResult <span class="title function_">onProcessingTime</span><span class="params">(<span class="type">long</span> time, TimeWindow window, TriggerContext ctx)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> TriggerResult.FIRE_AND_PURGE;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先，SlidingProcessingTimeWindows会对每个进入窗口的元素根据系统时间分配到(size
/
slide)个不同的窗口，并会在每个窗口上根据窗口结束时间注册一个定时器（相同窗口只会注册一份），当定时器超时时意味着该窗口完成了，这时会回调对应窗口的Trigger的onProcessingTime方法，返回FIRE_AND_PURGE，也就是会执行窗口计算并清空窗口。整个过程示意图如下：</p>
<figure>
<img src="/2023/12/14/Flink-%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0-Window-%E6%9C%BA%E5%88%B6/202312141604.png" alt="示意图">
<figcaption aria-hidden="true">示意图</figcaption>
</figure>
<p>如上图所示横轴代表时间戳（为简化问题，时间戳从0开始），第一条record会被分配到[-5,5)和[0,10)两个窗口中，当系统时间到5时，就会计算[-5,5)窗口中的数据，并将结果发送出去，最后清空窗口中的数据，释放该窗口资源。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2023/12/06/Storm%E3%80%81Spark%E4%B8%8EFlink%E5%AF%B9%E6%AF%94/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/06/Storm%E3%80%81Spark%E4%B8%8EFlink%E5%AF%B9%E6%AF%94/" class="post-title-link" itemprop="url">Storm、Spark与Flink对比</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-06 16:06:42" itemprop="dateCreated datePublished" datetime="2023-12-06T16:06:42+08:00">2023-12-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-12-08 11:43:04" itemprop="dateModified" datetime="2023-12-08T11:43:04+08:00">2023-12-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/" itemprop="url" rel="index"><span itemprop="name">Flink</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="引言">引言</h1>
<p>Apache Flink(以下简称flink) 是一个旨在提供‘一站式’
的分布式开源数据处理框架。和Spark的目标一样，都希望提供一个统一功能的计算平台给用户。虽然目标非常类似，但是flink在实现上和spark存在着很大的区别，flink是一个面向流的处理框架，输入在flink中是无界的，流数据是flink中的头等公民。这方面flink和storm有几分相似。那么有spark和storm这样成熟的计算框架存在，为什么flink还能占有一席之地呢？今天我们就从流处理的角度将flink和这两个框架进行一些分析和比较。</p>
<h1 id="名词解析">名词解析</h1>
<table>
<colgroup>
<col style="width: 33%">
<col style="width: 16%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>名词</th>
<th>说明</th>
<th>举例</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>无边界数据</td>
<td>无边界数据通常指的是无限持续生成的数据流，没有明确定义的终点。这样的数据流可能是实时生成的事件、传感器数据、日志记录等，它们源源不断地产生，没有固定的结束点。</td>
<td>实时传感器数据，如温度传感器、网络日志，用户点击事件流等。这些数据是持续不断生成的，流式处理系统需要实时处理并适应新的数据产生。</td>
</tr>
<tr class="even">
<td>有边界数据</td>
<td>有边界数据是有明确定义的开始和结束点的数据集。这样的数据集在某一时刻是完整的，不再发生变化。传统的批处理任务就是对有边界数据进行处理的典型例子。</td>
<td>关系数据库中的表、存储在文件中的数据集等。这些数据集在某一时刻是完整的，可以通过一次性加载到内存中进行批处理分析。</td>
</tr>
<tr class="odd">
<td>Exactly-Once 语义</td>
<td>精确一次（Exactly-Once）处理语义，确保在出现故障或恢复时不会丢失或重复处理事件。这是保证数据处理的一致性的重要特性。</td>
<td></td>
</tr>
<tr class="even">
<td>有状态计算</td>
<td>有状态的计算指能够处理具有记忆和上下文关联的计算任务。这对于处理事件序列、实时聚合和复杂的模式匹配非常有用。</td>
<td></td>
</tr>
<tr class="odd">
<td>Sink</td>
<td>Sink是Flink中一种用于将数据从流处理应用程序发送到外部系统的组件，用于定义数据的最终目的地</td>
<td>File Sink（文件输出）; Kafka Sink; Elasticsearch Sink; JDBC Sink;
Custom Sink;</td>
</tr>
</tbody>
</table>
<h1 id="流框架基于的实现方式">流框架基于的实现方式</h1>
<p>本文涉及的流框架基于的实现方式分为两大类。</p>
<ul>
<li><p>第一类是Native
Streaming，这类引擎中所有的data在到来的时候就会被立即处理，一条接着一条（HINT：
狭隘的来说是一条接着一条，但流引擎有时会为提高性能缓存一小部分data然后一次性处理），其中的代表就是storm和flink。</p></li>
<li><p>第二类则是基于Micro-batch，数据流被切分为一个一个小的批次，
然后再逐个被引擎处理。这些batch一般是以时间为单位进行切分，单位一般是‘秒‘，其中的典型代表则是spark了，不论是老的spark
DStream还是2.0以后推出的spark structured
streaming都是这样的处理机制；另外一个基于Micro-batch实现的就是storm
trident，它是对storm的更高层的抽象，因为以batch为单位，所以storm
trident的一些处理变得简单且高效。</p></li>
</ul>
<figure>
<img src="/2023/12/06/Storm%E3%80%81Spark%E4%B8%8EFlink%E5%AF%B9%E6%AF%94/Snipaste_2023-12-06_17-54-25.png" alt="分类">
<figcaption aria-hidden="true">分类</figcaption>
</figure>
<h1 id="流框架比较的关键指标">流框架比较的关键指标</h1>
<p>从流处理的角度将flink与spark和storm这两个框架进行比较，会主要关注以下几点，后续的对比也主要基于这几点展开：</p>
<ul>
<li><p>功能性（Functionality）- 是否能很好解决流处理功能上的痛点 ,
比如event time和out of order data。</p></li>
<li><p>容错性（Fault Tolerance） -
在failure之后能否恢复到故障之前的状态，并输出一致的结果；此外容错的代价也是越低越好，因为其直接影响性能。</p></li>
<li><p>吞吐量(throughputs)&amp; 延时(latency) -
性能相关的指标，高吞吐和低延迟某种意义上是不可兼得的，但好的流引擎应能兼顾高吞吐&amp;低延时。</p></li>
</ul>
<h1 id="功能性">功能性</h1>
<h2 id="各类时间的定义">各类时间的定义</h2>
<p><a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.2/dev/event_time.html">flink文档中关于各种时间的定义</a>
- Event time:
指数据或事件真正发生时间，比如用户点击网页时产生一条点击事件的数据，点击时间就是这条数据固有的Event
time。理解为每个单独事件在其产生设备上发生的时间。 - Processing time:
指计算框架处理这条数据的时间 - Ingestion Time: 摄入时间是事件进入 Flink
的时间。</p>
<pre><code>![时钟](./Storm、Spark与Flink对比/times_clocks.svg)

spark DStream和storm 1.0以前版本往往都折中地使用processing time来近似地实现event time相关的业务。显然，使用processing time模拟event time必然会产生一些误差， 特别是在产生数据堆积的时候，误差则更明显，甚至导致计算结果不可用。

在使用event time时，自然而然需要解决由网络延迟等因素导致的迟到或者乱序数据的问题。为了解决这个问题， spark、storm及flink都参考[streaming 102](https://www.oreilly.com/radar/the-world-beyond-batch-streaming-102/)引入了watermark和lateness的概念。</code></pre>
<ul>
<li><p>watermark:
是引擎处理事件的时间进度，代表一种状态，一般随着数据中的event
time的增长而增长。比如 watermark(t)代表整个流的event
time处理进度已经到达t，
时间是有序的，那么streaming不应该会再收到timestamp t’ &lt;
t的数据，而只会接受到timestamp t’ &gt;= t的数据。 如果收到一条timestamp
t’ &lt; t的数据， 那么就说明这条数据是迟到的。</p></li>
<li><p>lateness:
表示可以容忍迟到的程度，在lateness可容忍范围内的数据还会参与计算，超过的会被丢弃。</p></li>
</ul>
<h2 id="窗口操作">窗口操作</h2>
<h3 id="spark-structured-streaming-和flink对event-time处理机制的比较">spark
structured streaming 和flink对event time处理机制的比较</h3>
<ul>
<li><p>Flink</p>
<p>首先，我们结合图来看flink， 时间轴从左往右增大。当watermark WM处于时
间窗口区间内时，即WM ∈ [start, end] , event
time落在窗口范围内的任何乱序数据都会被接受；随着WM的增长并超过了窗口的结束时间，但还未超过可容忍的lateness时间范围，即WM
∈ (window_end,window_end+ lateness]， 这时乱序数据仍然可以被接受；
只有当WM超过 window_end+lateness, 即WM ∈ (window_end+ lateness, ∞)，
迟到的数据将会被丢弃。</p>
<figure>
<img src="/2023/12/06/Storm%E3%80%81Spark%E4%B8%8EFlink%E5%AF%B9%E6%AF%94/handle-late-records.jpg" alt="handle-late-records">
<figcaption aria-hidden="true">handle-late-records</figcaption>
</figure>
<p>fiink中watermark的计算也比较灵活，可以选择build-in的（如最大时间戳），也可以通过继承接口自定义实现。此外，用户可以选择周期性更新或者事件触发更新watermark。</p></li>
<li><p>Spark</p>
<p>首先,spark中watermark是通过上一个batch最大的timestamp再减去lateness得到的，即watermark
= Max(last batch timestamps) - lateness。当数据的event
time大于watermark时，数据会被接受，否则不论这条数据属于哪个窗口都会被丢弃。细节请参考<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#window-operations-on-event-time">Window
Operations on Event Time</a></p></li>
</ul>
<p>下面来比较一下两者实现细节上的不同：</p>
<ul>
<li><p>lateness定义: 在spark中，迟到被定义为data的event
time和watermark的比较结果，当data的event time &lt;
watermark时，data被丢弃；flink中只有在watermark &gt; window_end +
lateness的时候，data才会被丢弃。</p></li>
<li><p>watermark更新: spark中watermark是上个batch中的max event
time，存在延迟；而在flink中是可以做到每条数据同步更新watermark。</p></li>
<li><p>window触发: flink中window计算会触发一次或多次，第一次在watermark
&gt;= window_end后立刻触发（main
fire），接着会在迟到数据到来后进行增量触发。spark只会在watermark（包含lateness）过了window_end之后才会触发，虽然计算结果一次性正确，但触发比flink起码多了一个lateness的延迟。</p></li>
</ul>
<p>上面三点可见flink在设计event
time处理模型还是较优的：watermark的计算实时性高，输出延迟低，而且接受迟到数据没有spark那么受限。</p>
<h3 id="sql-api方面的对比">SQL API方面的对比</h3>
<p>待续，网上资料都是较老版本的比对，不太准确，等自己实践过再来总结吧</p>
<h3 id="kafka-集成">Kafka 集成</h3>
<p>待续</p>
<h3 id="静态数据操作">静态数据操作</h3>
<p>待续</p>
<h1 id="吞吐量">吞吐量</h1>
<p>待续</p>
<h1 id="总结">总结</h1>
<p>待续</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2023/10/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%A6%82%E7%8E%87%E5%92%8C%E5%8F%91%E7%94%9F%E6%AF%94%E4%BB%A5%E5%8F%8ALogit/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/10/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%A6%82%E7%8E%87%E5%92%8C%E5%8F%91%E7%94%9F%E6%AF%94%E4%BB%A5%E5%8F%8ALogit/" class="post-title-link" itemprop="url">机器学习基础-概率和发生比以及Logit</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-10-31 18:57:17" itemprop="dateCreated datePublished" datetime="2023-10-31T18:57:17+08:00">2023-10-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-11-01 10:26:56" itemprop="dateModified" datetime="2023-11-01T10:26:56+08:00">2023-11-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">联邦学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">机器学习基础</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <hr>
<p>概率是机器学习中最常见的概念，在分类算法(Classification)中经常出现，而Logit也是逻辑回归（Logistic
Regression）中的重要概念，本文将总结概率（Probability），发生比（Odds）和Logit（log(odds)）之间的关系。</p>
<h2 id="概率发生比与logit的定义">概率，发生比与Logit的定义</h2>
<h3 id="概率">概率</h3>
<p>概率是指一个事件发生的可能性，假设一个袋子里有若干红球和蓝球，用P来表示概率。</p>
<p><span class="math inline">\(P_{抽到红球}=\frac{红球数}{总个数}\)</span></p>
<h3 id="发生比">发生比</h3>
<p>发生比指一件事发生与其不发生的概率之比，同样以红球和篮球为例，用Odds来表示概率，则：</p>
<p><span class="math inline">\(Odds_{抽到红球}=\frac
{P_{抽到红球}}{1-P_{抽到红球}}\)</span></p>
<h3 id="logit">Logit</h3>
<p>Logit是给发生比取对数，即log(Odds)，其中log是自然对数，以红球篮球为例，其公式为:</p>
<p><span class="math inline">\(Logit=log(\frac
{P_{抽到红球}}{1-P_{抽到红球}})\)</span></p>
<h2 id="概率发生比与logit的关系">概率发生比与Logit的关系</h2>
<p>概率与发生比的关系较为简单，
概率是指事件发生的可能性，而发生比是指该事件发生与不发生的概率之比。但发生比与Logit的关系就并没有那么直观了，从公式上我们确实可以知道Logit就是log(发生比),<strong>但我们该如何理解这一定义呢？</strong></p>
<p>通过下面的图表不难发现，概率与发生比有以下关系：</p>
<ol type="1">
<li>概率取值为(0,1)，发生比取值为(0,Inf)</li>
<li>当概率为0.5时，发生比为1，即事件发生与不发生的概率相同。</li>
<li>当概率小于0.5时，发生比取值范围为(0,1)，当概率大于0.5时，发生比取值范围为(1,Inf)</li>
<li>当概率大于或小于0.5时，发生比的取值范围并不对称。</li>
</ol>
<p>发生比的不对称性，时常会带来一些困扰，比如：</p>
<ul>
<li>概率为0.3和0.7时，这两个概率的均值是0.5，他们是“对称”的；但这两个概率的发生比却分别是0.429和2.333,这使我们很难直观地感受到他们的对称关系。</li>
<li>当概率从0.1增加到0.2，时，发生比增加了0.139；当概率从0.8增加到0.9时，发生比却增加了5，虽然概率的增量相同，但发生比的增量却大大的不同。</li>
</ul>
<p>而Logit就能有效地解决发生比带来的困惑：</p>
<ul>
<li>概率为0.3和0.7时，Logit分别为-0.847和0.847，这两个值关于0对称,因此我们知道这两Logit值对应的发生比是对称的。</li>
<li>当概率从0.2减少至0.1时，Logit的增量是-0.811；当概率从0.8增加到0.9时，Logit的增量也是0.811，我们可以直观地感受到发生比增量也是对称的。</li>
<li>当发生比为1时，Logit为0；当发生比小于或大于1时，Logit取值范围为(-Inf,0)和(0,Inf).取值范围对称。</li>
</ul>
<p>下表列出了当概率取不同值时，发生比和Logit之间的关系：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Probability</th>
<th>Odds</th>
<th>Logit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.0001</td>
<td>0.00010001</td>
<td>-9.21024</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.001</td>
<td>0.001001</td>
<td>-6.90675</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.01</td>
<td>0.010101</td>
<td>-4.59512</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.1</td>
<td>0.111111</td>
<td>-2.19722</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.2</td>
<td>0.25</td>
<td>-1.38629</td>
</tr>
<tr class="even">
<td>5</td>
<td>0.3</td>
<td>0.428571</td>
<td>-0.847298</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.4</td>
<td>0.666667</td>
<td>-0.405465</td>
</tr>
<tr class="even">
<td>7</td>
<td>0.5</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>8</td>
<td>0.6</td>
<td>1.5</td>
<td>0.405465</td>
</tr>
<tr class="even">
<td>9</td>
<td>0.7</td>
<td>2.33333</td>
<td>0.847298</td>
</tr>
<tr class="odd">
<td>10</td>
<td>0.8</td>
<td>4</td>
<td>1.38629</td>
</tr>
<tr class="even">
<td>11</td>
<td>0.9</td>
<td>9</td>
<td>2.19722</td>
</tr>
<tr class="odd">
<td>12</td>
<td>0.99</td>
<td>9</td>
<td>4.59512</td>
</tr>
<tr class="even">
<td>13</td>
<td>0.999</td>
<td>999</td>
<td>6.90675</td>
</tr>
<tr class="odd">
<td>14</td>
<td>0.9999</td>
<td>9999</td>
<td>9.21024</td>
</tr>
</tbody>
</table>
<p>用图形表示，三者关系如下，其中x轴为概率，两条曲线分别为发生比和Logit，为了让图形更加好看，概率只取了[0.1,
0.9]这一部分：</p>
<figure>
<img src="/2023/10/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E6%A6%82%E7%8E%87%E5%92%8C%E5%8F%91%E7%94%9F%E6%AF%94%E4%BB%A5%E5%8F%8ALogit/概率-发生比-Logit.jpg" alt="概率和发生比以及Logit">
<figcaption aria-hidden="true">概率和发生比以及Logit</figcaption>
</figure>
<h2 id="小结">小结</h2>
<p>由于逻辑回归模型的需求，我们引入了发生比这一概念，但发生比本身的不对称性让我们难以直观地比较发生比，因此我们引入了Logit这一概念。<strong>Logit赋予了发生比更强的”对称“性</strong>，让我们更容易比较不同发生比之间的关系。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/" class="post-title-link" itemprop="url">PySpark基础</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-10-26 15:01:25" itemprop="dateCreated datePublished" datetime="2023-10-26T15:01:25+08:00">2023-10-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-11-10 10:20:21" itemprop="dateModified" datetime="2023-11-10T10:20:21+08:00">2023-11-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="简介">简介</h1>
<p>PySpark是Python调用Spark的接口，可以通过调用Python
API的方式来编写Spark程序，它支持了大多数的Spark功能，比如SparkDataFrame、Spark
SQL、Streaming、MLlib等等。</p>
<h1 id="基础概念">基础概念</h1>
<h2 id="rdd">RDD</h2>
<p>RDD的全称是Resilient Distributed
Datasets，这是Spark的一种数据抽象集合，它可以被执行在分布式的集群上进行各种操作，而且有较强的容错机制。RDD可以被分为若干个分区，每一个分区就是一个数据集片段，从而可以支持分布式计算。</p>
<h2 id="rdd运行时相关的关键名词">RDD运行时相关的关键名词</h2>
<p>简单来说可以有Client、Job、Master、Worker、Driver、Stage、Task以及Executor，这几个东西在调优的时候也会经常遇到的。</p>
<ul>
<li>Client: 指的是客户端进程，主要负责提交job到Master；</li>
<li>Job:
Job来自于我们编写的程序，Application包含一个或多个job，job包含各种RDD操作；</li>
<li>Master:
指的是Standalone模式中的主控节点，负责接收来自Client的job，并管理着worker，可以给worker分配任务和资源（主要是dirver和executor资源）；</li>
<li>Worker:
指的是Standalone模式中的slave节点，负责管理本节点的资源，同时受Master管理，需要定期给Master回报heartbeat（心跳），启动Driver和Executor；</li>
<li>Driver:
指的是job（作业）的主进程，一般每个Spark作业都会有一个Driver进程，负责整个作业的运行，包括job的解析、Stage的生成、调度Task到Executor上去执行；</li>
<li>Stage:
中文名是：阶段，是Job的基本调度单位，因为每个Job会分成若干组Task，每组task就被称为Stage；</li>
<li>Task:
任务，指的是直接运行在executor上的东西，是executor上的一个线程；</li>
<li>Executor:
指的是执行器，顾名思义就是真正执行任务的地方了，一个集群可以被配置若干个Executor，每个Executor接收来自Driver的Task，并执行它（可同时执行多个Task）。</li>
</ul>
<h2 id="dag">DAG</h2>
<p>全称是Directed Acyclic Graph,
中文名是：有向无环图。Spark就是借用了DAG对RDD之间的关系进行了建模，用来描述RDD之间的因果依赖关系。因为在一个Spark作业调度中，多个作业任务之间也是相互依赖的，有些任务需要在一些任务执行完成了才可以执行的。在Spark调度中就是有DAGscheduler，它负责将job分成若干组Task组成的Stage。
<img src="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/dag.jpg" alt="DAG"></p>
<h2 id="spark运行模式">Spark运行模式</h2>
<p>主要有local模式、Standalone模式、Mesos模式、YARN模式。 - Standalone:
独立模式，Spark 原生的简单集群管理器， 自带完整的服务，
可单独部署到一个集群中，无需依赖任何其他资源管理系统， 使用 Standalone
可以很方便地搭建一个集群，一般在公司内部没有搭建其他资源管理框架的时候才会使用。
- Mesos:
一个强大的分布式资源管理框架，它允许多种不同的框架部署在其上，包括
yarn，由于mesos这种方式目前应用的比较少。 - YARN: 统一的资源管理机制，
在上面可以运行多套计算框架， 如map reduce、storm 等， 根据 driver
在集群中的位置不同，分为 yarn client 和 yarn cluster。
实际上Spark内部为了方便用户测试，自身也提供了一些部署模式。由于在实际工厂环境下使用的绝大多数的集群管理器是
Hadoop YARN，因此我们关注的重点是 Hadoop YARN 模式下的 Spark
集群部署。</p>
<p>Spark 的运行模式取决于传递给 SparkContext 的 MASTER 环境变量的值，
个别模式还需要辅助的程序接口来配合使用，目前支持的 Master 字符串及 URL
包括：</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Master URL</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>local</td>
<td>在本地运行，只有一个工作进程，无并行计算能力</td>
</tr>
<tr class="even">
<td>local[K]</td>
<td>在本地运行，只有K个工作进程，通常设置K为机器的CPU核心数量</td>
</tr>
<tr class="odd">
<td>local[*]</td>
<td>在本地运行，工作进程数量等于机器的CPU核心数量</td>
</tr>
<tr class="even">
<td>spark://host:port</td>
<td>以 Standalone 模式运行，这是 Spark
自身提供的集群运行模式，默认端口号: 7077</td>
</tr>
<tr class="odd">
<td>mesos://host:port</td>
<td>在 Mesos 集群上运行，Driver 进程和 Worker 进程运行在 Mesos
集群上，部署模式必须使用固定值:--deploy-mode cluster</td>
</tr>
<tr class="even">
<td>yarn-client</td>
<td>在 Yarn 集群上运行，Driver 进程在本地， Work 进程在 Yarn 集群上，
部署模式必须使用固定值:--deploy-modeclient。Yarn
集群地址必须在HADOOP_CONF_DIRorYARN_CONF_DIR 变量里定义。</td>
</tr>
<tr class="odd">
<td>yarn-cluster</td>
<td>yarn-cluster 效率比yarn-client高。在 Yarn 集群上运行，Driver 进程在
Yarn 集群上，Work 进程也在 Yarn
集群上，部署模式必须使用固定值:--deploy-mode cluster。Yarn
集群地址必须在HADOOP_CONF_DIR or YARN_CONF_DIR 变量里定义。</td>
</tr>
</tbody>
</table>
<p><strong>用户在提交任务给 Spark 处理时，以下两个参数共同决定了 Spark
的运行方式。</strong></p>
<ul>
<li>master MASTER_URL: 决定了 Spark 任务提交给哪种集群处理。</li>
<li>deploy-mode DEPLOY_MODE: 决定了 Driver 的运行方式，可选值为 Client
或者 Cluster。</li>
</ul>
<h3 id="standalone模式运行机制">Standalone模式运行机制</h3>
<p>Standalone 集群有四个重要组成部分，分别是：</p>
<ol type="1">
<li>Driver： 是一个进程，我们编写的 Spark 应用程序就运行在 Driver 上，
由Driver 进程执行；</li>
<li>Master：是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责；</li>
<li>Worker：是一个进程，一个 Worker
运行在集群中的一台服务器上，主要负责两个职责，一个是用自己的内存存储 RDD
的某个或某些 partition；另一个是启动其他进程和线程（Executor） ，对 RDD
上的 partition 进行并行的处理和计算。</li>
<li>Executor：是一个进程， 一个 Worker 上可以运行多个 Executor，
Executor 通过启动多个线程（task）来执行对 RDD 的 partition
进行并行计算，也就是执行我们对 RDD 定义的例如 map、flatMap、reduce
等算子操作。</li>
</ol>
<h4 id="standalone-client模式">Standalone Client模式</h4>
<p>在 Standalone Client 模式下，Driver
在任务提交的本地机器上运行，Driver 启动后向 Master 注册应用程序，Master
根据 submit 脚本的资源需求找到内部资源至少可以启动一个 Executor 的所有
Worker，然后在这些 Worker 之间分配 Executor，Worker 上的 Executor
启动后会向 Driver 反向注册，所有的 Executor 注册完成后，Driver 开始执行
main 函数，之后执行到 Action 算子时，开始划分 stage，每个 stage
生成对应的 taskSet，之后将 task 分发到各个 Executor 上执行。</p>
<h4 id="standalone-cluster模式">Standalone Cluster模式</h4>
<p>在 Standalone Cluster 模式下，任务提交后，Master 会找到一个 Worker
启动 Driver进程， Driver 启动后向 Master 注册应用程序， Master 根据
submit 脚本的资源需求找到内部资源至少可以启动一个 Executor 的所有
Worker，然后在这些 Worker 之间分配 Executor，Worker 上的 Executor
启动后会向 Driver 反向注册，所有的 Executor 注册完成后，Driver 开始执行
main 函数，之后执行到 Action 算子时，开始划分 stage，每个 stage
生成对应的 taskSet，之后将 task 分发到各个 Executor 上执行。</p>
<blockquote>
<p>注意， Standalone 的两种模式下（ client/cluster）， Master 在接到
Driver 注册 Spark 应用程序的请求后，会获取其所管理的剩余资源能够启动一个
Executor 的所有 Worker， 然后在这些 Worker 之间分发
Executor，此时的分发只考虑 Worker
上的资源是否足够使用，直到当前应用程序所需的所有 Executor 都分配完毕，
Executor 反向注册完毕后，Driver 开始执行 main 程序。</p>
</blockquote>
<h3 id="yarn模式运行机制">YARN模式运行机制</h3>
<h4 id="yarn-client模式">YARN Client模式</h4>
<ol type="1">
<li>Driver在任务提交的本地机器上运行，Driver启动后会和ResourceManager通讯申请启动ApplicationMaster；</li>
<li>随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster的功能相当于一个ExecutorLaucher，只负责向ResourceManager申
请Executor内存；</li>
<li>ResourceManager接到ApplicationMaster的资源申请后会分配Container，然后ApplicationMaster在资源分配指定的NodeManager上启动Executor进程；</li>
<li>Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数；</li>
<li>之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分Stage，每个Stage生成对应的TaskSet，之后将Task分发到各个Executor上执行。</li>
</ol>
<figure>
<img src="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/yarn-client.png" alt="yarn-client模式">
<figcaption aria-hidden="true">yarn-client模式</figcaption>
</figure>
<h4 id="yarn-cluster-模式">YARN Cluster 模式</h4>
<ol type="1">
<li>任务提交后会和ResourceManager通讯申请启动ApplicationMaster;</li>
<li>随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster就是Driver；</li>
<li>Driver启动后向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster的资源申请后会分配Container,然后在合适的NodeManager上启动Executor进程;</li>
<li>Executor进程启动后会向Driver反向注册;</li>
<li>Executor全部注册完成后Driver开始执行main函数，之后执行到Action算子时，触发一个job，并根据宽依赖开始划分stage，每个stage生成对应的taskSet，之后将task分发到各个Executor上执行;</li>
</ol>
<figure>
<img src="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/yarn-cluster.jpg" alt="yarn-cluster模式">
<figcaption aria-hidden="true">yarn-cluster模式</figcaption>
</figure>
<h2 id="shuffle操作是什么">Shuffle操作是什么</h2>
<p>Shuffle指的是数据从Map端到Reduce端的数据传输过程，Shuffle性能的高低会直接影响程序的性能。因为Reduce
task需要跨节点去拉分布在不同节点上的Map
task计算结果，这一个过程是需要有磁盘IO消耗以及数据网络传输的消耗的，所以需要根据实际数据情况进行适当调整。另外，Shuffle可以分为两部分，分别是Map阶段的数据准备与Reduce阶段的数据拷贝处理，在Map端我们叫Shuffle
Write，在Reduce端我们叫Shuffle Read。</p>
<h2 id="什么是惰性执行">什么是惰性执行</h2>
<p>这是RDD的一个特性，在RDD中的算子可以分为Transform算子和Action算子，其中Transform算子的操作都不会真正执行，只会记录一下依赖关系，直到遇见了Action算子，在这之前的所有Transform操作才会被触发计算，这就是所谓的惰性执行。具体哪些是Transform和Action算子，可以看下一节。</p>
<h2 id="常用函数">常用函数</h2>
<p>常用的算子大概可以分为以下几种：</p>
<h3 id="transformations">Transformations</h3>
<ul>
<li>map</li>
<li>flatmap</li>
<li>filter</li>
<li>distinct</li>
<li>reduceByKey</li>
<li>mapPartitions</li>
<li>sortBy</li>
</ul>
<h3 id="actions">Actions</h3>
<ul>
<li>collect</li>
<li>collectAsMap</li>
<li>reduce</li>
<li>countByKey/countByValue</li>
<li>take</li>
<li>first</li>
</ul>
<h3 id="代码示例">代码示例</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pyspark</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">&quot;test_spark_app&quot;</span>).setMaster(<span class="string">&quot;local[4]&quot;</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 parallelize方法直接实例化一个RDD</span></span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">11</span>),<span class="number">4</span>) <span class="comment"># 这里的 4 指的是分区数量</span></span><br><span class="line">rdd.take(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">----------------------------------------------</span></span><br><span class="line"><span class="string">                Transform算子解析</span></span><br><span class="line"><span class="string">----------------------------------------------</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 以下的操作由于是Transform操作，因为我们需要在最后加上一个collect算子用来触发计算。</span></span><br><span class="line"><span class="comment"># 1. map: 和python差不多，map转换就是对每一个元素进行一个映射</span></span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>), <span class="number">4</span>)</span><br><span class="line">rdd_map = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x*<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始数据：&quot;</span>, rdd.collect())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;扩大2倍：&quot;</span>, rdd_map.collect())</span><br><span class="line"><span class="comment"># 原始数据： [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</span></span><br><span class="line"><span class="comment"># 扩大2倍： [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. flatMap: 这个相比于map多一个flat（压平）操作，顾名思义就是要把高维的数组变成一维</span></span><br><span class="line">rdd2 = sc.parallelize([<span class="string">&quot;hello SamShare&quot;</span>, <span class="string">&quot;hello PySpark&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始数据：&quot;</span>, rdd2.collect())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;直接split之后的map结果：&quot;</span>, rdd2.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&quot; &quot;</span>)).collect())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;直接split之后的flatMap结果：&quot;</span>, rdd2.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">&quot; &quot;</span>)).collect())</span><br><span class="line"><span class="comment"># 直接split之后的map结果： [[&#x27;hello&#x27;, &#x27;SamShare&#x27;], [&#x27;hello&#x27;, &#x27;PySpark&#x27;]]</span></span><br><span class="line"><span class="comment"># 直接split之后的flatMap结果： [&#x27;hello&#x27;, &#x27;SamShare&#x27;, &#x27;hello&#x27;, &#x27;PySpark&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. filter: 过滤数据</span></span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>), <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始数据：&quot;</span>, rdd.collect())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;过滤奇数：&quot;</span>, rdd.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x % <span class="number">2</span> == <span class="number">0</span>).collect())</span><br><span class="line"><span class="comment"># 原始数据： [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</span></span><br><span class="line"><span class="comment"># 过滤奇数： [2, 4, 6, 8, 10]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. distinct: 去重元素</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始数据：&quot;</span>, rdd.collect())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;去重数据：&quot;</span>, rdd.distinct().collect())</span><br><span class="line"><span class="comment"># 原始数据： [2, 2, 4, 8, 8, 8, 8, 16, 32, 32]</span></span><br><span class="line"><span class="comment"># 去重数据： [4, 8, 16, 32, 2]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. reduceByKey: 根据key来映射数据</span></span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</span><br><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始数据：&quot;</span>, rdd.collect())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始数据：&quot;</span>, rdd.reduceByKey(add).collect())</span><br><span class="line"><span class="comment"># 原始数据： [(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 1), (&#x27;a&#x27;, 1)]</span></span><br><span class="line"><span class="comment"># 原始数据： [(&#x27;b&#x27;, 1), (&#x27;a&#x27;, 2)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. mapPartitions: 根据分区内的数据进行映射操作</span></span><br><span class="line"><span class="comment"># 把数据集发成两个分区，在分区内进行求和操作</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], <span class="number">2</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">iterator</span>):</span><br><span class="line">    <span class="keyword">yield</span> <span class="built_in">sum</span>(iterator)</span><br><span class="line"><span class="built_in">print</span>(rdd.collect())</span><br><span class="line"><span class="built_in">print</span>(rdd.mapPartitions(f).collect())</span><br><span class="line"><span class="comment"># [1, 2, 3, 4]</span></span><br><span class="line"><span class="comment"># [3, 7]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. sortBy: 根据规则进行排序</span></span><br><span class="line">tmp = [(<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;1&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;d&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;2&#x27;</span>, <span class="number">5</span>)]</span><br><span class="line"><span class="built_in">print</span>(sc.parallelize(tmp).sortBy(<span class="keyword">lambda</span> x: x[<span class="number">0</span>]).collect())</span><br><span class="line"><span class="built_in">print</span>(sc.parallelize(tmp).sortBy(<span class="keyword">lambda</span> x: x[<span class="number">1</span>]).collect())</span><br><span class="line"><span class="comment"># [(&#x27;1&#x27;, 3), (&#x27;2&#x27;, 5), (&#x27;a&#x27;, 1), (&#x27;b&#x27;, 2), (&#x27;d&#x27;, 4)]</span></span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 2), (&#x27;1&#x27;, 3), (&#x27;d&#x27;, 4), (&#x27;2&#x27;, 5)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. subtract: 数据集相减, Return each value in self that is not contained in other.</span></span><br><span class="line">x = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>)])</span><br><span class="line">y = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;c&quot;</span>, <span class="literal">None</span>)])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(x.subtract(y).collect()))</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 4), (&#x27;b&#x27;, 5)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9. union: 合并两个RDD</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(rdd.union(rdd).collect())</span><br><span class="line"><span class="comment"># [1, 1, 2, 3, 1, 1, 2, 3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 10. intersection: 取两个RDD的交集，同时有去重的功效</span></span><br><span class="line">rdd1 = sc.parallelize([<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">rdd2 = sc.parallelize([<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line"><span class="built_in">print</span>(rdd1.intersection(rdd2).collect())</span><br><span class="line"><span class="comment"># [1, 2, 3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 11. cartesian: 生成笛卡尔积</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(rdd.cartesian(rdd).collect()))</span><br><span class="line"><span class="comment"># [(1, 1), (1, 2), (2, 1), (2, 2)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 12. zip: 拉链合并，需要两个RDD具有相同的长度以及分区数量</span></span><br><span class="line">x = sc.parallelize(<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">5</span>))</span><br><span class="line">y = sc.parallelize(<span class="built_in">range</span>(<span class="number">1000</span>, <span class="number">1005</span>))</span><br><span class="line"><span class="built_in">print</span>(x.collect())</span><br><span class="line"><span class="built_in">print</span>(y.collect())</span><br><span class="line"><span class="built_in">print</span>(x.<span class="built_in">zip</span>(y).collect())</span><br><span class="line"><span class="comment"># [0, 1, 2, 3, 4]</span></span><br><span class="line"><span class="comment"># [1000, 1001, 1002, 1003, 1004]</span></span><br><span class="line"><span class="comment"># [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 13. zipWithIndex: 将RDD和一个从0开始的递增序列按照拉链方式连接。</span></span><br><span class="line">rdd_name = sc.parallelize([<span class="string">&quot;LiLei&quot;</span>, <span class="string">&quot;Hanmeimei&quot;</span>, <span class="string">&quot;Lily&quot;</span>, <span class="string">&quot;Lucy&quot;</span>, <span class="string">&quot;Ann&quot;</span>, <span class="string">&quot;Dachui&quot;</span>, <span class="string">&quot;RuHua&quot;</span>])</span><br><span class="line">rdd_index = rdd_name.zipWithIndex()</span><br><span class="line"><span class="built_in">print</span>(rdd_index.collect())</span><br><span class="line"><span class="comment"># [(&#x27;LiLei&#x27;, 0), (&#x27;Hanmeimei&#x27;, 1), (&#x27;Lily&#x27;, 2), (&#x27;Lucy&#x27;, 3), (&#x27;Ann&#x27;, 4), (&#x27;Dachui&#x27;, 5), (&#x27;RuHua&#x27;, 6)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 14. groupByKey: 按照key来聚合数据</span></span><br><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="built_in">print</span>(rdd.collect())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(rdd.groupByKey().mapValues(<span class="built_in">len</span>).collect()))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(rdd.groupByKey().mapValues(<span class="built_in">list</span>).collect()))</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 1), (&#x27;a&#x27;, 1)]</span></span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 2), (&#x27;b&#x27;, 1)]</span></span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, [1, 1]), (&#x27;b&#x27;, [1])]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 15. sortByKey:</span></span><br><span class="line">tmp = [(<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;1&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;d&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;2&#x27;</span>, <span class="number">5</span>)]</span><br><span class="line"><span class="built_in">print</span>(sc.parallelize(tmp).sortByKey(<span class="literal">True</span>, <span class="number">1</span>).collect())</span><br><span class="line"><span class="comment"># [(&#x27;1&#x27;, 3), (&#x27;2&#x27;, 5), (&#x27;a&#x27;, 1), (&#x27;b&#x27;, 2), (&#x27;d&#x27;, 4)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 16. join:</span></span><br><span class="line">x = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)])</span><br><span class="line">y = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>)])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(x.join(y).collect()))</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, (1, 2)), (&#x27;a&#x27;, (1, 3))]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 17. leftOuterJoin/rightOuterJoin</span></span><br><span class="line">x = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)])</span><br><span class="line">y = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">2</span>)])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(x.leftOuterJoin(y).collect()))</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, (1, 2)), (&#x27;b&#x27;, (4, None))]</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">----------------------------------------------</span></span><br><span class="line"><span class="string">                Action算子解析</span></span><br><span class="line"><span class="string">----------------------------------------------</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 1. collect: 指的是把数据都汇集到driver端，便于后续的操作</span></span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">5</span>))</span><br><span class="line">rdd_collect = rdd.collect()</span><br><span class="line"><span class="built_in">print</span>(rdd_collect)</span><br><span class="line"><span class="comment"># [0, 1, 2, 3, 4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. first: 取第一个元素</span></span><br><span class="line">sc.parallelize([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]).first()</span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. collectAsMap: 转换为dict，使用这个要注意了，不要对大数据用，不然全部载入到driver端会爆内存</span></span><br><span class="line">m = sc.parallelize([(<span class="number">1</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">4</span>)]).collectAsMap()</span><br><span class="line">m</span><br><span class="line"><span class="comment"># &#123;1: 2, 3: 4&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. reduce: 逐步对两个元素进行操作</span></span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">10</span>),<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(rdd.reduce(<span class="keyword">lambda</span> x,y:x+y))</span><br><span class="line"><span class="comment"># 45</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. countByKey/countByValue:</span></span><br><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(rdd.countByKey().items()))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(rdd.countByValue().items()))</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 2), (&#x27;b&#x27;, 1)]</span></span><br><span class="line"><span class="comment"># [((&#x27;a&#x27;, 1), 2), ((&#x27;b&#x27;, 1), 1)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. take: 相当于取几个数据到driver端</span></span><br><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="built_in">print</span>(rdd.take(<span class="number">5</span>))</span><br><span class="line"><span class="comment"># [(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 1), (&#x27;a&#x27;, 1)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. saveAsTextFile: 保存rdd成text文件到本地</span></span><br><span class="line">text_file_path = <span class="string">&quot;./data/rdd&quot;</span></span><br><span class="line">shutil.rmtree(text_file_path, <span class="literal">True</span>)</span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">5</span>))</span><br><span class="line">rdd.saveAsTextFile(text_file_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. takeSample: 随机取数</span></span><br><span class="line">rdd = sc.textFile(<span class="string">&quot;./samples/zipcodes.csv&quot;</span>, <span class="number">4</span>)  <span class="comment"># 这里的 4 指的是分区数量</span></span><br><span class="line">rdd_sample = rdd.takeSample(<span class="literal">True</span>, <span class="number">2</span>, <span class="number">0</span>)  <span class="comment"># withReplacement 参数1：代表是否是有放回抽样</span></span><br><span class="line"><span class="built_in">print</span>(rdd_sample)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9. foreach: 对每一个元素执行某种操作，不生成新的RDD</span></span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">10</span>), <span class="number">5</span>)</span><br><span class="line">accum = sc.accumulator(<span class="number">0</span>)</span><br><span class="line">rdd.foreach(<span class="keyword">lambda</span> x: accum.add(x))</span><br><span class="line"><span class="built_in">print</span>(accum.value)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="spark-sql使用">Spark SQL使用</h2>
<p>在讲Spark
SQL前，先解释下这个模块。这个模块是Spark中用来处理结构化数据的，提供一个叫SparkDataFrame的东西并且自动解析为分布式SQL查询数据。用过Python的Pandas库，也大致了解了DataFrame，这个其实和它没有太大的区别，只是调用的API可能有些不同罢了。</p>
<p>我们通过使用Spark
SQL来处理数据，会让我们更加熟悉，比如可以用SQL语句、用SparkDataFrame的API或者Datasets
API，我们可以按照需求随心转换，通过SparkDataFrame API 和 SQL
写的逻辑，会被Spark优化器Catalyst自动优化成RDD。</p>
<h2 id="创建sparkdataframe">创建SparkDataFrame</h2>
<p>开始讲SparkDataFrame，我们先学习下几种创建的方法，分别是使用RDD来创建、使用pandas的DataFrame来创建、使用List来创建、读取数据文件来创建、通过读取数据库来创建。</p>
<h3 id="使用rdd来创建">使用RDD来创建</h3>
<p>主要使用RDD的toDF方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;Sam&quot;</span>, <span class="number">28</span>, <span class="number">88</span>), (<span class="string">&quot;Flora&quot;</span>, <span class="number">28</span>, <span class="number">90</span>), (<span class="string">&quot;Run&quot;</span>, <span class="number">1</span>, <span class="number">60</span>)])</span><br><span class="line">df = rdd.toDF([<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>, <span class="string">&quot;score&quot;</span>])</span><br><span class="line">df.show()</span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br><span class="line"><span class="comment"># | name|age|score|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment">#  |-- name: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- score: long (nullable = true)</span></span><br></pre></td></tr></table></figure>
<h3 id="使用pandas的dataframe来创建">使用pandas的DataFrame来创建</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame([[<span class="string">&#x27;Sam&#x27;</span>, <span class="number">28</span>, <span class="number">88</span>], [<span class="string">&#x27;Flora&#x27;</span>, <span class="number">28</span>, <span class="number">90</span>], [<span class="string">&#x27;Run&#x27;</span>, <span class="number">1</span>, <span class="number">60</span>]],</span><br><span class="line">                  columns=[<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;score&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&gt;&gt; 打印DataFrame:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">Spark_df = spark.createDataFrame(df)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&gt;&gt; 打印SparkDataFrame:&quot;</span>)</span><br><span class="line">Spark_df.show()</span><br><span class="line"><span class="comment"># &gt;&gt; 打印DataFrame:</span></span><br><span class="line"><span class="comment">#     name  age  score</span></span><br><span class="line"><span class="comment"># 0    Sam   28     88</span></span><br><span class="line"><span class="comment"># 1  Flora   28     90</span></span><br><span class="line"><span class="comment"># 2    Run    1     60</span></span><br><span class="line"><span class="comment"># &gt;&gt; 打印SparkDataFrame:</span></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br><span class="line"><span class="comment"># | name|age|score|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br></pre></td></tr></table></figure>
<h3 id="使用list来创建">使用List来创建</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">list_values = [[<span class="string">&#x27;Sam&#x27;</span>, <span class="number">28</span>, <span class="number">88</span>], [<span class="string">&#x27;Flora&#x27;</span>, <span class="number">28</span>, <span class="number">90</span>], [<span class="string">&#x27;Run&#x27;</span>, <span class="number">1</span>, <span class="number">60</span>]]</span><br><span class="line">Spark_df = spark.createDataFrame(list_values, [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;score&#x27;</span>])</span><br><span class="line">Spark_df.show()</span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br><span class="line"><span class="comment"># | name|age|score|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+</span></span><br></pre></td></tr></table></figure>
<h3 id="读取数据文件来创建">读取数据文件来创建</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CSV文件</span></span><br><span class="line">df = spark.read.option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;delimiter&quot;</span>, <span class="string">&quot;,&quot;</span>) \</span><br><span class="line">    .csv(<span class="string">&quot;./samples/breast_homo_test.csv&quot;</span>)</span><br><span class="line">df.show(<span class="number">5</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># json文件</span></span><br><span class="line">df = spark.read.json(<span class="string">&quot;./samples/zipcodes.json&quot;</span>)</span><br><span class="line">df.show(<span class="number">5</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line">df = spark.read.option(<span class="string">&quot;multiline&quot;</span>, <span class="string">&quot;true&quot;</span>).json(<span class="string">&quot;./samples/multiline-zipcode.json&quot;</span>)</span><br><span class="line">df.show(<span class="number">5</span>)</span><br><span class="line">df.printSchema()</span><br></pre></td></tr></table></figure>
<h3 id="通过读取数据库来创建">通过读取数据库来创建</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取hive数据</span></span><br><span class="line">spark.sql(<span class="string">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;LOAD DATA LOCAL INPATH &#x27;data/kv1.txt&#x27; INTO TABLE src&quot;</span>)</span><br><span class="line">df = spark.sql(<span class="string">&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;</span>)</span><br><span class="line">df.show(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取mysql数据</span></span><br><span class="line">url = <span class="string">&quot;jdbc:mysql://localhost:3306/test&quot;</span></span><br><span class="line">df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;jdbc&quot;</span>) \</span><br><span class="line"> .option(<span class="string">&quot;url&quot;</span>, url) \</span><br><span class="line"> .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;runoob_tbl&quot;</span>) \</span><br><span class="line"> .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>) \</span><br><span class="line"> .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;8888&quot;</span>) \</span><br><span class="line"> .load()\</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>
<h2 id="常用的sparkdataframe-api">常用的SparkDataFrame API</h2>
<p>这里我大概是分成了几部分来看这些APIs，分别是查看DataFrame的APIs、简单处理DataFrame的APIs、DataFrame的列操作APIs、DataFrame的一些思路变换操作APIs、DataFrame的一些统计操作APIs，这样子也有助于我们了解这些API的功能，以后遇见实际问题的时候可以解决。</p>
<p>首先我们这小节全局用到的数据集如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="comment"># SparkSQL的许多功能封装在SparkSession的方法接口中, SparkContext则不行的。</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">&quot;test_spark_app&quot;</span>) \</span><br><span class="line">    .config(<span class="string">&quot;master&quot;</span>, <span class="string">&quot;local[4]&quot;</span>) \</span><br><span class="line">    .enableHiveSupport() \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个SparkDataFrame</span></span><br><span class="line">rdd = sc.parallelize([(<span class="string">&quot;Sam&quot;</span>, <span class="number">28</span>, <span class="number">88</span>, <span class="string">&quot;M&quot;</span>),</span><br><span class="line">                      (<span class="string">&quot;Flora&quot;</span>, <span class="number">28</span>, <span class="number">90</span>, <span class="string">&quot;F&quot;</span>),</span><br><span class="line">                      (<span class="string">&quot;Run&quot;</span>, <span class="number">1</span>, <span class="number">60</span>, <span class="literal">None</span>),</span><br><span class="line">                      (<span class="string">&quot;Peter&quot;</span>, <span class="number">55</span>, <span class="number">100</span>, <span class="string">&quot;M&quot;</span>),</span><br><span class="line">                      (<span class="string">&quot;Mei&quot;</span>, <span class="number">54</span>, <span class="number">95</span>, <span class="string">&quot;F&quot;</span>)])</span><br><span class="line">df = rdd.toDF([<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>, <span class="string">&quot;score&quot;</span>, <span class="string">&quot;sex&quot;</span>])</span><br><span class="line">df.show()</span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"><span class="comment"># | name|age|score| sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|   M|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|   F|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|null|</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|   M|</span></span><br><span class="line"><span class="comment"># |  Mei| 54|   95|   F|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment">#  |-- name: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- score: long (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- sex: string (nullable = true)</span></span><br></pre></td></tr></table></figure>
<h3 id="查看dataframe的apis">查看DataFrame的APIs</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame.collect</span></span><br><span class="line"><span class="comment"># 以列表形式返回行</span></span><br><span class="line">df.collect()</span><br><span class="line"><span class="comment"># [Row(name=&#x27;Sam&#x27;, age=28, score=88, sex=&#x27;M&#x27;),</span></span><br><span class="line"><span class="comment"># Row(name=&#x27;Flora&#x27;, age=28, score=90, sex=&#x27;F&#x27;),</span></span><br><span class="line"><span class="comment"># Row(name=&#x27;Run&#x27;, age=1, score=60, sex=None),</span></span><br><span class="line"><span class="comment"># Row(name=&#x27;Peter&#x27;, age=55, score=100, sex=&#x27;M&#x27;),</span></span><br><span class="line"><span class="comment"># Row(name=&#x27;Mei&#x27;, age=54, score=95, sex=&#x27;F&#x27;)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.count</span></span><br><span class="line">df.count()</span><br><span class="line"><span class="comment"># 5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.columns</span></span><br><span class="line">df.columns</span><br><span class="line"><span class="comment"># [&#x27;name&#x27;, &#x27;age&#x27;, &#x27;score&#x27;, &#x27;sex&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.dtypes</span></span><br><span class="line">df.dtypes</span><br><span class="line"><span class="comment"># [(&#x27;name&#x27;, &#x27;string&#x27;), (&#x27;age&#x27;, &#x27;bigint&#x27;), (&#x27;score&#x27;, &#x27;bigint&#x27;), (&#x27;sex&#x27;, &#x27;string&#x27;)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.describe</span></span><br><span class="line"><span class="comment"># 返回列的基础统计信息</span></span><br><span class="line">df.describe([<span class="string">&#x27;age&#x27;</span>]).show()</span><br><span class="line"><span class="comment"># +-------+------------------+</span></span><br><span class="line"><span class="comment"># |summary|               age|</span></span><br><span class="line"><span class="comment"># +-------+------------------+</span></span><br><span class="line"><span class="comment"># |  count|                 5|</span></span><br><span class="line"><span class="comment"># |   mean|              33.2|</span></span><br><span class="line"><span class="comment"># | stddev|22.353970564532826|</span></span><br><span class="line"><span class="comment"># |    min|                 1|</span></span><br><span class="line"><span class="comment"># |    max|                55|</span></span><br><span class="line"><span class="comment"># +-------+------------------+</span></span><br><span class="line">df.describe().show()</span><br><span class="line"><span class="comment"># +-------+-----+------------------+------------------+----+</span></span><br><span class="line"><span class="comment"># |summary| name|               age|             score| sex|</span></span><br><span class="line"><span class="comment"># +-------+-----+------------------+------------------+----+</span></span><br><span class="line"><span class="comment"># |  count|    5|                 5|                 5|   4|</span></span><br><span class="line"><span class="comment"># |   mean| null|              33.2|              86.6|null|</span></span><br><span class="line"><span class="comment"># | stddev| null|22.353970564532826|15.582040944625966|null|</span></span><br><span class="line"><span class="comment"># |    min|Flora|                 1|                60|   F|</span></span><br><span class="line"><span class="comment"># |    max|  Sam|                55|               100|   M|</span></span><br><span class="line"><span class="comment"># +-------+-----+------------------+------------------+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.select</span></span><br><span class="line"><span class="comment"># 选定指定列并按照一定顺序呈现</span></span><br><span class="line">df.select(<span class="string">&quot;sex&quot;</span>, <span class="string">&quot;score&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.first</span></span><br><span class="line"><span class="comment"># DataFrame.head</span></span><br><span class="line"><span class="comment"># 查看第1条数据</span></span><br><span class="line">df.first()</span><br><span class="line"><span class="comment"># Row(name=&#x27;Sam&#x27;, age=28, score=88, sex=&#x27;M&#x27;)</span></span><br><span class="line">df.head(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># [Row(name=&#x27;Sam&#x27;, age=28, score=88, sex=&#x27;M&#x27;)]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.freqItems</span></span><br><span class="line"><span class="comment"># 查看指定列的枚举值</span></span><br><span class="line">df.freqItems([<span class="string">&quot;age&quot;</span>,<span class="string">&quot;sex&quot;</span>]).show()</span><br><span class="line"><span class="comment"># +---------------+-------------+</span></span><br><span class="line"><span class="comment"># |  age_freqItems|sex_freqItems|</span></span><br><span class="line"><span class="comment"># +---------------+-------------+</span></span><br><span class="line"><span class="comment"># |[55, 1, 28, 54]|      [M, F,]|</span></span><br><span class="line"><span class="comment"># +---------------+-------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.summary</span></span><br><span class="line">df.summary().show()</span><br><span class="line"><span class="comment"># +-------+-----+------------------+------------------+----+</span></span><br><span class="line"><span class="comment"># |summary| name|               age|             score| sex|</span></span><br><span class="line"><span class="comment"># +-------+-----+------------------+------------------+----+</span></span><br><span class="line"><span class="comment"># |  count|    5|                 5|                 5|   4|</span></span><br><span class="line"><span class="comment"># |   mean| null|              33.2|              86.6|null|</span></span><br><span class="line"><span class="comment"># | stddev| null|22.353970564532826|15.582040944625966|null|</span></span><br><span class="line"><span class="comment"># |    min|Flora|                 1|                60|   F|</span></span><br><span class="line"><span class="comment"># |    25%| null|                28|                88|null|</span></span><br><span class="line"><span class="comment"># |    50%| null|                28|                90|null|</span></span><br><span class="line"><span class="comment"># |    75%| null|                54|                95|null|</span></span><br><span class="line"><span class="comment"># |    max|  Sam|                55|               100|   M|</span></span><br><span class="line"><span class="comment"># +-------+-----+------------------+------------------+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.sample</span></span><br><span class="line"><span class="comment"># 按照一定规则从df随机抽样数据</span></span><br><span class="line">df.sample(<span class="number">0.5</span>).show()</span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"><span class="comment"># | name|age|score| sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|   M|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|null|</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|   M|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br></pre></td></tr></table></figure>
<h3 id="简单处理dataframe的apis">简单处理DataFrame的APIs</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame.distinct</span></span><br><span class="line"><span class="comment"># 对数据集进行去重</span></span><br><span class="line">df.distinct().show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.dropDuplicates</span></span><br><span class="line"><span class="comment"># 对指定列去重</span></span><br><span class="line">df.dropDuplicates([<span class="string">&quot;sex&quot;</span>]).show()</span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"><span class="comment"># | name|age|score| sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|   F|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|null|</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|   M|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.exceptAll</span></span><br><span class="line"><span class="comment"># DataFrame.subtract</span></span><br><span class="line"><span class="comment"># 根据指定的df对df进行去重</span></span><br><span class="line">df1 = spark.createDataFrame(</span><br><span class="line">        [(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>,  <span class="number">3</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>)], [<span class="string">&quot;C1&quot;</span>, <span class="string">&quot;C2&quot;</span>])</span><br><span class="line">df2 = spark.createDataFrame([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>)], [<span class="string">&quot;C1&quot;</span>, <span class="string">&quot;C2&quot;</span>])</span><br><span class="line">df3 = df1.exceptAll(df2)  <span class="comment"># 没有去重的功效</span></span><br><span class="line">df4 = df1.subtract(df2)  <span class="comment"># 有去重的奇效</span></span><br><span class="line">df1.show()</span><br><span class="line">df2.show()</span><br><span class="line">df3.show()</span><br><span class="line">df4.show()</span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># | C1| C2|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># |  a|  1|</span></span><br><span class="line"><span class="comment"># |  a|  1|</span></span><br><span class="line"><span class="comment"># |  b|  3|</span></span><br><span class="line"><span class="comment"># |  c|  4|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># | C1| C2|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># |  a|  1|</span></span><br><span class="line"><span class="comment"># |  b|  3|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># | C1| C2|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># |  a|  1|</span></span><br><span class="line"><span class="comment"># |  c|  4|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># | C1| C2|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># |  c|  4|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.intersectAll</span></span><br><span class="line"><span class="comment"># 返回两个DataFrame的交集</span></span><br><span class="line">df1 = spark.createDataFrame(</span><br><span class="line">        [(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>,  <span class="number">3</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>)], [<span class="string">&quot;C1&quot;</span>, <span class="string">&quot;C2&quot;</span>])</span><br><span class="line">df2 = spark.createDataFrame([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)], [<span class="string">&quot;C1&quot;</span>, <span class="string">&quot;C2&quot;</span>])</span><br><span class="line">df1.intersectAll(df2).show()</span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># | C1| C2|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># |  a|  1|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.drop</span></span><br><span class="line"><span class="comment"># 丢弃指定列</span></span><br><span class="line">df.drop(<span class="string">&#x27;age&#x27;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.withColumn</span></span><br><span class="line"><span class="comment"># 新增列</span></span><br><span class="line">df1 = df.withColumn(<span class="string">&quot;birth_year&quot;</span>, <span class="number">2021</span> - df.age)</span><br><span class="line">df1.show()</span><br><span class="line"><span class="comment"># +-----+---+-----+----+----------+</span></span><br><span class="line"><span class="comment"># | name|age|score| sex|birth_year|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+----------+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|   M|      1993|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|   F|      1993|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|null|      2020|</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|   M|      1966|</span></span><br><span class="line"><span class="comment"># |  Mei| 54|   95|   F|      1967|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+----+----------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.withColumnRenamed</span></span><br><span class="line"><span class="comment"># 重命名列名</span></span><br><span class="line">df1 = df.withColumnRenamed(<span class="string">&quot;sex&quot;</span>, <span class="string">&quot;gender&quot;</span>)</span><br><span class="line">df1.show()</span><br><span class="line"><span class="comment"># +-----+---+-----+------+</span></span><br><span class="line"><span class="comment"># | name|age|score|gender|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+------+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|     M|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|     F|</span></span><br><span class="line"><span class="comment"># |  Run|  1|   60|  null|</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|     M|</span></span><br><span class="line"><span class="comment"># |  Mei| 54|   95|     F|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.dropna</span></span><br><span class="line"><span class="comment"># 丢弃空值，DataFrame.dropna(how=&#x27;any&#x27;, thresh=None, subset=None)</span></span><br><span class="line">df.dropna(how=<span class="string">&#x27;all&#x27;</span>, subset=[<span class="string">&#x27;sex&#x27;</span>]).show()</span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># | name|age|score|sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|  M|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|  F|</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|  M|</span></span><br><span class="line"><span class="comment"># |  Mei| 54|   95|  F|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.fillna</span></span><br><span class="line"><span class="comment"># 空值填充操作</span></span><br><span class="line">df1 = spark.createDataFrame(</span><br><span class="line">        [(<span class="string">&quot;a&quot;</span>, <span class="literal">None</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="literal">None</span>,  <span class="number">3</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>)], [<span class="string">&quot;C1&quot;</span>, <span class="string">&quot;C2&quot;</span>])</span><br><span class="line"><span class="comment"># df2 = df1.na.fill(&#123;&quot;C1&quot;: &quot;d&quot;, &quot;C2&quot;: 99&#125;)</span></span><br><span class="line">df2 = df1.fillna(&#123;<span class="string">&quot;C1&quot;</span>: <span class="string">&quot;d&quot;</span>, <span class="string">&quot;C2&quot;</span>: <span class="number">99</span>&#125;)</span><br><span class="line">df1.show()</span><br><span class="line">df2.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.filter</span></span><br><span class="line"><span class="comment"># 根据条件过滤</span></span><br><span class="line">df.<span class="built_in">filter</span>(df.age&gt;<span class="number">50</span>).show()</span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># | name|age|score|sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|  M|</span></span><br><span class="line"><span class="comment"># |  Mei| 54|   95|  F|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line">df.where(df.age==<span class="number">28</span>).show()</span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># | name|age|score|sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|  M|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|  F|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line">df.<span class="built_in">filter</span>(<span class="string">&quot;age&lt;18&quot;</span>).show()</span><br><span class="line"><span class="comment"># +----+---+-----+----+</span></span><br><span class="line"><span class="comment"># |name|age|score| sex|</span></span><br><span class="line"><span class="comment"># +----+---+-----+----+</span></span><br><span class="line"><span class="comment"># | Run|  1|   60|null|</span></span><br><span class="line"><span class="comment"># +----+---+-----+----+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.join</span></span><br><span class="line"><span class="comment"># 这个不用多解释了，直接上案例来看看具体的语法即可，DataFrame.join(other, on=None, how=None)</span></span><br><span class="line">df1 = spark.createDataFrame(</span><br><span class="line">        [(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>,  <span class="number">3</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>)], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;num1&quot;</span>])</span><br><span class="line">df2 = spark.createDataFrame([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>)], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;num2&quot;</span>])</span><br><span class="line">df1.join(df2, df1.<span class="built_in">id</span> == df2.<span class="built_in">id</span>, <span class="string">&#x27;left&#x27;</span>).select(df1.<span class="built_in">id</span>.alias(<span class="string">&quot;df1_id&quot;</span>),</span><br><span class="line">                                               df1.num1.alias(<span class="string">&quot;df1_num&quot;</span>),</span><br><span class="line">                                               df2.num2.alias(<span class="string">&quot;df2_num&quot;</span>)</span><br><span class="line">                                               ).sort([<span class="string">&quot;df1_id&quot;</span>], ascending=<span class="literal">False</span>)\</span><br><span class="line">    .show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.agg(*exprs)</span></span><br><span class="line"><span class="comment"># 聚合数据，可以写多个聚合方法，如果不写groupBy的话就是对整个DF进行聚合</span></span><br><span class="line"><span class="comment"># DataFrame.alias</span></span><br><span class="line"><span class="comment"># 设置列或者DataFrame别名</span></span><br><span class="line"><span class="comment"># DataFrame.groupBy</span></span><br><span class="line"><span class="comment"># 根据某几列进行聚合，如有多列用列表写在一起，如 df.groupBy([&quot;sex&quot;, &quot;age&quot;])</span></span><br><span class="line">df.groupBy(<span class="string">&quot;sex&quot;</span>).agg(F.<span class="built_in">min</span>(df.age).alias(<span class="string">&quot;最小年龄&quot;</span>),</span><br><span class="line">                      F.expr(<span class="string">&quot;avg(age)&quot;</span>).alias(<span class="string">&quot;平均年龄&quot;</span>),</span><br><span class="line">                      F.expr(<span class="string">&quot;collect_list(name)&quot;</span>).alias(<span class="string">&quot;姓名集合&quot;</span>)</span><br><span class="line">                      ).show()</span><br><span class="line"><span class="comment"># +----+--------+--------+------------+</span></span><br><span class="line"><span class="comment"># | sex|最小年龄|平均年龄|    姓名集合|</span></span><br><span class="line"><span class="comment"># +----+--------+--------+------------+</span></span><br><span class="line"><span class="comment"># |   F|      28|    41.0|[Flora, Mei]|</span></span><br><span class="line"><span class="comment"># |null|       1|     1.0|       [Run]|</span></span><br><span class="line"><span class="comment"># |   M|      28|    41.5|[Sam, Peter]|</span></span><br><span class="line"><span class="comment"># +----+--------+--------+------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.foreach</span></span><br><span class="line"><span class="comment"># 对每一行进行函数方法的应用</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">person</span>):</span><br><span class="line">    <span class="built_in">print</span>(person.name)</span><br><span class="line">df.foreach(f)</span><br><span class="line"><span class="comment"># Peter</span></span><br><span class="line"><span class="comment"># Run</span></span><br><span class="line"><span class="comment"># Sam</span></span><br><span class="line"><span class="comment"># Flora</span></span><br><span class="line"><span class="comment"># Mei</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.replace</span></span><br><span class="line"><span class="comment"># 修改df里的某些值</span></span><br><span class="line">df1 = df.na.replace(&#123;<span class="string">&quot;M&quot;</span>: <span class="string">&quot;Male&quot;</span>, <span class="string">&quot;F&quot;</span>: <span class="string">&quot;Female&quot;</span>&#125;)</span><br><span class="line">df1.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.union</span></span><br><span class="line"><span class="comment"># 相当于SQL里的union all操作</span></span><br><span class="line">df1 = spark.createDataFrame(</span><br><span class="line">        [(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>,  <span class="number">3</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>)], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;num&quot;</span>])</span><br><span class="line">df2 = spark.createDataFrame([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>)], [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;num&quot;</span>])</span><br><span class="line">df1.union(df2).show()</span><br><span class="line">df1.unionAll(df2).show()</span><br><span class="line"><span class="comment"># 这里union没有去重，不知道为啥，有知道的朋友麻烦解释下，谢谢了。</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># | id|num|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># |  a|  1|</span></span><br><span class="line"><span class="comment"># |  d|  1|</span></span><br><span class="line"><span class="comment"># |  b|  3|</span></span><br><span class="line"><span class="comment"># |  c|  4|</span></span><br><span class="line"><span class="comment"># |  a|  1|</span></span><br><span class="line"><span class="comment"># |  b|  3|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.unionByName</span></span><br><span class="line"><span class="comment"># 根据列名来进行合并数据集</span></span><br><span class="line">df1 = spark.createDataFrame([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]], [<span class="string">&quot;col0&quot;</span>, <span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>])</span><br><span class="line">df2 = spark.createDataFrame([[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], [<span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>, <span class="string">&quot;col0&quot;</span>])</span><br><span class="line">df1.unionByName(df2).show()</span><br><span class="line"><span class="comment"># +----+----+----+</span></span><br><span class="line"><span class="comment"># |col0|col1|col2|</span></span><br><span class="line"><span class="comment"># +----+----+----+</span></span><br><span class="line"><span class="comment"># |   1|   2|   3|</span></span><br><span class="line"><span class="comment"># |   6|   4|   5|</span></span><br><span class="line"><span class="comment"># +----+----+----+</span></span><br></pre></td></tr></table></figure>
<h3 id="dataframe的列操作apis">DataFrame的列操作APIs</h3>
<p>这里主要针对的是列进行操作，比如说重命名、排序、空值判断、类型判断等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Column.alias(*alias, **kwargs)  <span class="comment"># 重命名列名</span></span><br><span class="line">Column.asc()  <span class="comment"># 按照列进行升序排序</span></span><br><span class="line">Column.desc()  <span class="comment"># 按照列进行降序排序</span></span><br><span class="line">Column.astype(dataType)  <span class="comment"># 类型转换</span></span><br><span class="line">Column.cast(dataType)  <span class="comment"># 强制转换类型</span></span><br><span class="line">Column.between(lowerBound, upperBound)  <span class="comment"># 返回布尔值，是否在指定区间范围内</span></span><br><span class="line">Column.contains(other)  <span class="comment"># 是否包含某个关键词</span></span><br><span class="line">Column.endswith(other)  <span class="comment"># 以什么结束的值，如 df.filter(df.name.endswith(&#x27;ice&#x27;)).collect()</span></span><br><span class="line">Column.isNotNull()  <span class="comment"># 筛选非空的行</span></span><br><span class="line">Column.isNull()</span><br><span class="line">Column.isin(*cols)  <span class="comment"># 返回包含某些值的行 df[df.name.isin(&quot;Bob&quot;, &quot;Mike&quot;)].collect()</span></span><br><span class="line">Column.like(other)  <span class="comment"># 返回含有关键词的行</span></span><br><span class="line">Column.when(condition, value)  <span class="comment"># 给True的赋值</span></span><br><span class="line">Column.otherwise(value)  <span class="comment"># 与when搭配使用，df.select(df.name, F.when(df.age &gt; 3, 1).otherwise(0)).show()</span></span><br><span class="line">Column.rlike(other)  <span class="comment"># 可以使用正则的匹配 df.filter(df.name.rlike(&#x27;ice$&#x27;)).collect()</span></span><br><span class="line">Column.startswith(other)  <span class="comment"># df.filter(df.name.startswith(&#x27;Al&#x27;)).collect()</span></span><br><span class="line">Column.substr(startPos, length)  <span class="comment"># df.select(df.name.substr(1, 3).alias(&quot;col&quot;)).collect()</span></span><br></pre></td></tr></table></figure>
<h3 id="dataframe的一些思路变换操作apis">DataFrame的一些思路变换操作APIs</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame.createOrReplaceGlobalTempView</span></span><br><span class="line"><span class="comment"># DataFrame.dropGlobalTempView</span></span><br><span class="line"><span class="comment"># 创建全局的试图，注册后可以使用sql语句来进行操作，生命周期取决于Spark application本身</span></span><br><span class="line">df.createOrReplaceGlobalTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;select * from global_temp.people where sex = &#x27;M&#x27; &quot;</span>).show()</span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># | name|age|score|sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|  M|</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|  M|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.createOrReplaceTempView</span></span><br><span class="line"><span class="comment"># DataFrame.dropTempView</span></span><br><span class="line"><span class="comment"># 创建本地临时试图，生命周期取决于用来创建此数据集的SparkSession</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;tmp_people&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;select * from tmp_people where sex = &#x27;F&#x27; &quot;</span>).show()</span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># | name|age|score|sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|  F|</span></span><br><span class="line"><span class="comment"># |  Mei| 54|   95|  F|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.cache\DataFrame.persist</span></span><br><span class="line"><span class="comment"># 可以把一些数据放入缓存中，default storage level (MEMORY_AND_DISK).</span></span><br><span class="line">df.cache()</span><br><span class="line">df.persist()</span><br><span class="line">df.unpersist()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.crossJoin</span></span><br><span class="line"><span class="comment"># 返回两个DataFrame的笛卡尔积关联的DataFrame</span></span><br><span class="line">df1 = df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;sex&quot;</span>)</span><br><span class="line">df2 = df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;sex&quot;</span>)</span><br><span class="line">df3 = df1.crossJoin(df2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;表1的记录数&quot;</span>, df1.count())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;表2的记录数&quot;</span>, df2.count())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;笛卡尔积后的记录数&quot;</span>, df3.count())</span><br><span class="line"><span class="comment"># 表1的记录数 5</span></span><br><span class="line"><span class="comment"># 表2的记录数 5</span></span><br><span class="line"><span class="comment"># 笛卡尔积后的记录数 25</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.toPandas</span></span><br><span class="line"><span class="comment"># 把SparkDataFrame转为 Pandas的DataFrame</span></span><br><span class="line">df.toPandas()</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.rdd</span></span><br><span class="line"><span class="comment"># 把SparkDataFrame转为rdd，这样子可以用rdd的语法来操作数据</span></span><br><span class="line">df.rdd</span><br></pre></td></tr></table></figure>
<h3 id="dataframe的一些统计操作apis">DataFrame的一些统计操作APIs</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame.cov</span></span><br><span class="line"><span class="comment"># 计算指定两列的样本协方差</span></span><br><span class="line">df.cov(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;score&quot;</span>)</span><br><span class="line"><span class="comment"># 324.59999999999997</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.corr</span></span><br><span class="line"><span class="comment"># 计算指定两列的相关系数，DataFrame.corr(col1, col2, method=None)，目前method只支持Pearson相关系数</span></span><br><span class="line">df.corr(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;score&quot;</span>, method=<span class="string">&quot;pearson&quot;</span>)</span><br><span class="line"><span class="comment"># 0.9319004030498815</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame.cube</span></span><br><span class="line"><span class="comment"># 创建多维度聚合的结果，通常用于分析数据，比如我们指定两个列进行聚合，比如name和age，那么这个函数返回的聚合结果会</span></span><br><span class="line"><span class="comment"># groupby(&quot;name&quot;, &quot;age&quot;)</span></span><br><span class="line"><span class="comment"># groupby(&quot;name&quot;)</span></span><br><span class="line"><span class="comment"># groupby(&quot;age&quot;)</span></span><br><span class="line"><span class="comment"># groupby(all)</span></span><br><span class="line"><span class="comment"># 四个聚合结果的union all 的结果</span></span><br><span class="line"></span><br><span class="line">df1 = df.<span class="built_in">filter</span>(df.name != <span class="string">&quot;Run&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(df1.show())</span><br><span class="line">df1.cube(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;sex&quot;</span>).count().show()</span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># | name|age|score|sex|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># |  Sam| 28|   88|  M|</span></span><br><span class="line"><span class="comment"># |Flora| 28|   90|  F|</span></span><br><span class="line"><span class="comment"># |Peter| 55|  100|  M|</span></span><br><span class="line"><span class="comment"># |  Mei| 54|   95|  F|</span></span><br><span class="line"><span class="comment"># +-----+---+-----+---+</span></span><br><span class="line"><span class="comment"># cube 聚合之后的结果</span></span><br><span class="line"><span class="comment"># +-----+----+-----+</span></span><br><span class="line"><span class="comment"># | name| sex|count|</span></span><br><span class="line"><span class="comment"># +-----+----+-----+</span></span><br><span class="line"><span class="comment"># | null|   F|    2|</span></span><br><span class="line"><span class="comment"># | null|null|    4|</span></span><br><span class="line"><span class="comment"># |Flora|null|    1|</span></span><br><span class="line"><span class="comment"># |Peter|null|    1|</span></span><br><span class="line"><span class="comment"># | null|   M|    2|</span></span><br><span class="line"><span class="comment"># |Peter|   M|    1|</span></span><br><span class="line"><span class="comment"># |  Sam|   M|    1|</span></span><br><span class="line"><span class="comment"># |  Sam|null|    1|</span></span><br><span class="line"><span class="comment"># |  Mei|   F|    1|</span></span><br><span class="line"><span class="comment"># |  Mei|null|    1|</span></span><br><span class="line"><span class="comment"># |Flora|   F|    1|</span></span><br><span class="line"><span class="comment"># +-----+----+-----+</span></span><br></pre></td></tr></table></figure>
<h2 id="保存数据写入数据库">保存数据/写入数据库</h2>
<p>这里的保存数据主要是保存到Hive中的栗子，主要包括了overwrite、append等方式。</p>
<h3 id="当结果集为sparkdataframe的时候">当结果集为SparkDataFrame的时候</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> HiveContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf()\</span><br><span class="line">      .setAppName(<span class="string">&quot;test&quot;</span>)\</span><br><span class="line">      .<span class="built_in">set</span>(<span class="string">&quot;hive.exec.dynamic.partition.mode&quot;</span>, <span class="string">&quot;nonstrict&quot;</span>) <span class="comment"># 动态写入hive分区表</span></span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line">hc = HiveContext(sc)</span><br><span class="line">sc.setLogLevel(<span class="string">&quot;ERROR&quot;</span>)</span><br><span class="line">    </span><br><span class="line">list_values = [[<span class="string">&#x27;Sam&#x27;</span>, <span class="number">28</span>, <span class="number">88</span>], [<span class="string">&#x27;Flora&#x27;</span>, <span class="number">28</span>, <span class="number">90</span>], [<span class="string">&#x27;Run&#x27;</span>, <span class="number">1</span>, <span class="number">60</span>]]</span><br><span class="line">Spark_df = spark.createDataFrame(list_values, [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;score&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(Spark_df.show())</span><br><span class="line">save_table = <span class="string">&quot;tmp.samshare_pyspark_savedata&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式1:直接写入到Hive</span></span><br><span class="line">Spark_df.write.<span class="built_in">format</span>(<span class="string">&quot;hive&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).saveAsTable(save_table) <span class="comment"># 或者改成append模式</span></span><br><span class="line"><span class="built_in">print</span>(datetime.now().strftime(<span class="string">&quot;%y/%m/%d %H:%M:%S&quot;</span>), <span class="string">&quot;测试数据写入到表&quot;</span> + save_table)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2:注册为临时表，使用SparkSQL来写入分区表</span></span><br><span class="line">Spark_df.createOrReplaceTempView(<span class="string">&quot;tmp_table&quot;</span>)</span><br><span class="line">write_sql = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">insert overwrite table &#123;0&#125; partitions (pt_date=&#x27;&#123;1&#125;&#x27;)</span></span><br><span class="line"><span class="string">select * from tmp_table</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>.<span class="built_in">format</span>(save_table, <span class="string">&quot;20210520&quot;</span>)</span><br><span class="line">hc.sql(write_sql)</span><br><span class="line"><span class="built_in">print</span>(datetime.now().strftime(<span class="string">&quot;%y/%m/%d %H:%M:%S&quot;</span>), <span class="string">&quot;测试数据写入到表&quot;</span> + save_table)</span><br></pre></td></tr></table></figure>
<h3 id="当结果集为python的dataframe的时候">当结果集为Python的DataFrame的时候</h3>
<p>如果是Python的DataFrame，我们就需要多做一步把它转换为SparkDataFrame，其余操作就一样了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> HiveContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf()\</span><br><span class="line">      .setAppName(<span class="string">&quot;test&quot;</span>)\</span><br><span class="line">      .<span class="built_in">set</span>(<span class="string">&quot;hive.exec.dynamic.partition.mode&quot;</span>, <span class="string">&quot;nonstrict&quot;</span>) <span class="comment"># 动态写入hive分区表</span></span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line">hc = HiveContext(sc)</span><br><span class="line">sc.setLogLevel(<span class="string">&quot;ERROR&quot;</span>)</span><br><span class="line">    </span><br><span class="line">result_df = pd.DataFrame([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], columns=[<span class="string">&#x27;a&#x27;</span>])</span><br><span class="line">save_table = <span class="string">&quot;tmp.samshare_pyspark_savedata&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取DataFrame的schema</span></span><br><span class="line">c1 = <span class="built_in">list</span>(result_df.columns)</span><br><span class="line"><span class="comment"># 转为SparkDataFrame</span></span><br><span class="line">result = hc.createDataFrame(result_df.astype(<span class="built_in">str</span>), c1)</span><br><span class="line">result.write.<span class="built_in">format</span>(<span class="string">&quot;hive&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).saveAsTable(save_table) <span class="comment"># 或者改成append模式</span></span><br><span class="line"><span class="built_in">print</span>(datetime.now().strftime(<span class="string">&quot;%y/%m/%d %H:%M:%S&quot;</span>), <span class="string">&quot;测试数据写入到表&quot;</span> + save_table)</span><br></pre></td></tr></table></figure>
<h2 id="spark-调优思路">Spark 调优思路</h2>
<figure>
<img src="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/调优思维导图.jpg" alt="调优">
<figcaption aria-hidden="true">调优</figcaption>
</figure>
<h3 id="开发习惯优化">开发习惯优化</h3>
<h4 id="尽可能使用同一rdd避免重复创建并且适当持久化数据">尽可能使用同一RDD，避免重复创建，并且适当持久化数据</h4>
<p>这种开发习惯是需要我们对于即将要开发的应用逻辑有比较深刻的思考，并且可以通过code
review来发现的，讲白了就是要记得我们创建过啥数据集，可以复用的尽量广播（broadcast）下，能很好提升性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最低级写法，相同数据集重复创建。</span></span><br><span class="line">rdd1 = sc.textFile(<span class="string">&quot;./test/data/hello_samshare.txt&quot;</span>, <span class="number">4</span>) <span class="comment"># 这里的 4 指的是分区数量</span></span><br><span class="line">rdd2 = sc.textFile(<span class="string">&quot;./test/data/hello_samshare.txt&quot;</span>, <span class="number">4</span>) <span class="comment"># 这里的 4 指的是分区数量</span></span><br><span class="line"><span class="built_in">print</span>(rdd1.take(<span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(rdd2.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:x[<span class="number">0</span>:<span class="number">1</span>]).take(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 稍微进阶一些，复用相同数据集，但因中间结果没有缓存，数据会重复计算</span></span><br><span class="line">rdd1 = sc.textFile(<span class="string">&quot;./test/data/hello_samshare.txt&quot;</span>, <span class="number">4</span>) <span class="comment"># 这里的 4 指的是分区数量</span></span><br><span class="line"><span class="built_in">print</span>(rdd1.take(<span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(rdd1.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:x[<span class="number">0</span>:<span class="number">1</span>]).take(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相对比较高效，使用缓存来持久化数据</span></span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>), <span class="number">4</span>).cache()  <span class="comment"># 或者persist()</span></span><br><span class="line">rdd_map = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x*<span class="number">2</span>)</span><br><span class="line">rdd_reduce = rdd.reduce(<span class="keyword">lambda</span> x, y: x+y)</span><br><span class="line"><span class="built_in">print</span>(rdd_map.take(<span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(rdd_reduce)</span><br></pre></td></tr></table></figure>
<p>下面我们就来对比一下使用缓存能给我们的Spark程序带来多大的效率提升吧，我们先构造一个程序运行时长测量器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># 统计程序运行时间</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">time_me</span>(<span class="params">info=<span class="string">&quot;used&quot;</span></span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_time_me</span>(<span class="params">fn</span>):</span><br><span class="line"><span class="meta">        @functools.wraps(<span class="params">fn</span>)</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">            start = time.time()</span><br><span class="line">            fn(*args, **kwargs)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;%s %s %s&quot;</span> % (fn.__name__, info, time.time() - start), <span class="string">&quot;second&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> _wrapper</span><br><span class="line">    <span class="keyword">return</span> _time_me</span><br></pre></td></tr></table></figure>
<p>下面我们运行下面的代码，看下使用了cache带来的效率提升：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@time_me()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">types=<span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> types == <span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;使用持久化缓存&quot;</span>)</span><br><span class="line">        rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10000000</span>), <span class="number">4</span>)</span><br><span class="line">        rdd1 = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x*x + <span class="number">2</span>*x + <span class="number">1</span>).cache()  <span class="comment"># 或者 persist(StorageLevel.MEMORY_AND_DISK_SER)</span></span><br><span class="line">        <span class="built_in">print</span>(rdd1.take(<span class="number">10</span>))</span><br><span class="line">        rdd2 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x+y)</span><br><span class="line">        rdd3 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">        rdd4 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">        rdd5 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">        <span class="built_in">print</span>(rdd5)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;不使用持久化缓存&quot;</span>)</span><br><span class="line">        rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10000000</span>), <span class="number">4</span>)</span><br><span class="line">        rdd1 = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x * x + <span class="number">2</span> * x + <span class="number">1</span>)</span><br><span class="line">        <span class="built_in">print</span>(rdd1.take(<span class="number">10</span>))</span><br><span class="line">        rdd2 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">        rdd3 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">        rdd4 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">        rdd5 = rdd1.reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">        <span class="built_in">print</span>(rdd5)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">test()   <span class="comment"># 不使用持久化缓存</span></span><br><span class="line">time.sleep(<span class="number">10</span>)</span><br><span class="line">test(<span class="number">1</span>)  <span class="comment"># 使用持久化缓存</span></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># 使用持久化缓存</span></span><br><span class="line"><span class="comment"># [4, 9, 16, 25, 36, 49, 64, 81, 100, 121]</span></span><br><span class="line"><span class="comment"># 333333383333334999999</span></span><br><span class="line"><span class="comment"># test used 26.36529278755188 second</span></span><br><span class="line"><span class="comment"># 使用持久化缓存</span></span><br><span class="line"><span class="comment"># [4, 9, 16, 25, 36, 49, 64, 81, 100, 121]</span></span><br><span class="line"><span class="comment"># 333333383333334999999</span></span><br><span class="line"><span class="comment"># test used 17.49532413482666 second</span></span><br></pre></td></tr></table></figure>
<p>因为我们的代码是需要重复调用RDD1的，当没有对RDD1进行持久化的时候，每次当它被action算子消费了之后，就释放了，等下一个算子计算的时候要用，就从头开始计算一下RDD1。代码中需要重复调用RDD1
五次，所以没有缓存的话，差不多每次都要6秒，总共需要耗时26秒左右，但是，做了缓存，每次就只需要3s不到，总共需要耗时17秒左右。</p>
<p>另外，这里需要提及一下一个知识点，那就是持久化的级别，一般cache的话就是放入内存中，就没有什么好说的，需要讲一下的就是另外一个
persist()，它的持久化级别是可以被我们所配置的： | 存储级别 | 说明 | | --
| -- | | MEMORY_ONLY |
使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。
| | MEMORY_AND_DISK |
使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。
| | MEMORY_ONLY_SER |
基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。
| | MEMORY_AND_DISK_SER |
基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。|
| DISK_ONLY | 使用未序列化的Java对象格式，将数据全部写入磁盘文件中。 | |
MEMORY_ONLY_2，MEMORY_AND_DISK_2，等等 |
对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。
|</p>
<h4 id="尽量避免使用低性能算子">尽量避免使用低性能算子</h4>
<p>shuffle类算子算是低性能算子的一种代表，如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。</p>
<p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</p>
<p>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p>
<h6 id="broadcast与map进行join代码示例">Broadcast与map进行join代码示例</h6>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统的join操作会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Broadcast+map的join操作，不会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></span><br><span class="line"><span class="keyword">val</span> rdd2Data = rdd2.collect()</span><br><span class="line"><span class="keyword">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></span><br><span class="line"><span class="comment">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span></span><br><span class="line"><span class="comment">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></span><br><span class="line"><span class="comment">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原则2：尽量避免使用低性能算子</span></span><br><span class="line">rdd1 = sc.parallelize([(<span class="string">&#x27;A1&#x27;</span>, <span class="number">211</span>), (<span class="string">&#x27;A1&#x27;</span>, <span class="number">212</span>), (<span class="string">&#x27;A2&#x27;</span>, <span class="number">22</span>), (<span class="string">&#x27;A4&#x27;</span>, <span class="number">24</span>), (<span class="string">&#x27;A5&#x27;</span>, <span class="number">25</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&#x27;A1&#x27;</span>, <span class="number">11</span>), (<span class="string">&#x27;A2&#x27;</span>, <span class="number">12</span>), (<span class="string">&#x27;A3&#x27;</span>, <span class="number">13</span>), (<span class="string">&#x27;A4&#x27;</span>, <span class="number">14</span>)])</span><br><span class="line"><span class="comment"># 低效的写法，也是传统的写法，直接join</span></span><br><span class="line">rdd_join = rdd1.join(rdd2)</span><br><span class="line"><span class="built_in">print</span>(rdd_join.collect())</span><br><span class="line"><span class="comment"># [(&#x27;A4&#x27;, (24, 14)), (&#x27;A2&#x27;, (22, 12)), (&#x27;A1&#x27;, (211, 11)), (&#x27;A1&#x27;, (212, 11))]</span></span><br><span class="line">rdd_left_join = rdd1.leftOuterJoin(rdd2)</span><br><span class="line"><span class="built_in">print</span>(rdd_left_join.collect())</span><br><span class="line"><span class="comment"># [(&#x27;A4&#x27;, (24, 14)), (&#x27;A2&#x27;, (22, 12)), (&#x27;A5&#x27;, (25, None)), (&#x27;A1&#x27;, (211, 11)), (&#x27;A1&#x27;, (212, 11))]</span></span><br><span class="line">rdd_full_join = rdd1.fullOuterJoin(rdd2)</span><br><span class="line"><span class="built_in">print</span>(rdd_full_join.collect())</span><br><span class="line"><span class="comment"># [(&#x27;A4&#x27;, (24, 14)), (&#x27;A3&#x27;, (None, 13)), (&#x27;A2&#x27;, (22, 12)), (&#x27;A5&#x27;, (25, None)), (&#x27;A1&#x27;, (211, 11)), (&#x27;A1&#x27;, (212, 11))]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 高效的写法，使用广播+map来实现相同效果</span></span><br><span class="line"><span class="comment"># tips1: 这里需要注意的是，用来broadcast的RDD不可以太大，最好不要超过1G</span></span><br><span class="line"><span class="comment"># tips2: 这里需要注意的是，用来broadcast的RDD不可以有重复的key的</span></span><br><span class="line">rdd1 = sc.parallelize([(<span class="string">&#x27;A1&#x27;</span>, <span class="number">11</span>), (<span class="string">&#x27;A2&#x27;</span>, <span class="number">12</span>), (<span class="string">&#x27;A3&#x27;</span>, <span class="number">13</span>), (<span class="string">&#x27;A4&#x27;</span>, <span class="number">14</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">&#x27;A1&#x27;</span>, <span class="number">211</span>), (<span class="string">&#x27;A1&#x27;</span>, <span class="number">212</span>), (<span class="string">&#x27;A2&#x27;</span>, <span class="number">22</span>), (<span class="string">&#x27;A4&#x27;</span>, <span class="number">24</span>), (<span class="string">&#x27;A5&#x27;</span>, <span class="number">25</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># step1： 先将小表进行广播，也就是collect到driver端，然后广播到每个Executor中去。</span></span><br><span class="line">rdd_small_bc = sc.broadcast(rdd1.collect())</span><br><span class="line"></span><br><span class="line"><span class="comment"># step2：从Executor中获取存入字典便于后续map操作</span></span><br><span class="line">rdd_small_dict = <span class="built_in">dict</span>(rdd_small_bc.value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step3：定义join方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">broadcast_join</span>(<span class="params">line, rdd_small_dict, join_type</span>):</span><br><span class="line">    k = line[<span class="number">0</span>]</span><br><span class="line">    v = line[<span class="number">1</span>]</span><br><span class="line">    small_table_v = rdd_small_dict[k] <span class="keyword">if</span> k <span class="keyword">in</span> rdd_small_dict <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> join_type == <span class="string">&#x27;join&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> (k, (v, small_table_v)) <span class="keyword">if</span> k <span class="keyword">in</span> rdd_small_dict <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">elif</span> join_type == <span class="string">&#x27;left_join&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> (k, (v, small_table_v <span class="keyword">if</span> small_table_v <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;not support join type!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># step4：使用 map 实现 两个表join的功能</span></span><br><span class="line">rdd_join = rdd2.<span class="built_in">map</span>(<span class="keyword">lambda</span> line: broadcast_join(line, rdd_small_dict, <span class="string">&quot;join&quot;</span>)).<span class="built_in">filter</span>(<span class="keyword">lambda</span> line: line <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)</span><br><span class="line">rdd_left_join = rdd2.<span class="built_in">map</span>(<span class="keyword">lambda</span> line: broadcast_join(line, rdd_small_dict, <span class="string">&quot;left_join&quot;</span>)).<span class="built_in">filter</span>(<span class="keyword">lambda</span> line: line <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)</span><br><span class="line"><span class="built_in">print</span>(rdd_join.collect())</span><br><span class="line"><span class="built_in">print</span>(rdd_left_join.collect())</span><br><span class="line"><span class="comment"># [(&#x27;A1&#x27;, (211, 11)), (&#x27;A1&#x27;, (212, 11)), (&#x27;A2&#x27;, (22, 12)), (&#x27;A4&#x27;, (24, 14))]</span></span><br><span class="line"><span class="comment"># [(&#x27;A1&#x27;, (211, 11)), (&#x27;A1&#x27;, (212, 11)), (&#x27;A2&#x27;, (22, 12)), (&#x27;A4&#x27;, (24, 14)), (&#x27;A5&#x27;, (25, None))]</span></span><br></pre></td></tr></table></figure>
<p>上面的RDD join被改写为
broadcast+map的PySpark版本实现，不过里面有两个点需要注意：</p>
<ul>
<li>tips1: 用来broadcast的RDD不可以太大，最好不要超过1G</li>
<li>tips2: 用来broadcast的RDD不可以有重复的key的</li>
</ul>
<h4 id="使用map-side预聚合的shuffle操作">使用map-side预聚合的shuffle操作</h4>
<p>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。</p>
<p>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p>
<p>比如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p>
<p><img src="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/map-side-shuffle0.png" alt="groupByKey原理"> <img src="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/map-side-shuffle1.png" alt="reduceByKey原理"></p>
<h4 id="尽量使用高性能算子">尽量使用高性能算子</h4>
<p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p>
<h5 id="使用reducebykeyaggregatebykey替代groupbykey">使用reduceByKey/aggregateByKey替代groupByKey</h5>
<p>详情见：使用map-side预聚合的shuffle操作。</p>
<h5 id="使用mappartitions替代普通map">使用mapPartitions替代普通map</h5>
<p>mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p>
<h5 id="使用foreachpartitions替代foreach">使用foreachPartitions替代foreach</h5>
<p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p>
<h5 id="使用filter之后进行coalesce操作">使用filter之后进行coalesce操作</h5>
<p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p>
<h5 id="使用repartitionandsortwithinpartitions替代repartition与sort类操作">使用repartitionAndSortWithinPartitions替代repartition与sort类操作</h5>
<p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p>
<h4 id="广播大变量">广播大变量</h4>
<p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p>
<p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p>
<p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p>
<h5 id="python示例">python示例</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原则4：广播大变量</span></span><br><span class="line">rdd1 = sc.parallelize([(<span class="string">&#x27;A1&#x27;</span>, <span class="number">11</span>), (<span class="string">&#x27;A2&#x27;</span>, <span class="number">12</span>), (<span class="string">&#x27;A3&#x27;</span>, <span class="number">13</span>), (<span class="string">&#x27;A4&#x27;</span>, <span class="number">14</span>)])</span><br><span class="line">rdd1_broadcast = sc.broadcast(rdd1.collect())</span><br><span class="line"><span class="built_in">print</span>(rdd1.collect())</span><br><span class="line"><span class="built_in">print</span>(rdd1_broadcast.value)</span><br><span class="line"><span class="comment"># [(&#x27;A1&#x27;, 11), (&#x27;A2&#x27;, 12), (&#x27;A3&#x27;, 13), (&#x27;A4&#x27;, 14)]</span></span><br><span class="line"><span class="comment"># [(&#x27;A1&#x27;, 11), (&#x27;A2&#x27;, 12), (&#x27;A3&#x27;, 13), (&#x27;A4&#x27;, 14)]</span></span><br></pre></td></tr></table></figure>
<h5 id="scala示例">scala示例</h5>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 以下代码在算子函数中，使用了外部的变量。</span></span><br><span class="line"><span class="comment">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line">rdd1.map(list1...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下代码将list1封装成了Broadcast类型的广播变量。</span></span><br><span class="line"><span class="comment">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span></span><br><span class="line"><span class="comment">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span></span><br><span class="line"><span class="comment">// 每个Executor内存中，就只会驻留一份广播变量副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line"><span class="keyword">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class="line">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure>
<h4 id="使用kryo优化序列化性能">使用Kryo优化序列化性能</h4>
<p>在Spark中，主要有三个地方涉及到了序列化：</p>
<ul>
<li>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见：广播大变量”中的讲解）。</li>
<li>将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。</li>
<li>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</li>
</ul>
<p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream
API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p>
<p>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建SparkConf对象。</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line"><span class="comment">// 设置序列化器为KryoSerializer。</span></span><br><span class="line">conf.set(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line"><span class="comment">// 注册要序列化的自定义类型。</span></span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br></pre></td></tr></table></figure>
<h4 id="spark作业基本原理">Spark作业基本原理</h4>
<p>如果要进行资源调优，我们就必须先知道Spark运行的机制与流程。</p>
<figure>
<img src="/2023/10/26/PySpark%E5%9F%BA%E7%A1%80001/Spark作业基本原理.png" alt="Spark作业基本运行原理">
<figcaption aria-hidden="true">Spark作业基本运行原理</figcaption>
</figure>
<p>详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU
core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark
Standalone集群，也可以是其他的资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU
core。</p>
<p>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。<strong>task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。</strong>一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。</p>
<p><strong>Spark是根据shuffle类算子来进行stage的划分。</strong>如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，<strong>它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。</strong></p>
<p>当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。</p>
<p>因此Executor的内存主要分为三块：</p>
<ul>
<li>第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；</li>
<li>第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；</li>
<li>第三块是让RDD持久化时使用，默认占Executor总内存的60%。</li>
</ul>
<p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU
core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU
core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</p>
<p>以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。</p>
<h4 id="资源参数调优">资源参数调优</h4>
<p>了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p>
<h5 id="num-executors">num-executors</h5>
<ul>
<li>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li>
<li>参数调优建议：每个Spark作业的运行一般设置<code>50~100</code>个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li>
</ul>
<h5 id="executor-memory">executor-memory</h5>
<ul>
<li>参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM
OOM异常，也有直接的关联。</li>
<li>参数调优建议：每个Executor进程的内存设置<code>4G~8G</code>较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的<code>1/3~1/2</code>，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li>
</ul>
<h5 id="executor-cores">executor-cores</h5>
<ul>
<li>参数说明：该参数用于设置每个Executor进程的CPU
core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU
core同一时间只能执行一个task线程，因此每个Executor进程的CPU
core数量越多，越能够快速地执行完分配给自己的所有task线程。</li>
<li>参数调优建议：Executor的CPU
core数量设置为<code>2~4</code>个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU
core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU
core。同样建议，如果是跟他人共享这个队列，那么num-executors *
executor-cores不要超过队列总CPU
core的<code>1/3~1/2</code>左右比较合适，也是避免影响其它业务的作业运行。</li>
</ul>
<h5 id="driver-memory">driver-memory</h5>
<ul>
<li>参数说明：该参数用于设置Driver进程的内存。</li>
<li>参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</li>
</ul>
<h5 id="spark.default.parallelism">spark.default.parallelism</h5>
<ul>
<li>参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。</li>
<li>参数调优建议：Spark作业的默认task数量为<code>500~1000</code>个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS
block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors
* executor-cores的<code>2~3</code>倍较为合适，比如Executor的总CPU
core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</li>
</ul>
<h5 id="spark.storage.memoryfraction">spark.storage.memoryFraction</h5>
<ul>
<li>参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor
60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li>
<li>参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark
web
ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li>
</ul>
<h5 id="spark.shuffle.memoryfraction">spark.shuffle.memoryFraction</h5>
<ul>
<li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li>
<li>参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li>
</ul>
<blockquote>
<p>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark
web
ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</p>
</blockquote>
<h4 id="资源参数参考示例">资源参数参考示例</h4>
<p>以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --master yarn-cluster \</span><br><span class="line">  --num-executors 100 \</span><br><span class="line">  --executor-memory 6G \</span><br><span class="line">  --executor-cores 4 \</span><br><span class="line">  --driver-memory 1G \</span><br><span class="line">  --conf spark.default.parallelism=1000 \</span><br><span class="line">  --conf spark.storage.memoryFraction=0.5 \</span><br><span class="line">  --conf spark.shuffle.memoryFraction=0.3 \</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.blockchainof.com/2023/10/19/Spark%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="暂留白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="自留地">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/10/19/Spark%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/" class="post-title-link" itemprop="url">Spark部署模式</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-10-19 17:00:27" itemprop="dateCreated datePublished" datetime="2023-10-19T17:00:27+08:00">2023-10-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-10-26 14:15:30" itemprop="dateModified" datetime="2023-10-26T14:15:30+08:00">2023-10-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/" itemprop="url" rel="index"><span itemprop="name">Spark</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="简介">简介</h1>
<p><code>Spark Application</code>提交运行时的部署模式<code>Deploy Mode</code>，表示的是<code>Driver Program</code>运行的地方。
- <code>client</code>:Driver Program
运行在<code>提交应用的Client</code>上 -
<code>cluster</code>:要么就是集群中从节点<code>(Standalone：Worker，YARN：NodeManager)</code>。</p>
<p>默认值为<code>client</code>，当在实际的开发环境中，尤其是生产环境，使用<code>cluster</code>部署模式提交应用运行。</p>
<h1 id="client-模式">Client 模式</h1>
<p>以<code>Spark Application</code>运行到<code>Standalone</code>集群上为例，前面提交运行圆周率PI或者词频统计<code>WordCount程序</code>时，默认
<code>DeployMode</code>为<code>Client</code>，表示应用<code>Driver Program</code>运行在提交应用的<code>Client 主机</code>上（启动
JVM Process 进程），示意图如下： <img src="/2023/10/19/Spark%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/Deploy-Mode-Client.jpg" alt="Client模式"></p>
<h1 id="cluster-模式">Cluster 模式</h1>
<p>如果采用cluster模式运行应用，应用Driver
Program运行在集群从节点Worker某台机器上，示意图如下： <img src="/2023/10/19/Spark%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/Deploy-Mode-Cluster.jpg" alt="Cluster模式"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.kaizi.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于Scala语言使用SparkCore编程实现词频统计：WordCount</span></span><br><span class="line"><span class="comment"> * 从HDFS上读取数据，统计WordCount，将结果保存到HDFS上</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> 当应用运行在集群上的时候，MAIN函数就是Driver Program，必须创建SparkContext对象</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建SparkConf对象，设置应用的配置信息，比如应用名称和应用运行模式</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setMaster(<span class="string">&quot;local[2]&quot;</span>)  <span class="comment">// 设置运行本地模式</span></span><br><span class="line">      .setAppName(<span class="string">&quot;SparkWordCount&quot;</span>)</span><br><span class="line">    <span class="comment">// 构建SparkContext上下文实例对象，读取数据和调度Job执行</span></span><br><span class="line">    <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ************第一步、读取数据:在Spark的Executor中执行***************</span></span><br><span class="line">    <span class="comment">// 封装到RDD集合，认为列表List</span></span><br><span class="line">    <span class="keyword">val</span> inputRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;/datas/wordcount.data&quot;</span>)</span><br><span class="line">    <span class="comment">// ***************************************************************</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// ************第二步、处理数据:在Spark的Executor中执行***************</span></span><br><span class="line">    <span class="comment">// 调用RDD中函数，认为调用列表中的函数</span></span><br><span class="line">    <span class="comment">// a. 每行数据分割为单词</span></span><br><span class="line">    <span class="keyword">val</span> wordsRDD = inputRDD.flatMap(line =&gt; line.split(<span class="string">&quot;\\s+&quot;</span>))</span><br><span class="line">    <span class="comment">// b. 转换为二元组，表示每个单词出现一次</span></span><br><span class="line">    <span class="keyword">val</span> tuplesRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordsRDD.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">// c. 按照Key分组聚合</span></span><br><span class="line">    <span class="keyword">val</span> wordCountsRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = tuplesRDD.reduceByKey((tmp, item) =&gt; tmp + item)</span><br><span class="line">    <span class="comment">// ***************************************************************</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// ************第三步、输出数据:在Spark的Executor中执行****************</span></span><br><span class="line">    wordCountsRDD.foreach(println)</span><br><span class="line">    <span class="comment">// 保存到为存储系统，比如HDFS</span></span><br><span class="line">    wordCountsRDD.saveAsTextFile(<span class="string">s&quot;/datas/swc-output-<span class="subst">$&#123;System.currentTimeMillis()&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment">// ***************************************************************</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 为了测试，线程休眠，查看WEB UI界面</span></span><br><span class="line">    <span class="type">Thread</span>.sleep(<span class="number">10000000</span>)</span><br><span class="line">    <span class="comment">// TODO：应用程序运行接收，关闭资源</span></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>main方法中一开始的创建SparkContext对象和最后的关闭SparkContext资源，都是在Driver
Program中执行的，代码中的第一步加载数据、第二步处理数据、第三步输出数据都是在Executor上执行。</p>
</blockquote>
<p>综上所述Spark Application中Job执行有两个主要点： -
RDD输出函数分类两类 -
第一类：返回值给<code>Driver Progam</code>，比如<code>count</code>、<code>first</code>、<code>take</code>、<code>collect</code>等
- 第二类：没有返回值，比如直接打印结果、保存至外部存储系统（HDFS文件）等
-
在<code>Job</code>中从读取数据封装为<code>RDD</code>和一切<code>RDD调用方法</code>都是在<code>Executor</code>中执行，其他代码都是在<code>Driver Program</code>中执行
-
<code>SparkContext</code>创建与关闭、其他变量创建等在<code>Driver Program</code>中执行
- <code>RDD调用函数</code>都是在Executors中执行</p>
<h1 id="client模式和cluster模式的区别">Client模式和Cluster模式的区别</h1>
<p>Cluster和Client模式最本质的区别是：Driver程序运行在哪里。 -
cluster模式：生产环境中使用该模式 - Driver程序在YARN集群当中 -
应用的运行结果不能在客户端显示 -
client模式：学习测试时使用（也不一定，个人觉得生产环境也可以用） -
Driver运行在Client上的SparkSubmit进程中 -
应用程序运行结果会在客户端显示</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">暂留白</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">87</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">38</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">183</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">暂留白</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>
-->

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
